.LOG
14:32 2012-11-01
GTD
0, -18:20

1, today
1), 15:06-16:37 update my libxl lock patch. see"11:31 2012-10-29"
2), -18:16 try to add domainInterfaceStats in libxl api. see"16:38 2012-11-01"

16:38 2012-11-01
virtualization, libvirt, xenlight, add libxl api, cont1, domainInterfaceStats
1, modify and compile code success. and test fail: start vm fail.
2, after recompile, the former error does not exist. 
but there is not vif in xenlight(it is list as vif in xend with the same xml file).
linux-vm4:/etc/xen/vm # virsh domiflist 6
Interface  Type       Source     Model       MAC
-------------------------------------------------------
-          bridge     br0        -           00:16:3e:01:88:54
3, (15:33 2012-11-02)
linuxDomainInterfaceStats read the data from host "/proc/net/dev", i could found the vif device while using xenlight, but there is no packget counts. why? xenlight does not use vif? 

8:02 2012-11-02
company, virtualization, suse, xen, regular meeting: US / China Virtualization Sync, meeting
1, xen and kvm future. 
2, BoYang
macvtype. 
3, Lin Ma
1), Jim, sle11 sp3 will use libvirt 1.0.0

17:07 2012-11-02
software skill, SCM, git, external diff
http://technotales.wordpress.com/2009/05/17/git-diff-with-vimdiff/
1, .git/config
[diff]
    external = git_diff_wrapper

2, bamvor@linux-bjrd:~> cat bin/git_diff_wrapper
#!/bin/bash

gvimdiff --nofork $2 $5

"--nofork" could block the gvim. it will not create lots of gvim while git diff.

17:43 2012-11-02
GTD
1, today
1), summary: 当初遇到migration fail, 我如果想到追到xenlight library里面, 也许问题早就解决了. 

10:03 2012-11-05
arm, armv8, Cortex-A50, Cortex-A53, Cortex-A57
摘要: 延续大小核技术, 第一批芯片2013年出货(?), 内核空间64bit同时用户空间32bit
http://www.eet-china.com/ART_8800677493_617693_NT_7a9b2e89.HTM
20nm A50系列产品2013年生产，FinFET产品2013年流片，由台积电(TSMC)提供早期实现帮助。
海思是个非常好的例证！从最初在机顶盒方面的合作，到现在的网络系统、数据中心产品，正是以海思为代表的合作伙伴多样化的产品需求，推动了ARM的产品研发。有时候，我们能够听到客户更多的抱怨反而是研发进度过慢而不是过快。
ARM处理器部门负责Program Management的总监John Goodacre日前撰文称，正是AArch32和AArch64这两种处理器执行状态的融合，使得ARMv8成为倍受业界关注的架构。他解释称，从以往来看，当处理器架构要支持64位处理时，通常在以下两种演进方式中二选其一：创造一个全新架构，摒弃所有高效传统模式；或在现有32位架构的基础上添加64位处理功能，导致复杂性提高而且低效。而ARMv8的优势在于既可支持性能出众的传统模式，又采用全新64位设计，可以最大程度地提高两种状态的功效，同时还为软件提供渐进式路标图，从而按市场要求的步调采用新功能。
例如，在很多网络和企业市场，用户运行的应用要求具有2GB或3GB以上的RAM，这些市场将会直接采用特定架构的AArch64 状态。在某些合作伙伴的具体实施中，没有需要在处理器的特定模式下工作的传统软件，因此他们可以决定放弃架构对AArch32 状态的支持，而只提供64位的支持。
操作系统“内存不足”很可能是移动设备在64位支持方面遇到的第一个问题。John Goodacre表示，ARMv8 架构采用了简易合理的方法，允许操作系统在AArch64的64位虚拟地址模式下运行，而用户应用仍可在AArch32状态下运行。这使解决方案能够做到两全其美：既可运行无限数量的完全性能32位用户应用，同时又能使操作系统在ARMv8设备上的AArch64位模式下高效运行。

10:16 2012-11-05
community, linaro, arm Server
1, 
http://www.linaro.org/news/release/industry-leaders-collaborate-to-accelerate-software-ecosystem-for-arm-servers-and-join-linaro/en/
Santa Clara, US - 1 Nov 2012
AMD, AppliedMicro, Calxeda, Canonical, Cavium, Facebook, HP, Marvell and Red Hat join existing Linaro members ARM, HiSilicon, Samsung and ST-Ericsson to form new group focused on accelerating Linux development for ARM servers 
Linaro Enterprise Group (LEG) will build on Linaro’s experience of bringing competing companies together to work on common solutions and enable OEMs, commercial Linux providers and System on a Chip (SoC) vendors to collaborate in a neutral environment on the development and optimization of the core software needed by the rapidly emerging market for low-power hyperscale servers. 
2, Bamvor
众多公司加入ARM Server阵营
现在这么多公司都加入到LEG做服务器开发, 大家都看好arm server的前景.
IC company: AMD, AppliedMicro, ARM, Calxeda, Cavium, HiSilicon, Marvell, Samsung and ST-Ericsson.  
Linux Distrubution: Canonical, Red Hat. 
Server: HP
Other: Facebook
现在芯片公司还差TI, Qualcomm. Linux distribution公司还差suse. 

10:37 2012-11-05
GTD
0, 9:40-11:44 13:51-18:18

1, today
1), 40' armv8 and arm server, see"10:03 2012-11-05", "10:16 2012-11-05". 
\TODO: write a blog for this. 
2), 10' 14:25-17:00 add api in libvirt libxl driver. see"11:17 2012-11-05"
3), 20' work report. "17:36 2012-11-05"

11:17 2012-11-05
virtualization, libvirt, xenlight, add libxl api, cont2, domainBlockStats
1, do some investigation for domainBlockStats in last weekend. the qdisk is created by qemu through xenstore. So, if i could find how to get statics from qemu(if it is possible). i could implement the domainBlockStats api. It looks better than domainInterfaceStats, because vif statics in host "/proc/net/dev" is always zero while using xenlight. 
2, qemu-dm
1), command line(get after xen vm): 
qemu-dm -d 1 -domain-name sles11_hvm_10 -vnc 127.0.0.1:0 -serial pty -videoram 8 -boot c -acpi -vcpus 4 -vcpu_avail 0xf -net none -M xenfv
2), list block statics in qemu monitor: 
info blockstats
3), 虽然找到了统计信息在什么地方, 但是如果想从qemu monitor拿到这个数据并且返回给xenlight. 似乎要做很多事情. 
(1), 如果要通过qemu monitor得到信息的话, 目前xenlight里面没有这个功能, 需要新增. 为了一个block statics有必要么? 同时xen 4.3里面的PVH mode就不会使用qemu了. 这个修改也就没用了(所以xen upstream)不一定会收. 
(2), 如果不通过qemu monitor, 需要看看xenstore里面有没有其他方式.
似乎是xenbus? 
3, 
(gdb) break blk_event 
Breakpoint 1 at 0x479eb0: file /usr/src/debug/xen-4.1.2-testing/tools/ioemu-dir/hw/xen_disk.c, line 778.
(gdb) cont
Continuing.

Breakpoint 1, blk_event (xendev=0xadd280)
    at /usr/src/debug/xen-4.1.2-testing/tools/ioemu-dir/hw/xen_disk.c:778
778         struct XenBlkDev *blkdev = container_of(xendev, struct XenBlkDev, xendev);
(gdb) p xendev
$1 = (struct XenDevice *) 0xadd280
(gdb) p *xendev
$2 = {type = 0x4f855e "qdisk", dom = 1, dev = 768,
  name = "qdisk-768", '\000' <repeats 54 times>, debug = 0,
  be_state = XenbusStateConnected, fe_state = XenbusStateConnected,
  online = 1,
  be = "/local/domain/0/backend/qdisk/1/768", '\000' <repeats 988 times>,
  fe = 0xadae70 "/local/domain/1/device/vbd/768",
  protocol = 0xb096a0 "x86_64-abi", remote_port = 8, local_port = 69,
  evtchndev = 0xadd7e0, gnttabdev = 0xaddca0, ops = 0x72d820, next = {
    tqe_next = 0x0, tqe_prev = 0xadc4a0}}
4, it seems that both domainBlockStats and domainInterfaceStats is not eary to implement. So, find another api for this week!!!
But jiaju suggect me first improve lock patch. 
5, 
domainGetMaxVcpus
domainBlockStats
domainInterfaceStats
domainBlockPeek
nodeGetCellsFreeMemory
nodeDeviceDettach
nodeDeviceReAttach
nodeDeviceReset
domainOpenConsole
nodeSuspendForDuration
nodeGetMemoryParameters
nodeSetMemoryParameters
1), BlockPeek: 
(1), validate "path"
(2), open "path", lseek and saferead. 

17:36 2012-11-05
work report - week 44
1, [devel-server] work report - week 44
work for libvirt libxl driver: 
1), send lock patch version 1 and version2 to libvirt upstream after upstream response. 
2), continue to try to add domainBlockStats for libxl driver. 
I found that xenlight use qemu as disk backend while disk type is qdisk for both pv and hvm guest. I could get block statics from qemu through monitor command, but it seems that there is no such machanism in xenlight for this. while in the xen 4.3, a new vm type PVH type will be used which will not use the qemu daemon in xenlight. So, maybe it is not worth to add block statics in current xenlight drivers? 

16:16 2012-11-06
GTD
0, 16:00-18:50 20:35

1, today
1), Chunyan her api status. about BlockPeek: from her work report, her finish all the api except for interface and block stats. 
2), 1h read migration(domain suspend) in xen upstream. see"16:52 2012-11-06"
3), 20' Jiaju talk to me, about my performance.
4), 20:35- search one api which i could finish this week. 

16:52 2012-11-06
virtualization, libvirt, xen, libxl, read migration(domain suspend) in xen upstream
1, 现在的关键就是到底xenlight 4.2里面migration能不能打断，如果能打断，我现在就能干活了。如果不能，还得找其他事情。
简单看了看，原来migration里面的核心代码"xc_domain_save"放到了tools/libxl/libxl_save_helper.c
2, 从下面的分析看，目前suspend(to disk or to remote socket)都是建立的一个子进程做xc_domain_save的工作。但是没有看到任何异步打断的机制，看起来只是为了便于上层调用，允许同时调下来多个命令？如果是这样的话，qemu的monitor lock似乎xenlight也需要。
1), \TODO: discuss with Jim and Chunyan for xenlight event lock(similar to qemu monitor lock), 在适当的时候.
2), 具体分析

libxl__domain_suspend -> set dss->shs.callbacks: suspend, postcopy, checkpoint
                         set toolstack_save as libxl__toolstack_save
                         libxl__xc_domain_save
                         domain_suspend_done
libxl__xc_domain_save -> run_helper
                         libxl__xc_domain_save_done

libxl__toolstack_save: get phy status?

run_helper -> fork
              exec suspend in child
              libxl__ev_fd_register: register event for monitoring suspend 
                                     process? 

xc_domain_save -> suspend_and_state -> suspend
suspend is set by libxl__domain_suspend as remus or common suspend. 

12:52 2012-11-07
GTD
0, 10:00 20:03-22:30

1, today
1), 15:03 search one api which i could finish this week. see"15:16 2012-11-07"
2), try to improve attach-disk, detach-disk in libxl driver, see"18:17 2012-11-07"

13:01 2012-11-07
virtualization, libvirt, 1.0.0 release; localization
1, 1.0.0 release notes
https://www.redhat.com/archives/libvir-list/2012-November/msg00118.html
2, localization
https://www.transifex.com/projects/p/libvirt/

14:07 2012-11-07
arm, board, allwinner A10, cubie board
rumor said that cubie board guys come from VIA. 

14:59 2012-11-07
software skill, mount, network, nfs; rpc, rpcbind
1, mount fail because the rpcbind not started
linux-iwgo:/home/suse # mount.nfs 147.2.207.77:/home/bamvor/sda3/home sda3/home/ -v
mount.nfs: timeout set for Wed Nov  7 15:00:44 2012
mount.nfs: trying text-based options 'vers=4,addr=147.2.207.77,clientaddr=147.2.207.74'
mount.nfs: mount(2): No such file or directory
Starting rpc.statd ... portmapper not running                        failed
mount.nfs: rpc.statd is not running but is required for remote locking.
mount.nfs: Either use '-o nolock' to keep locks local, or start statd.
mount.nfs: an incorrect mount option was specified
2, check and start rpcbind
linux-iwgo:/home/suse # zypper se rpc
Loading repository data...
Reading installed packages...

S | Name                    | Summary                                  | Type
--+-------------------------+------------------------------------------+--------
  | geronimo-jaxrpc-1_1-api | Geronimo J2EE server J2EE specifications | package
  | libgfrpc0               | GlusterFS Remote Procedure Call library  | package
  | librpcsecgss-devel      | Library Implementing GSSAPI Security f-> | package
  | librpcsecgss3           | Library Implementing GSSAPI Security f-> | package
  | libtirpc-devel          | Transport Independent RPC Library        | package
i | libtirpc1               | Transport Independent RPC Library        | package
  | libtirpc1-32bit         | Transport Independent RPC Library        | package
  | nagios-plugins-rpc      | Check RPC service                        | package
  | perl-JSON-RPC           | Perl implementation of JSON-RPC 1.1 pr-> | package
i | perl-PlRPC              | Perl Extension for Writing PlRPC Servers | package
  | perl-RPC-XML            | A set of classes for core data, messag-> | package
  | php5-pear-Horde_Rpc     | PEAR: Horde RPC API                      | package
  | php5-xmlrpc             | PHP5 Extension Module                    | package
i | rpcbind                 | Transport independent RPC portmapper     | package
linux-iwgo:/home/suse # rcrpcbind status
redirecting to systemctl
rpcbind.service - RPC Bind
          Loaded: loaded (/lib/systemd/system/rpcbind.service; disabled)
          Active: inactive (dead)
          CGroup: name=systemd:/system/rpcbind.service

linux-iwgo:/home/suse # rcrpcbind restart
redirecting to systemctl
linux-iwgo:/home/suse # rcrpcbind status
redirecting to systemctl
rpcbind.service - RPC Bind
          Loaded: loaded (/lib/systemd/system/rpcbind.service; disabled)
          Active: active (running) since Wed, 07 Nov 2012 14:59:16 +0800; 1s ago
        Main PID: 21209 (rpcbind)
          CGroup: name=systemd:/system/rpcbind.service
                  └ 21209 /sbin/rpcbind -w -f

Nov 07 14:59:16 linux-iwgo rpcbind[21209]: Cannot open '/var/lib/rpcbind/rpc...)
Nov 07 14:59:16 linux-iwgo rpcbind[21209]: Cannot open '/var/lib/rpcbind/por...)
linux-iwgo:/home/suse # mount.nfs 147.2.207.77:/home/bamvor/sda3/home sda3/home/ -v
mount.nfs: timeout set for Wed Nov  7 15:01:21 2012
mount.nfs: trying text-based options 'vers=4,addr=147.2.207.77,clientaddr=147.2.207.74'
mount.nfs: mount(2): No such file or directory
Starting rpc.statd ...                                               done
mount.nfs: trying text-based options 'addr=147.2.207.77'
mount.nfs: prog 100003, trying vers=3, prot=6
mount.nfs: trying 147.2.207.77 prog 100003 vers 3 prot TCP port 2049
mount.nfs: prog 100005, trying vers=3, prot=17
mount.nfs: trying 147.2.207.77 prog 100005 vers 3 prot UDP port 33276
3, nfs mount successful
linux-iwgo:/home/suse # ls sda3/home/
novell

15:16 2012-11-07
domainMemoryPeek: not support in xenlight

18:17 2012-11-07
virtualization, libvirt, xenlight, attach-disk, detach-disk
0, git branch libxl_attach_detach_disk
1, 
linux-vm4:/etc/xen/vm # virsh attach-disk sles11_hvm_10 /dev/sdb1 sda
Disk attached successfully

linux-vm4:/etc/xen/vm # xm
linux-vm4:/etc/xen/vm # virsh detach-disk sles11_hvm_10 sda
error: Failed to detach disk
error: End of file while reading data: Input/output error
error: Failed to reconnect to the hypervisor

2, xm support scsi-attach, scsi-detach, scsi-list

3, 原来的libxl是否支持ide，或者任何image或设备或lvm？
xen默认就是用俄ide硬盘，所以至少ide要支持吧. 如果SCSI的热插拔没法做的话.

4, attach-disk
1), command successful, but will valid after reboot. 
2), compare xenstore: the state is different. 
(1), hda
((1)) frontend
linux-vm4:/etc/xen/vm # xenstore-ls /local/domain/2/device/vbd/768
backend = "/local/domain/0/backend/qdisk/2/768"
backend-id = "0"
state = "4"
virtual-device = "768"
device-type = "disk"
ring-ref = "8"
event-channel = "8"
protocol = "x86_64-abi"
((2)), backend
linux-vm4:/etc/xen/vm # xenstore-ls /local/domain/0/backend/qdisk/2        
768 = ""                                                                   
 frontend = "/local/domain/2/device/vbd/768"                               
 params = "aio:/var/lib/xen/images_2/sles11_hvm_10/disk0.raw"              
 frontend-id = "2"                                                         
 online = "1"                                                              
 removable = "1"                                                           
 bootable = "1"                                                            
 state = "4"
 dev = "hda"
 type = "tap"
 mode = "w"
 feature-barrier = "1"
 info = "0"
 sector-size = "512"
 sectors = "8388608"
 hotplug-status = "connected"
(2), sda
((1)), frontend
linux-vm4:/etc/xen/vm # xenstore-ls /local/domain/2/device/vbd/2048
backend = "/local/domain/133/backend/qdisk/2/2048"
backend-id = "133"
state = "3"
virtual-device = "2048"
device-type = "disk"
ring-ref = "896"
event-channel = "10"
protocol = "x86_64-abi"
((2)), backend
linux-vm4:/etc/xen/vm # xenstore-ls /local/domain/133/backend/qdisk/2
2048 = ""
 frontend = "/local/domain/2/device/vbd/2048"
 params = "aio:/dev/sdb"
 frontend-id = "2"
 online = "1"
 removable = "1"
 bootable = "1"
 state = "1"
 dev = "sda"
 type = "tap"
 mode = "w"
3), libxl_device_disk_add force set state as 1. what does "1" mean?
4), 其实xenlight本来也支持attach-disk, 效果和我加不加scsi是一样的, 只是虚拟机看到的设备类型不同. 

4, detach-disk fail(scsi and xen are same)
2012-11-07 12:02:29.089+0000: 26374: error : virNetSocketReadWire:1293 : End of file while reading data: Input/output error
看起来是因为xen不允许热插拔???
但是xend是支持scsi attach detach的, 如果我能把xend scsi干的活在xenlight上面实现, 就好了. 

\TODO: 明天看看能否解决这个问题, 如果能避免出这个问题, 这周也算有个交代.

10:24 2012-11-08
GTD
0, 10:24-18:40 20:37-22:11

1, today
1), 10:33-11:25 13:19- libxl lock: check Jim reply, see"10:24 2012-11-08"
2), reply to Jim both libvirt list and jim work email

2), 具体看看xenlight patch和pci passthrough, 看看有没有可能做两个里面的一个.

3), Summary: 这周工作主要是"强迫"自己工作, 从目前情况看, 现在一天已经能工作4-5个小时. 如果从下周开始, 每周出了正常工作的40小时, 再多加上8小时. 5个月就相当于工作其他同时工作6个月. 只要从现在开始抓紧时间, 估计到春节时, 应该会有点起色.

2, next
1), try to improve attach-disk, detach-disk in libxl driver, see"18:17 2012-11-07"4
2), test attach-disk command. see"10:41 2012-10-30"2
3), maybe libxl and qemu could communicate with qemu through xenstore?

libxl_dm.c: 
        /* Find uuid and the write the vnc password to xenstore for qemu. */
        t = xs_transaction_start(ctx->xsh);
        vm_path = libxl__xs_read(&gc,t,libxl__sprintf(&gc, "%s/vm", p->dom_path));
        if (vm_path) {
            /* Now write the vncpassword into it. */
            pass_stuff = libxl__calloc(&gc, 3, sizeof(char *));
            pass_stuff[0] = "vncpasswd";
            pass_stuff[1] = info->vncpasswd;
            libxl__xs_writev(&gc,t,vm_path,pass_stuff);

3, for Jim
1), xenlight event lock in libvirt. see"16:52 2012-11-06"2-1). 

4, next month
1), opensuse on MK802
(1), generate the 512MB version with opensuse12.2 RC2 which enable yast fist run. 
(2), try opensuse based on ubuntu kernel in order to solve the error from miniand. 
2), try armv8 emulator and read armv8 kernel code. 

10:24 2012-11-08
virtualization, libvirt, xen, patch, lock, cont1, upstream reply to v2
1, Jim_email_20121108_0654
Bamvor Jian Zhang wrote:
>>> +static int 
>>> +libxlDomainAbortJob(virDomainPtr dom) 
>>  
>> This function will always fail with the above logic.  ret is initialized 
>> to -1 and is never changed. 
>>  
>> Is it even possible to safely abort a libxl operation?  If not, this 
>> function should probably remain unimplemented.  Maybe it will be useful 
>> when the libxl driver supports migration. 
>>     
> return error because of the there is no cancelation opeartion in libvirt libxl
> driver with xen 4.1. according to xen4.2 release document, maybe the
> cancelation of long-running jobs is supported.

I finally got some time to take a closer look at Xen 4.2 libxl and
notice that the "long running" operations (create, save, dump, restore,
etc.) now support a 'libxl_asyncop_how *ao_how' parameter to control
their concurrency.  If ao_how->callback is NULL, a libxl_event is
generated when the operation completes.  We'll just need to handle these
events in the existing libxlEventHandler.  Some of the async operations
support reporting intermediate progress (e.g. for
libxlDomainGetJobInfo), but at this time none of them support
cancellation AFAICT.

With the new asynchronous support in Xen 4.2 libxl, IMO we should delay
this patchset until converting the driver to support 4.2.  I didn't
think this patch would be affected by Xen 4.1 vs 4.2 libxl, but it is
and I don't see any reason to add code that further complicates the
conversion.

BTW, Ondrej was working on a patch to convert the driver to 4.2.  Now
that I have some free time, I'll pick up some of this work too.

> but it is still useful for save, dump and migration(in future), because libvirt
> should block the user abort operation othervise xenlight might crash
>   

If it is left unimplemented, libvirt would block the operation anyhow,
failing with "not supported"

Regards,
Jim

2, test and try to reply
1), 之前我记得测试过如果没有AbortJob, 用户Ctrl+c时会直接打断异步job. 但是今天从代码看, 似乎不是这样, 确认一下. 
加不加abort只是出错信息有区别, 所以Abort还真的可以不加. 

3, reply
run_helper: save, migrate, restore.
restore could be a async job at this point.

10:40 2012-11-08
software, skill, text editor, vim, moving around, buffer
1, list all buffer
:files[!]
:buffers[!]
:ls[!]
		When the [!] is included the list will show unlisted buffers
		(the term "unlisted" is a bit confusing then...).
		Each buffer has a unique number.  That number will not change,
		so you can always go to a specific buffer with ":buffer N" or
		"N CTRL-^", where N is the buffer number.

2, jump
buffer N
N CTRL-^

15:59 2012-11-08
virtualization, libvirt, libxl; work
1, Jim_email_"Work items for next week"_20121108_1405
Hi Bamvor and Chunyan,

Just a reminder about the libvirt tasks I want to discuss and work on
while in the Beijing office next week.

1. Bamvor's lock patchset for libxl driver.
   Thanks Bamvor for posting a v2 of your patches upstream.  After more
   review and testing today, it seems the patches are not completely
   independent of libxl versions, thus we should rework the patches after
   converting the driver to Xen 4.2 libxl.  I replied upstream with a
   similar comment.  Do you agree that we should convert the driver to
   Xen 4.2 libxl before merging the lock patches upstream?

2. Convert libxl driver to Xen 4.2 libxl.
   This will be our primary task.  It is essential to support Xen 4.2
   libxl since it is now the primary toolstack in Xen 4.2.  Upstream
   libvirt has agreed to drop libxl support for Xen 4.1, allowing us to
   convert the driver to Xen 4.2 libxl and ignore Xen 4.1 libxl.  Ondrej
   has started some work on the conversion and I will pick up his patches
   tomorrow and continue with the effort.  Feel free to take a look and
   become familiar with his initial work in

https://build.suse.de/package/show?package=libvirt&project=home%3Aoholecek%3Abranches%3ADevel%3AVirt%3ASLE-11-SP3

   The patches of interest are libxl_to_xen_42.patch and
   libxl_compat.patch.

3. Migration patch for Xen 4.2 libxl.
   No need to continue with the migration patch for Xen 4.1.  We need to
   make this patch work with the converted Xen 4.2 libxl driver.

4. Feature parity with legacy Xen driver.
   At a minimum, we will need to have the libxl driver support all
   functions currently implemented in the legacy Xen driver.

5. Support for PCI passthrough in legacy xen driver
   We will still be using the xm/xend toolstack in SLES11 SP3, but
   mandatory fate #313570 requests that libvirt support PCI passthrough
   for Xen as it does for KVM.  Unfortunately, this is a feature we'll
   have to implement twice for Xen; first in the legacy driver to satisfy
   fate#313570, second in the Xen 4.2-compatible libxl driver.

I know that both of you have already been working on some of these tasks
or are otherwise familiar with them, but please be prepared to start
digging in on Monday when I show up in the office.

Thanks and see you next week!
Jim
2, reply to Jim
agree. i continue reading xen4.2 code this week. i found that the code for save, migrate and restore changes a lots, the core function for these api is called by subprocess. at this point, restore is a async job. 
meanwhile, i checkout the Ondrej's branch which build fail. i want to try to fix compile error and read the event handling relative code then improve lock patch based on it. 

17:42 2012-11-08
build service, how to add package to in build service
1, "Marcus Schäfer<ms@suse.de>"_email_"Re: [opensuse-arm] ARM images fail"_20121108_1624
> Should "vgs" be installed elsewhere?
ah ok now I understand. the buildservice should install lvm2 as part of
the buildsystem. I think that can be achieved if you change your prjconf
   osc meta -e prjconf
and add something like
   requires lvm2
I'm not 100% shure but Adrian knows it definitly :)
CC'ed him
2, bamvor: Adrian Schroeter

17:52 2012-11-08
virtualization, xen, arm server
1, xen arm website
http://xen.org/products/xen_arm.html
2, Xen ARMv7 with Virtualization Extensions
http://wiki.xen.org/wiki/Xen_ARMv7_with_Virtualization_Extensions
FAQ: http://wiki.xen.org/wiki/Xen_ARMv7_with_Virtualization_Extensions/DevFAQ
1), ARM Fast Model
Either the v7-A ARM Envelope Model (AEM) or the Cortex A15/A7 models (single CPU only). 
2), Device Tree
A device tree in the flat device tree format (.dtb). The device tree source for the AEM is here: git://xenbits.xen.org/people/dvrabel/device-trees.git. 
3), Xen
While xen-unstable is frozen for the 4.2 release ARM patches are being collected in the arm-for-4.3 branch of this git tree. This tree is based upon Anthony Perards conversion to git. 
4), Linux kernel for dom0
3.5-rc7-arm-2 branch of git://xenbits.xen.org/people/sstabellini/linux-pvhvm.git
5), Cross compile
http://www.kernel.org/pub/tools/crosstool/files/bin/x86_64/4.6.0/x86_64-gcc-4.6.0-nolibc_arm-unknown-linux-gnueabi.tar.bz2
4, Xen ARM in Linux! 
http://blog.xen.org/index.php/2012/10/08/xen-arm-in-linux/
5, XenSummit Sessions: New PVH virtualisation mode for ARM Cortex A15/ARM Servers and x86
http://blog.xen.org/index.php/2012/09/21/xensummit-sessions-new-pvh-virtualisation-mode-for-arm-cortex-a15arm-servers-and-x86/
6,Xen hypervisor ported to ARM chips 
Twice, in fact, and KVM in the works, too 
http://www.theregister.co.uk/2011/11/30/xen_kvm_hypervisor_for_arm_chips/

18:24 2012-11-08
http://wiki.xen.org/wiki/Choice_of_Toolstacks
libxl Design Principles

    Implement mechanisms, not policies
    Stateless
    Hide xenstore, libxenctrl, and libxenguest from higher levels
    Be as simple as possible 

21:39 2012-11-08
virtualization, libvirt, xen, xenlight, libxl
发现自己今年真的是挺晕的, 之前发libvirt patch的人是Daniel De Graaf <dgdegra@redhat.com>, 之前不知道他是redhat的人.
那个是否如果照着人家的patch接着改, 估计怎么也改完了. 
这事儿我还误导jiaju了.

23:50 2012-11-8
http://blog.xen.org/index.php/2012/10/31/the-paravirtualization-spectrum-part-2-from-poles-to-a-spectrum/
The pagetable protections protect both the guest kernel and Xen from userspace; the segmentation limits protect Xen from the guest kernel.

Unfortunately, at the time that Xen team was developing clever new uses for this little-used feature, AMD was designing their 64-bit extensions to the x86 architecture. Any unused processor feature makes hardware much more complicated to design, reason about, and verify. Since basically no operating systems use segmentation limits, AMD decided to get rid of them.

In 64-bit HVM mode, the problem doesn’t occur. The HVM extensions make it easy to have three different protection levels without needing to play clever tricks with little-used processor features. So making system calls in 64-bit HVM mode is just as fast as on real hardware. For this reason, a lot of people began running 64-bit Linux in fully virtualized mode.

But fully virtualized mode, even with PV drivers, has a number of things that are unnecessarily inefficient. One example is the interrupt controllers: fully virtualized mode provides the guest kernel with emulated interrupt controllers (APICs and IOAPICs). Each instruction that interacts with the APIC requires a trip up into Xen and a software instruction decode; and each interrupt delivered requires several of these emulations.

So Stefano Stabellini wrote some patches for the Linux kernel that allowed Linux, when it detects that it’s running in HVM mode under Xen, to switch from using the emulated interrupt controllers and timers to the paravirtualized interrupts and timers. This mode he called PVHVM mode, because although it runs in HVM mode, it uses the PV interfaces extensively.

(“PVHVM” mode should not be confused with “PV-on-HVM” mode, which is a term sometimes used in the past for “fully virtualized with PV drivers”.)

With the introduction of PVHVM mode, we can start to see paravirtualization not as binary on or off, but as a spectrum. In PVHVM mode, the disk and network are paravirtualized, as are interrupts and timers. But the guest still boots with an emulated motherboard, PCI bus, and so on. It also goes through a legacy boot, starting with a BIOS and then booting into 16-bit mode. Privileged instructions are virtualized using the HVM extensions, and pagetables are fully virtualized, using either shadow pagetables, or the hardware assisted paging (HAP) available on more recent AMD and Intel processors.
