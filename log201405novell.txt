.LOG
11:01 2014-05-04
On 22.04.14 11:33, Chun Yan Liu wrote:
> Hello, Alex,
>
> This is Chunyan. Currently I'm working on xen HVM direct kernel boot and encounter some qemu questions. I found you have some patches regarding to that before, so just send mail for some knowledge.
>
> I know KVM supports direct kernel boot but not quite clear how it's implemented. In pc_memory_init(), I can see bochs_bios_init() and load_linux(). I think I can understand load_linux(). It reads kernel and initrd to specific addresses, and put linuxboot.bin to option rom which override int19 to intercept BIOS process. linuxboot.bin can then goto 'kernel' address to execute. But I don't quite understand bochs_bios_init(), seems it only registers some things to FW_CFG (?) So, when is BIOS loaded?

The BIOS gets loaded directly into the guest's address space on machine
init. -kernel and -initrd however only get loaded into blobs in fw_cfg
that the linuxboot.bin file pull from fw_cfg during runtime.

> For Xen, hvmloader provides BIOS, which is loaded to RAM in xen code, and will probe disk MBR for kernel and initrd. So in SLE11 SP2, to support xen hvm direct kernel boot, we make a different load_linux function, which read kernel and initrd and generate a boot sector in the head of first disk. But surely that's not acceptable for upstream. Now, I wonder if we can reuse qemu code. If BIOS can be loaded in qemu for xen, then maybe I can remove hvmloader from xen code (don't load it at all), and in qemu side, let it do pc_memory_init() also for xen case. Don't know if there is other blocks, but can try.

I don't see why this wouldn't work, yeah. I think it makes a lot of sense.

Alex

16:53 2014-05-04
http://wiki.qemu.org/QMP#By_hand

17:25 2014-05-04
company, reserve machine
orthos.arch.suse.de

17:53 2014-05-04
company, email, client
http://147.2.207.240/gw_mail.html
Server Name: imap.novell.com Port: 993
User Name: your novell ID
Connection security: STARTTLS
Outgoing Server Name: smtp.novell.com Port:25

13:54 2014-05-05
software skill, gdb, signal, ignore
http://www.cppblog.com/elva/archive/2011/11/01/159457.html
handle SIGUSR1 nostop noprint

16:30 2014-05-05
bnc: bugzilla@novell.com

16:34 2014-05-05
(10:17 2014-05-06)
(15:00 2014-05-09)
1,
Hi, Ian

thanks your reply. sorry reply late.
>
> On Fri, 2014-04-18 at 04:10 -0600, Bamvor Jian Zhang wrote:
> > Hi,
> >
> > here is the second version about vm snapshot documents, the first version is
> > here[1]
>
> Have you considered attending the hackathon and proposing this as a
> topic? See http://wiki.xen.org/wiki/Hackathon/May2014 for details, there
> are not a lot of places remaining...
>
> > thanks Ian Jackson suggestion, i write this doc about vm snapshot including
> > new
> > command, struct and api. currently, the disk snapshot(create, delete, list)
> > worked(patch is already sent[2]), and vm snapshot(create, delete, list) work.
> > but i have some questions about snapshot apply(see the last section).
> >
> > feel free to comment it. thanks.
> >
> > 1, new command
> > =head1 SNAPSHOT
> >
> > there are two types of snapshots supported by libxl: disk snapshot and vm
> > snapshot. The following subcommands management the snapshot of a domain,
> > including create, delete, list and apply.
> > a domain snapshot(or a vm snapshot) means save the domain config, memory and
> > disk snapshot. currently, only support qcow2 and internal disk snapshot.
> >
> > the vm snapshot information will store in the follow path:
> > /var/lib/xen/snapshots/<domain_uuid>/snapshotdata-<snapshot_name>.xl
> >
> > here is an example for snapshot information file:
> > snapshot_name="1397207577"
> > snapshot_creationtime="1397207577"
> > snapshot_save="/var/lib/xen/snapshots/5c84adcc-bd59-788a-96d2-195f9b599cfe/1397207577.save"
> >
> > the user could give a snapshot name when vm snapshot created. if not, the
> > epoch
> > seconds will set as name as the above examples.
>
> This all sounds like internal libxl implementation detail. What might be
> interesting to see would be the libxl library API for causing this stuff
> to happen.
yes, lots of function is implemented in libxl. except the top level of vm
snapshot. the latter one is implemented in the xl_cmdimpl.c.
>
> At an implementation level I think this could profitably use the
> autogenerated json generators and parsers which the libxl IDL can
> provide (Wei is working on the parsing side now).
the struct is generated by libxl IDL. meanwhile for the snapshot information
file, i use the xlu_cfg_get_xxx for parse the file. i write the file with
my own function. is there some apis for generating xen config like file?
>
> > =over 4
> >
> > =item B<vm-snapshot-create> [I<OPTIONS>] I<domain-id>
> >
> > create vm snapshot.
>
> Which aspects of the VM are snapshotted? Disk you mention. What about
> memory?
vm snapshot include disk and memory.
>
> > it will call the qmp transaction for creating the disk snapshot in order to
> > ensure the disk snapshot is coherence.
>
> I'm not sure if you intend this document to literally become the xl
> manpage at some point or if it is an outline with implementation notes
> sprinkled around.
>
> If this is the manpage then talking about qmp (an implementation detail)
> isn't appropriate here.
sorry for confuse. i want to discuss the implementation. i will write the
dedicated man page with my code.
>
> If this is a implementation note then
yes, it is an implementation notes.
> it is worth considering non-qdisk
> backed snapshots and/or the mechanisms for configuring a domain such
> that it is "snapshotable" (which might for instance put constraints on
> which disk backend is used).
yes, it is worth, but i am not how to deal with non-qdisk backed. currently,
for the qdisk backed: if all the image in a domain support snapshot(qcow2,
qed?), the code support both internal and external snapshot. for others (e.g.
the domain includes raw image disk), only external snapshot is supported.
could i only consider qdisk right now?
>
> > vm is paused during snapshot create, and is unpause after take snapshot
> > finished.
>
> Is there any possibility of a live snapshot?
compare with libvirt qemu case, i guess the live snapshot is live memory save
and external disk snapshot.
> (Might make sense in the
> context of snapshotting memory at the same time)
do you mean do the disk snapshot with memory save at the same time?
>
> > =item B<vm-snapshot-apply> [I<OPTIONS>] I<domain-id>
> >
> > apply vm snapshot for the dedicated domain.
>
> What does "apply" mean here? DO you mean some sort of rollback for a
> running domain? Or just "start this snapshot as a fresh domain"?
it called revert in libvirt. currently, i implement the "apply" for the latter
one. which one do you prefer? in qemu, it seems a rollback. but it will lead
to do more about restore domain memory, i mean i need to write a function like
create_domain(xl_cmdimpl.c) without domain id changes.
>
> It's a bit unusual for an xl level API to provide this sort of
> management of a list of resources. Would it make more sense to structure
> this more like save/restore (where the user provides the filename)?
if i do not save the snapshot list for such domain, i could not provide the vm
snapshot list command in xl level.
>
> That doesn't constrain other toolstack from providing higher level
> management functionality, providing you design the libxl layer
> correctly.
>
> > =item B<disk-snapshot-create> [I<OPTIONS>] I<domain-id>
> >
> > create disk snapshot.
>
> Hrm. so I got the impression that vm-snapshot was creating disk
> snapshots too. Maybe one is a subset of the other?
yes. i disk-snapshot will be the subset of vm snapshot. and i will add
the --disk-only parameter in vm snapshot command.
>
> Since this is described here in terms of xl and not libxl then how do
> you envisage this working in the absence of any kind of "storage
> manager" like functionality, e.g. describe where the snapshots disks
> should go etc.
for internal snapshot, there is no need to provide the filename.
for external snapshot, the user should provide the filename.
>
> > 2, new struct and new api
>
> It would have been less confusing for me if you would have started at
> the low level (libxl) and then done the higher level stuff (xl) rather
> than vice versa.
thanks your advice. i will rewrite the whole doc to make it more clearer.
>
> > 1), new struct
> > (1), libxl_snapshot struct store a disk snapshot information, which get from
> > qcow2 image through "query-block" qmp command.
>
> This all seems very specific to a particular backend. The API should be
> designed so that it can work with multiple backends, even if the
> implementation only supports qdisk at first.
yeah, understand. these struct are only used by disk snapshot list. it is
useless if we do not need the dedicate disk snapshot list. in fact, the user
could get it with qemu-img command for qcow2.
>
> > libxl_snapshot = Struct("snapshot",[
> >     ("device",        string),
>
> What is this? Which device?
the disk device in vm, such as sda, hda...
>
> >     ("name",          string),
> >     ("id",            string),
>
> ID of what?
the snapshot ID. it is a unique sequence number of each disk snapshot file.
it will be created by qemu when taking disk snapshot.
>
> >     ("vm_state_size", uint64),
> >     ("date_sec",      uint64),
> >     ("date_nsec",     uint64),
> >     ("vm_clock_sec",  uint64),
> >     ("vm_clock_nsec", uint64),
>
> Sounds like we need a time and/or date meta types.
>
> > (2), libxl_vm_snapshot store vm snapshot information which store in the path
> > shown above. i add some api for create, delete and list these information.
> > at first, i want to add these information to xenstore, but it will lose when
> > xenstore reboot or dom0 reboot.
> >
> > libxl_vm_snapshot = Struct("vm_snapshot",[
> >     ("name",          string),
> >     ("creation_time", uint64),
> >     ("save",          string),
>
> What is "save"?
the image path for save vm memory. it is named snapshot_save in snapshot file.
as i wrote above, the path will be
snapshot_save="/var/lib/xen/snapshots/<uuid>/<snapshot-name>.save"
>
> >     ])
> >
> > 2), new api
> > (1), in libxl/libxl.h
> > /* disk snapshot api
> >  * support create, delete and list for internal snapshot of a single disk
> >  * only support internal snapshot rigt now.
> >  */
> > /* create disk snapshot according to the device name in snapshot array. nb is
> >  * the number of snapshot array.
>
> So the user must supply one of these per guest disk?
yes, if user want to take specific disk snapshot. meanwhile as you Ian Jackson
said, maybe we do not need export these disk snapshot operation for users.
>
> Is libxl_snapshot really libxl_disk_snapshot?
yes.
>
> Should there be some mechanisms to convert from libxl_disk_snapshot to a
> libxl_device_disk so that it can then be attached to a domain? Either a
> utility function or there should be a field in the snapshot.
yeah, i have a api for convert from libxl_device_disk to libxl_disk_snapshot.
it will be called in vm snapshot command.
>
> >  * use the qmp transaction to ensure all snapshot of disk is coherence.
> >  */
> > int libxl_disk_snapshot_create(libxl_ctx *ctx, int domid,
> >                                libxl_snapshot *snapshot, int nb);
>
> None of these seem to have support for async. What if the disk backend
> might take a few seconds to take a snapshot.
>
> (as a thought experiment you might consider taking a snapshot with "dd")
thanks, do you mean ao in create and save?
>
> > /* delete number of nb disk snapshot describe in snapshot array
> >  */
> > int libxl_disk_snapshot_delete(libxl_ctx *ctx, int domid,
> >                                libxl_snapshot *snapshot, int nb);
> > int libxl__disk_snapshot_delete(libxl_ctx *ctx, int domid,
> >                                 libxl_snapshot *snapshot);
> > /* apply the disk snapshot by qemu-img command
>
> There's that "apply" again. What does it mean?
revert. reference my comment above.
>
> >  */
> > int libxl_disk_snapshot_apply(libxl_ctx *ctx, int domid,
> >                               libxl_snapshot *snapshot, int nb);
> > /* list disk snapshot by qmp query-block command
> >  */
> > int libxl__disk_snapshot_list(libxl_ctx *ctx, int domid,
> >                               libxl_snapshot **snapshotp);
>
> I think the resource management aspect here needs careful thought. i.e.
> the management of the backing storage, should it be the responsibility
> of libxl or the toolstack (xl, libvirt) etc.
>
> I have a feeling that libxl should be providing the mechanisms but the
> actual management (i.e. lists of snapshots) will be the toolstack's
> doing -- i.e. doesn't libvirt already have a storage and snapshot
> manager?
libvirt already have such snapshot manager: user need provide filename for
external snapshot file. so as to xl.
for other resource do not provided by libvirt, libxl should manage it.
e.g. the memory file saved by xen, should managed by libxl as i mentioned in
snapshot information file.
>
> > (2), libxl/xl_cmdimpl.c
> > /* get disk device name (hda, hdc..) and image path through
> >  * libxl_device_disk_list and libxl_device_disk_getinfo
> >  */
> > static int get_disk(uint32_t domid, libxl_snapshot **snapshotp, char *name);
>
> What is this?
as we talk above, it convert libxl_device_disk to libxl_disk_snapshot for
domain.
>
> > 3, question
> > 1), how to do disk snapshot apply?
> > when i apply the vm snapshot, i need to revert the disk snapshot. in qemu, it
> > is
> > done by bdrv_snapshot_goto api. this api is only called by loadvm hmp or
> > qemu-img commands.
> > there is no hmp api in libxl. so the only choice is using qemu-img command.
>
> "hmp"? Did you mean "qmp"? I don't think so because there is definitely
> a qmp api in libxl, but I can't htink of anything else.
hmp is another channel for qemu operation. it could be called through qmp i
know this after i send this email.
there are loadvm and savevm hmp in qemu, libxl. it is useful for doing the
internal snapshot. savevm means save qemu memory and do internal disk snapshot.
and loadvm mean revert the snapshot state the one of savevm state.
but it will almost save qemu state twice if the vm snapshot create flow is as
follows:
1, save memory, the core function will be libxl_domain_suspend. it will save
domain memory and qemu state.
after memory save, the domain should remain pause for savevm.
2, call savevm hmp through qemu qmp. the save vm will save qemu memory
(including qemu state saved by step 1) and do internal disk snapshot.

the qemu state is very small, it will not waste a lot. is it ok?

>
> > i know usually i should call qmp for qemu operation. but there is not qmp at
> > the moment, could i call qemu-img in libxl for disk snapshot apply?
>
> In general I think it would be useful to go back and define what you
> actually mean by all these things at a higher semantic layer. i.e. what
> does it mean to "apply" a disk snapshot?
yes, it seems that i need write a new version of this doc for review.
i will update api, struct and implementation details for distinguish
non-live, live snapshot, internal and external snapshot.
meanwhile, IIUC, libvirt/qemu only support revert the non-live, internal
snapshot revert. could i fosus on non-live internal snapshot operations for
the first version of code in next steps?

regards

bamvor

>
> Ian.

2, live snapshot
1), check the status in libvirt
only external checkpoints supported by live snapshot.

09:41 2014-05-06
GTD
0, 9:30

1, today
1), reply Ian C. see"16:34 2014-05-05"
2), -14:45. personal.
3), go to Xiaozhuang Hospital.
4), 电热水器。

17:06 2014-05-06
software skill, glibc, "corrupted double-linked list"
1,
struct malloc_chunk {

  INTERNAL_SIZE_T      prev_size;  /* Size of previous chunk (if free).  */
  INTERNAL_SIZE_T      size;       /* Size in bytes, including overhead. */

  struct malloc_chunk* fd;         /* double links -- used only if free. */
  struct malloc_chunk* bk;

  /* Only used for large blocks: pointer to next larger size.  */
  struct malloc_chunk* fd_nextsize; /* double links -- used only if free. */
  struct malloc_chunk* bk_nextsize;
};

#define unlink(P, BK, FD) {                                            \
    FD = P->fd;                                   \
    BK = P->bk;                                   \
    if (__builtin_expect (FD->bk != P || BK->fd != P, 0))             \
      malloc_printerr (check_action, "corrupted double-linked list", P);      \
    else {                                    \
        FD->bk = BK;                                  \
        BK->fd = FD;                                  \
...

17:16 2014-05-06
1, Lin Ma
1), oss test
installation is almost ok.
2), virt-test.

15:50 2014-05-07
snapshot
1, "David kiarie <davidkiarie4@gmail.com>"_email_"libxl snapshot"
cc jim

Hi, David

nice to meet you.
> Hi Bamvor,
>
> I am a Kenyan student participating in GSoC.I am the same student who
> had proposed a xen project to implement snapshot support in libxl but > I gave up on that.
> I got seleted in GSoC to add snapshot support to libvirt xenlight
> driver.Anyway that is just a trick I did to get selected.
>
> Well,..to implement the snapshot support in libvirt xenlight driver I
> need the support from libxl.I have been following your work on the
> snapshot support.I wish to help
> so that we can finally finish this and I can move off to libvirt.
>
> I was wishing you could let me in on your work on the vm_snapshots.I
> am just trying to avoid both of us having to do the same work over
> again.
we have discuss with Jim about this GSOC project month ago. from my point of
view, we could work on libxl and libvirt driver respectively. and what we need
is discuss the interface between them. and this is actually a blocker for
snapshot in libxl as you may see in the xen-devel. and after these interface
and struct is confirmed, you could work on libvirt driver, you know, discuss,
coding, review, it might be takes a long time.
after your libvirt driver is in good shape, we could discuss this again. there
are lots of advanced feature need to do or corner need to be tested in libxl
side.

regards

bamvor
>
> Regards,
> David,

2,
> On Wed, Apr 23, 2014 at 1:18 PM, Bamvor Jian Zhang <bjzhang@suse.com> wrote:
> > Hi,
> >
> >  >>>Jim Fehlig <jfehlig@suse.com> wrote:
> >> Bamvor Jian Zhang wrote:
...
> >> > 2, new struct and new api
> >> > 1), new struct
> >> > (1), libxl_snapshot struct store a disk snapshot information, which get from
> >> > qcow2 image through "query-block" qmp command.
> >> >
> >> > libxl_snapshot = Struct("snapshot",[
> >> >     ("device",        string),
> >> >
> >>
> >> Should this be libxl_device_disk instead of a string? And should
> >> multiple be supported, allowing control over which of a domain's disks
> >> are included in the snapshot?
> > one libxl_snapshot represent one disk. see my comment below.
> >>
> >> "description" would be helpful.
> >>
> >> >     ("id",            string),
> >> >     ("vm_state_size", uint64),
> >> >     ("date_sec",      uint64),
> >> >     ("date_nsec",     uint64),
> >> >     ("vm_clock_sec",  uint64),
> >> >     ("vm_clock_nsec", uint64),
> >> >
> >>
> >> A lot of these are read-only and wouldn't be provided when calling
> >> libxl_snapshot_create(). I think adding the domain state at time of
> >> snapshot would be useful.
> > do you mean the stopped or running state for a domain?
> >> There are probably other items I'm not
> >> considering atm, which makes me nervous about future extensibility.
> > so, how about this?
> >  libxl_vm_snapshot = Struct("vm_snapshot",[
> >      ("name",          string),
> >      ("creation_time", uint64),
> >      ("save",          string),
> > +    ("state",         string),
> > +    ("disks", Array(libxl_snapshot, "num_disks")),
> >      ])
> >
> > BTW: maybe i should rename libxl_snapshot to libxl_disk_snapshot?
> >
> >>
> >> > (2), libxl_vm_snapshot store vm snapshot information which store in the path
> >> > shown above. i add some api for create, delete and list these information.
> >> > at first, i want to add these information to xenstore, but it will lose when
> >> > xenstore reboot or dom0 reboot.
> >> >
> >> > libxl_vm_snapshot = Struct("vm_snapshot",[
> >> >     ("name",          string),
> >> >     ("creation_time", uint64),
> >> >     ("save",          string),
> >> >     ])
> >> >
> >> > 2), new api
> >> > (1), in libxl/libxl.h
> >> > /* disk snapshot api
> >> >  * support create, delete and list for internal snapshot of a single disk
> >> >  * only support internal snapshot rigt now.
> >> >  */
> >> > /* create disk snapshot according to the device name in snapshot array. nb is
> >> >  * the number of snapshot array.
> >> >  * use the qmp transaction to ensure all snapshot of disk is coherence.
> >> >  */
> >> > int libxl_disk_snapshot_create(libxl_ctx *ctx, int domid,
> >> >                                libxl_snapshot *snapshot, int nb);
> >> > /* delete number of nb disk snapshot describe in snapshot array
> >> >  */
> >> > int libxl_disk_snapshot_delete(libxl_ctx *ctx, int domid,
> >> >                                libxl_snapshot *snapshot, int nb);
> >> > int libxl__disk_snapshot_delete(libxl_ctx *ctx, int domid,
> >> >                                 libxl_snapshot *snapshot);
> >> > /* apply the disk snapshot by qemu-img command
> >> >  */
> >> > int libxl_disk_snapshot_apply(libxl_ctx *ctx, int domid,
> >> >                               libxl_snapshot *snapshot, int nb);
> >> > /* list disk snapshot by qmp query-block command
> >> >  */
> >> > int libxl__disk_snapshot_list(libxl_ctx *ctx, int domid,
> >> >                               libxl_snapshot **snapshotp);
> >> >
> >> > /* vm snapshot api
> >> >  */
> >> > /* write vm snapshot information to file
> >> >  */
> >> > int libxl_vm_snapshot_add(libxl_ctx *ctx, uint32_t domid,
> >> >                           libxl_vm_snapshot *snapshot,
> >> >                           const libxl_asyncop_how *ao_how)
> >> >                           LIBXL_EXTERNAL_CALLERS_ONLY;
> >> > /* delete vm snapshot information file
> >> >  */
> >> > int libxl_vm_snapshot_remove(libxl_ctx *ctx, uint32_t domid,
> >> >                              libxl_vm_snapshot *snapshot,
> >> >                              const libxl_asyncop_how *ao_how)
> >> >                              LIBXL_EXTERNAL_CALLERS_ONLY;
> >> > /* list vm snapshot from file
> >> >  */
> >> > libxl_vm_snapshot *libxl_vm_snapshot_list(libxl_ctx *ctx, uint32_t domid,
> >> >                                           int *num)
> >> >                                           LIBXL_EXTERNAL_CALLERS_ONLY;
> >> > /* read vm snapshot information from dedicated file
> >> >  */
> >> > int libxl_vm_snapshot_getinfo(libxl_ctx *ctx, uint32_t domid,
> >> >                               libxl_vm_snapshot *snapshot);
> >> > /* delete vm save image */
> >> > int libxl_vm_snapshot_delete_save_image(libxl_ctx *ctx,
> >> >                                         libxl_vm_snapshot *snapshot);
> >> >
> >>
> >> IMO, these too should be combined into libxl_snapshot_*(), with disk vs
> >> domain snapshot controlled via a flags argument.
>
> Yes, I also think so.
> I was also thinking that one struct could be used to represent all
> snapshot(vm and disk) information.
except the id/name in vm, disk, there is no common information for vm and disk.
i am not sure how do you want to do, maybe you could give a example?

16:26 2014-05-08
software, filesystem, btrfs, compress
1, mount -o compress=lzo /dev/host/btrfs /mnt
2, 2014-05-08T16:07:40.823462+08:00 linux-3lk1 kernel: [ 9169.908179] btrfs: compression is not supported, load module with allow_unsupported=1
3, re insert btrfs module:
rmmod btrfs
modprobe btrfs allow_unsupported=1
4, linux-3lk1:~ # mount |grep mnt
/dev/mapper/host-btrfs on /mnt type btrfs (rw,relatime,compress=lzo,space_cache)

12:03 2014-05-09
1,
l3 workflow
l3support.suse.de

l3doc.suse.de
https://wiki.innerweb.novell.com/index.php/SUSE/Labs_Publications/Linux_Kernel_Maintainers

2, support the last service pack or buy dedicate ltss.

3, btrfs
1), extent based filesystem.
ext2/ext3: block based. use bitmap and point to block. 文件和inode数量有限制.
2), 64bit.
3), snapshot.
4), subvolume.
5), checksums on data and metadata(crc32c).
6), SSD awareness.
mount -t btrfs -o SSD /dev/sda5 /btrfsdisk
7), support software RAID.
mkfs.btrfs -d raid1 /dev/sda6 /dev/sda7
mount -t btrfs /dev/sda6 /btrfsdisk
8), from celen
btrfs balance

4, Hao Junwei
1), tourist
Albrecht Dürer
2, Prague
1), SLEPOS
https://wiki.innerweb.novell.com/index.php/SLEPOS

10:28 2014-05-12
GTD
0, 10:00

1, today
1), -10:42 misc stuff.
2), 2' 14:09-14:37 reply to David kiakie.
3), 11:30 13:52- write snapshot doc.
4), 11:30-12:45 lunch.
LWN done.
5), -13:52 misc.
install sles12 beta6 candidate.
6), 17:45- bios. see"18:00 2014-05-12"

2, todo
work report.

14:09 2014-05-12
snapshot, gsoc, xen
Hi, david

> Am  running the the unstable version from xen.git.
sorry for confuse. but i want to know the exact version for your xen and
toolstack. you could get xen hypervior version from xl dmesg or sysfs, here is
my version(it is a stable version, not for testing).
get xen hypervisor version from xl:
# xl dmesg | head
 __  __            _  _    ____    _____    ___  _    _   ____  ____   _  _
 \ \/ /___ _ __   | || |  |___ \  |___ /   / _ \/ |  / | |___ \|___ \ | || |
  \  // _ \ '_ \  | || |_   __) |   |_ \  | | | | |__| |   __) | __) || || |_
  /  \  __/ | | | |__   _| / __/ _ ___) | | |_| | |__| |_ / __/ / __/ |__   _|
 /_/\_\___|_| |_|    |_|(_)_____(_)____/___\___/|_|  |_(_)_____|_____(_) |_|
                                      |_____|
(XEN) Xen version 4.2.3_01-1.22.4 (abuild@) (gcc (SUSE Linux) 4.7.2 20130108 [gcc-4_7-branch revision 195012]) Mon Dec  2 12:22:01 UTC 2013
(XEN) Latest ChangeSet: 26064
(XEN) Bootloader: GRUB2 2.00
(XEN) Command line:

get xen version from sysfs:
/sys/hypervisor/version # cat major
4
/sys/hypervisor/version # cat minor
2
/sys/hypervisor/version # cat extra
.3_01-1.22.4

>
> Am suspecting a supporting module is not loaded.Am using a SUSE kernel-xen
yes, i think so. meanwhile i guess the mismatch of xen hypervisor and toolstack
 could be the root cause which lead to xen module load fail.
usually, we need to update hypervisor and toolstack at the same time.
> uname -r
>
> 3.11.10-7-xen
>
> cat /boot/config-$(uname -r)* | grep XEN
>
> CONFIG_X86_64_XEN=y
...
> CONFIG_XEN_DEV_EVTCHN=m
...
> CONFIG_XEN_PRIVCMD=y
>
> I see that xen-evtchn is a module but I don't think it is getting loaded.
>
>
> I try to probe it
>
> #modprobe xen-evtchn
> #FATAL :module xen-evtch not found
FYI: you may be already know it. there is a script for managing xen start, stop:
"/etc/init.d/xencommons"

>
>
> Can I recompile this kernel?
of course u could. but i am not sure if u really needed.
in your kernel configuration, EVTCHN is compiled as module. it should be there.
in my system(opensuse 12.3 with kernel 3.7) the module is named evtchn:
> lsmod |grep evt
evtchn                 13162  1

and it located at
/lib/modules/3.7.10-1.16-xen/kernel/drivers/xen/evtchn.ko
are you look into the xen directories before asking?

here is my system version:
> uname -a
Linux laptop-work.site 3.7.10-1.16-xen #1 SMP Fri May 31 20:21:23 UTC 2013 (97c14ba) x86_64 x86_64 x86_64 GNU/Linux

>
> On Mon, May 12, 2014 at 5:45 AM, Bamvor Jian Zhang <bjzhang@suse.com> wrote:
> > Hi,
> >
> >  >>>David kiarie <davidkiarie4@gmail.com> wrote:
> >> Could you know why am running into this?
> > which version of xen hypervisor do you use?
> > different version between xen and toolstack may lead to this issue.
> >
> > BTW: you could cc me and Jim when you send mail to xen-users.
> >
regards

bamvor

18:00 2014-05-12
virtualization, xen, hvmloader, bios
1, http://people.cs.nctu.edu.tw/~huangmc/works/web/Boot_x86/Boot_x86.html
電源正常啟動後，x86 CPU 會先執行 0xFFFF0，也就是 BIOS ROM 的進入點。由於 0xFFFF0 ~ 0xFFFFF 只有少的很可憐的 16 bytes，真正的 BIOS code 勢必要擺到其他位置，此時 0xFFFF0 的作用便是 jmp 到該位置執行 BIOS 程式。

2, xen/tools/firmware/hvmloader/hvmloader.c
    /* Enter real mode, reload all segment registers and IDT. */
    "    ljmp $"STR(SEL_CODE16)",$0x0\n"
    "trampoline_start: .code16       \n"
    "    mov  %eax,%cr0              \n"
    "    ljmp $0,$1f-trampoline_start\n"
    "1:  mov  %ax,%ds                \n"
    "    mov  %ax,%es                \n"
    "    mov  %ax,%fs                \n"
    "    mov  %ax,%gs                \n"
    "    mov  %ax,%ss                \n"
    "    lidt 1f-trampoline_start    \n"
    "    ljmp $0xf000,$0xfff0        \n"
    "1:  .word 0x3ff,0,0             \n"
    "trampoline_end:   .code32       \n"

3, So, how and when does hvmloader put bios to ffff0?

