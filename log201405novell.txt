.LOG
11:01 2014-05-04
On 22.04.14 11:33, Chun Yan Liu wrote:
> Hello, Alex,
>
> This is Chunyan. Currently I'm working on xen HVM direct kernel boot and encounter some qemu questions. I found you have some patches regarding to that before, so just send mail for some knowledge.
>
> I know KVM supports direct kernel boot but not quite clear how it's implemented. In pc_memory_init(), I can see bochs_bios_init() and load_linux(). I think I can understand load_linux(). It reads kernel and initrd to specific addresses, and put linuxboot.bin to option rom which override int19 to intercept BIOS process. linuxboot.bin can then goto 'kernel' address to execute. But I don't quite understand bochs_bios_init(), seems it only registers some things to FW_CFG (?) So, when is BIOS loaded?

The BIOS gets loaded directly into the guest's address space on machine
init. -kernel and -initrd however only get loaded into blobs in fw_cfg
that the linuxboot.bin file pull from fw_cfg during runtime.

> For Xen, hvmloader provides BIOS, which is loaded to RAM in xen code, and will probe disk MBR for kernel and initrd. So in SLE11 SP2, to support xen hvm direct kernel boot, we make a different load_linux function, which read kernel and initrd and generate a boot sector in the head of first disk. But surely that's not acceptable for upstream. Now, I wonder if we can reuse qemu code. If BIOS can be loaded in qemu for xen, then maybe I can remove hvmloader from xen code (don't load it at all), and in qemu side, let it do pc_memory_init() also for xen case. Don't know if there is other blocks, but can try.

I don't see why this wouldn't work, yeah. I think it makes a lot of sense.

Alex

16:53 2014-05-04
http://wiki.qemu.org/QMP#By_hand

17:25 2014-05-04
company, reserve machine
orthos.arch.suse.de

17:53 2014-05-04
company, email, client
http://147.2.207.240/gw_mail.html
Server Name: imap.novell.com Port: 993
User Name: your novell ID
Connection security: STARTTLS
Outgoing Server Name: smtp.novell.com Port:25

13:54 2014-05-05
software skill, gdb, signal, ignore
http://www.cppblog.com/elva/archive/2011/11/01/159457.html
handle SIGUSR1 nostop noprint

16:30 2014-05-05
bnc: bugzilla@novell.com

16:34 2014-05-05
(10:17 2014-05-06)
(15:00 2014-05-09)
1,
Hi, Ian

thanks your reply. sorry reply late.
>
> On Fri, 2014-04-18 at 04:10 -0600, Bamvor Jian Zhang wrote:
> > Hi,
> >
> > here is the second version about vm snapshot documents, the first version is
> > here[1]
>
> Have you considered attending the hackathon and proposing this as a
> topic? See http://wiki.xen.org/wiki/Hackathon/May2014 for details, there
> are not a lot of places remaining...
>
> > thanks Ian Jackson suggestion, i write this doc about vm snapshot including
> > new
> > command, struct and api. currently, the disk snapshot(create, delete, list)
> > worked(patch is already sent[2]), and vm snapshot(create, delete, list) work.
> > but i have some questions about snapshot apply(see the last section).
> >
> > feel free to comment it. thanks.
> >
> > 1, new command
> > =head1 SNAPSHOT
> >
> > there are two types of snapshots supported by libxl: disk snapshot and vm
> > snapshot. The following subcommands management the snapshot of a domain,
> > including create, delete, list and apply.
> > a domain snapshot(or a vm snapshot) means save the domain config, memory and
> > disk snapshot. currently, only support qcow2 and internal disk snapshot.
> >
> > the vm snapshot information will store in the follow path:
> > /var/lib/xen/snapshots/<domain_uuid>/snapshotdata-<snapshot_name>.xl
> >
> > here is an example for snapshot information file:
> > snapshot_name="1397207577"
> > snapshot_creationtime="1397207577"
> > snapshot_save="/var/lib/xen/snapshots/5c84adcc-bd59-788a-96d2-195f9b599cfe/1397207577.save"
> >
> > the user could give a snapshot name when vm snapshot created. if not, the
> > epoch
> > seconds will set as name as the above examples.
>
> This all sounds like internal libxl implementation detail. What might be
> interesting to see would be the libxl library API for causing this stuff
> to happen.
yes, lots of function is implemented in libxl. except the top level of vm
snapshot. the latter one is implemented in the xl_cmdimpl.c.
>
> At an implementation level I think this could profitably use the
> autogenerated json generators and parsers which the libxl IDL can
> provide (Wei is working on the parsing side now).
the struct is generated by libxl IDL. meanwhile for the snapshot information
file, i use the xlu_cfg_get_xxx for parse the file. i write the file with
my own function. is there some apis for generating xen config like file?
>
> > =over 4
> >
> > =item B<vm-snapshot-create> [I<OPTIONS>] I<domain-id>
> >
> > create vm snapshot.
>
> Which aspects of the VM are snapshotted? Disk you mention. What about
> memory?
vm snapshot include disk and memory.
>
> > it will call the qmp transaction for creating the disk snapshot in order to
> > ensure the disk snapshot is coherence.
>
> I'm not sure if you intend this document to literally become the xl
> manpage at some point or if it is an outline with implementation notes
> sprinkled around.
>
> If this is the manpage then talking about qmp (an implementation detail)
> isn't appropriate here.
sorry for confuse. i want to discuss the implementation. i will write the
dedicated man page with my code.
>
> If this is a implementation note then
yes, it is an implementation notes.
> it is worth considering non-qdisk
> backed snapshots and/or the mechanisms for configuring a domain such
> that it is "snapshotable" (which might for instance put constraints on
> which disk backend is used).
yes, it is worth, but i am not how to deal with non-qdisk backed. currently,
for the qdisk backed: if all the image in a domain support snapshot(qcow2,
qed?), the code support both internal and external snapshot. for others (e.g.
the domain includes raw image disk), only external snapshot is supported.
could i only consider qdisk right now?
>
> > vm is paused during snapshot create, and is unpause after take snapshot
> > finished.
>
> Is there any possibility of a live snapshot?
compare with libvirt qemu case, i guess the live snapshot is live memory save
and external disk snapshot.
> (Might make sense in the
> context of snapshotting memory at the same time)
do you mean do the disk snapshot with memory save at the same time?
>
> > =item B<vm-snapshot-apply> [I<OPTIONS>] I<domain-id>
> >
> > apply vm snapshot for the dedicated domain.
>
> What does "apply" mean here? DO you mean some sort of rollback for a
> running domain? Or just "start this snapshot as a fresh domain"?
it called revert in libvirt. currently, i implement the "apply" for the latter
one. which one do you prefer? in qemu, it seems a rollback. but it will lead
to do more about restore domain memory, i mean i need to write a function like
create_domain(xl_cmdimpl.c) without domain id changes.
>
> It's a bit unusual for an xl level API to provide this sort of
> management of a list of resources. Would it make more sense to structure
> this more like save/restore (where the user provides the filename)?
if i do not save the snapshot list for such domain, i could not provide the vm
snapshot list command in xl level.
>
> That doesn't constrain other toolstack from providing higher level
> management functionality, providing you design the libxl layer
> correctly.
>
> > =item B<disk-snapshot-create> [I<OPTIONS>] I<domain-id>
> >
> > create disk snapshot.
>
> Hrm. so I got the impression that vm-snapshot was creating disk
> snapshots too. Maybe one is a subset of the other?
yes. i disk-snapshot will be the subset of vm snapshot. and i will add
the --disk-only parameter in vm snapshot command.
>
> Since this is described here in terms of xl and not libxl then how do
> you envisage this working in the absence of any kind of "storage
> manager" like functionality, e.g. describe where the snapshots disks
> should go etc.
for internal snapshot, there is no need to provide the filename.
for external snapshot, the user should provide the filename.
>
> > 2, new struct and new api
>
> It would have been less confusing for me if you would have started at
> the low level (libxl) and then done the higher level stuff (xl) rather
> than vice versa.
thanks your advice. i will rewrite the whole doc to make it more clearer.
>
> > 1), new struct
> > (1), libxl_snapshot struct store a disk snapshot information, which get from
> > qcow2 image through "query-block" qmp command.
>
> This all seems very specific to a particular backend. The API should be
> designed so that it can work with multiple backends, even if the
> implementation only supports qdisk at first.
yeah, understand. these struct are only used by disk snapshot list. it is
useless if we do not need the dedicate disk snapshot list. in fact, the user
could get it with qemu-img command for qcow2.
>
> > libxl_snapshot = Struct("snapshot",[
> >     ("device",        string),
>
> What is this? Which device?
the disk device in vm, such as sda, hda...
>
> >     ("name",          string),
> >     ("id",            string),
>
> ID of what?
the snapshot ID. it is a unique sequence number of each disk snapshot file.
it will be created by qemu when taking disk snapshot.
>
> >     ("vm_state_size", uint64),
> >     ("date_sec",      uint64),
> >     ("date_nsec",     uint64),
> >     ("vm_clock_sec",  uint64),
> >     ("vm_clock_nsec", uint64),
>
> Sounds like we need a time and/or date meta types.
>
> > (2), libxl_vm_snapshot store vm snapshot information which store in the path
> > shown above. i add some api for create, delete and list these information.
> > at first, i want to add these information to xenstore, but it will lose when
> > xenstore reboot or dom0 reboot.
> >
> > libxl_vm_snapshot = Struct("vm_snapshot",[
> >     ("name",          string),
> >     ("creation_time", uint64),
> >     ("save",          string),
>
> What is "save"?
the image path for save vm memory. it is named snapshot_save in snapshot file.
as i wrote above, the path will be
snapshot_save="/var/lib/xen/snapshots/<uuid>/<snapshot-name>.save"
>
> >     ])
> >
> > 2), new api
> > (1), in libxl/libxl.h
> > /* disk snapshot api
> >  * support create, delete and list for internal snapshot of a single disk
> >  * only support internal snapshot rigt now.
> >  */
> > /* create disk snapshot according to the device name in snapshot array. nb is
> >  * the number of snapshot array.
>
> So the user must supply one of these per guest disk?
yes, if user want to take specific disk snapshot. meanwhile as you Ian Jackson
said, maybe we do not need export these disk snapshot operation for users.
>
> Is libxl_snapshot really libxl_disk_snapshot?
yes.
>
> Should there be some mechanisms to convert from libxl_disk_snapshot to a
> libxl_device_disk so that it can then be attached to a domain? Either a
> utility function or there should be a field in the snapshot.
yeah, i have a api for convert from libxl_device_disk to libxl_disk_snapshot.
it will be called in vm snapshot command.
>
> >  * use the qmp transaction to ensure all snapshot of disk is coherence.
> >  */
> > int libxl_disk_snapshot_create(libxl_ctx *ctx, int domid,
> >                                libxl_snapshot *snapshot, int nb);
>
> None of these seem to have support for async. What if the disk backend
> might take a few seconds to take a snapshot.
>
> (as a thought experiment you might consider taking a snapshot with "dd")
thanks, do you mean ao in create and save?
>
> > /* delete number of nb disk snapshot describe in snapshot array
> >  */
> > int libxl_disk_snapshot_delete(libxl_ctx *ctx, int domid,
> >                                libxl_snapshot *snapshot, int nb);
> > int libxl__disk_snapshot_delete(libxl_ctx *ctx, int domid,
> >                                 libxl_snapshot *snapshot);
> > /* apply the disk snapshot by qemu-img command
>
> There's that "apply" again. What does it mean?
revert. reference my comment above.
>
> >  */
> > int libxl_disk_snapshot_apply(libxl_ctx *ctx, int domid,
> >                               libxl_snapshot *snapshot, int nb);
> > /* list disk snapshot by qmp query-block command
> >  */
> > int libxl__disk_snapshot_list(libxl_ctx *ctx, int domid,
> >                               libxl_snapshot **snapshotp);
>
> I think the resource management aspect here needs careful thought. i.e.
> the management of the backing storage, should it be the responsibility
> of libxl or the toolstack (xl, libvirt) etc.
>
> I have a feeling that libxl should be providing the mechanisms but the
> actual management (i.e. lists of snapshots) will be the toolstack's
> doing -- i.e. doesn't libvirt already have a storage and snapshot
> manager?
libvirt already have such snapshot manager: user need provide filename for
external snapshot file. so as to xl.
for other resource do not provided by libvirt, libxl should manage it.
e.g. the memory file saved by xen, should managed by libxl as i mentioned in
snapshot information file.
>
> > (2), libxl/xl_cmdimpl.c
> > /* get disk device name (hda, hdc..) and image path through
> >  * libxl_device_disk_list and libxl_device_disk_getinfo
> >  */
> > static int get_disk(uint32_t domid, libxl_snapshot **snapshotp, char *name);
>
> What is this?
as we talk above, it convert libxl_device_disk to libxl_disk_snapshot for
domain.
>
> > 3, question
> > 1), how to do disk snapshot apply?
> > when i apply the vm snapshot, i need to revert the disk snapshot. in qemu, it
> > is
> > done by bdrv_snapshot_goto api. this api is only called by loadvm hmp or
> > qemu-img commands.
> > there is no hmp api in libxl. so the only choice is using qemu-img command.
>
> "hmp"? Did you mean "qmp"? I don't think so because there is definitely
> a qmp api in libxl, but I can't htink of anything else.
hmp is another channel for qemu operation. it could be called through qmp i
know this after i send this email.
there are loadvm and savevm hmp in qemu, libxl. it is useful for doing the
internal snapshot. savevm means save qemu memory and do internal disk snapshot.
and loadvm mean revert the snapshot state the one of savevm state.
but it will almost save qemu state twice if the vm snapshot create flow is as
follows:
1, save memory, the core function will be libxl_domain_suspend. it will save
domain memory and qemu state.
after memory save, the domain should remain pause for savevm.
2, call savevm hmp through qemu qmp. the save vm will save qemu memory
(including qemu state saved by step 1) and do internal disk snapshot.

the qemu state is very small, it will not waste a lot. is it ok?

>
> > i know usually i should call qmp for qemu operation. but there is not qmp at
> > the moment, could i call qemu-img in libxl for disk snapshot apply?
>
> In general I think it would be useful to go back and define what you
> actually mean by all these things at a higher semantic layer. i.e. what
> does it mean to "apply" a disk snapshot?
yes, it seems that i need write a new version of this doc for review.
i will update api, struct and implementation details for distinguish
non-live, live snapshot, internal and external snapshot.
meanwhile, IIUC, libvirt/qemu only support revert the non-live, internal
snapshot revert. could i fosus on non-live internal snapshot operations for
the first version of code in next steps?

regards

bamvor

>
> Ian.

2, live snapshot
1), check the status in libvirt
only external checkpoints supported by live snapshot.

09:41 2014-05-06
GTD
0, 9:30

1, today
1), reply Ian C. see"16:34 2014-05-05"
2), -14:45. personal.
3), go to Xiaozhuang Hospital.
4), 电热水器。

17:06 2014-05-06
software skill, glibc, "corrupted double-linked list"
1,
struct malloc_chunk {

  INTERNAL_SIZE_T      prev_size;  /* Size of previous chunk (if free).  */
  INTERNAL_SIZE_T      size;       /* Size in bytes, including overhead. */

  struct malloc_chunk* fd;         /* double links -- used only if free. */
  struct malloc_chunk* bk;

  /* Only used for large blocks: pointer to next larger size.  */
  struct malloc_chunk* fd_nextsize; /* double links -- used only if free. */
  struct malloc_chunk* bk_nextsize;
};

#define unlink(P, BK, FD) {                                            \
    FD = P->fd;                                   \
    BK = P->bk;                                   \
    if (__builtin_expect (FD->bk != P || BK->fd != P, 0))             \
      malloc_printerr (check_action, "corrupted double-linked list", P);      \
    else {                                    \
        FD->bk = BK;                                  \
        BK->fd = FD;                                  \
...

17:16 2014-05-06
1, Lin Ma
1), oss test
installation is almost ok.
2), virt-test.

15:50 2014-05-07
snapshot
1, "David kiarie <davidkiarie4@gmail.com>"_email_"libxl snapshot"
cc jim

Hi, David

nice to meet you.
> Hi Bamvor,
>
> I am a Kenyan student participating in GSoC.I am the same student who
> had proposed a xen project to implement snapshot support in libxl but > I gave up on that.
> I got seleted in GSoC to add snapshot support to libvirt xenlight
> driver.Anyway that is just a trick I did to get selected.
>
> Well,..to implement the snapshot support in libvirt xenlight driver I
> need the support from libxl.I have been following your work on the
> snapshot support.I wish to help
> so that we can finally finish this and I can move off to libvirt.
>
> I was wishing you could let me in on your work on the vm_snapshots.I
> am just trying to avoid both of us having to do the same work over
> again.
we have discuss with Jim about this GSOC project month ago. from my point of
view, we could work on libxl and libvirt driver respectively. and what we need
is discuss the interface between them. and this is actually a blocker for
snapshot in libxl as you may see in the xen-devel. and after these interface
and struct is confirmed, you could work on libvirt driver, you know, discuss,
coding, review, it might be takes a long time.
after your libvirt driver is in good shape, we could discuss this again. there
are lots of advanced feature need to do or corner need to be tested in libxl
side.

regards

bamvor
>
> Regards,
> David,

2,
> On Wed, Apr 23, 2014 at 1:18 PM, Bamvor Jian Zhang <bjzhang@suse.com> wrote:
> > Hi,
> >
> >  >>>Jim Fehlig <jfehlig@suse.com> wrote:
> >> Bamvor Jian Zhang wrote:
...
> >> > 2, new struct and new api
> >> > 1), new struct
> >> > (1), libxl_snapshot struct store a disk snapshot information, which get from
> >> > qcow2 image through "query-block" qmp command.
> >> >
> >> > libxl_snapshot = Struct("snapshot",[
> >> >     ("device",        string),
> >> >
> >>
> >> Should this be libxl_device_disk instead of a string? And should
> >> multiple be supported, allowing control over which of a domain's disks
> >> are included in the snapshot?
> > one libxl_snapshot represent one disk. see my comment below.
> >>
> >> "description" would be helpful.
> >>
> >> >     ("id",            string),
> >> >     ("vm_state_size", uint64),
> >> >     ("date_sec",      uint64),
> >> >     ("date_nsec",     uint64),
> >> >     ("vm_clock_sec",  uint64),
> >> >     ("vm_clock_nsec", uint64),
> >> >
> >>
> >> A lot of these are read-only and wouldn't be provided when calling
> >> libxl_snapshot_create(). I think adding the domain state at time of
> >> snapshot would be useful.
> > do you mean the stopped or running state for a domain?
> >> There are probably other items I'm not
> >> considering atm, which makes me nervous about future extensibility.
> > so, how about this?
> >  libxl_vm_snapshot = Struct("vm_snapshot",[
> >      ("name",          string),
> >      ("creation_time", uint64),
> >      ("save",          string),
> > +    ("state",         string),
> > +    ("disks", Array(libxl_snapshot, "num_disks")),
> >      ])
> >
> > BTW: maybe i should rename libxl_snapshot to libxl_disk_snapshot?
> >
> >>
> >> > (2), libxl_vm_snapshot store vm snapshot information which store in the path
> >> > shown above. i add some api for create, delete and list these information.
> >> > at first, i want to add these information to xenstore, but it will lose when
> >> > xenstore reboot or dom0 reboot.
> >> >
> >> > libxl_vm_snapshot = Struct("vm_snapshot",[
> >> >     ("name",          string),
> >> >     ("creation_time", uint64),
> >> >     ("save",          string),
> >> >     ])
> >> >
> >> > 2), new api
> >> > (1), in libxl/libxl.h
> >> > /* disk snapshot api
> >> >  * support create, delete and list for internal snapshot of a single disk
> >> >  * only support internal snapshot rigt now.
> >> >  */
> >> > /* create disk snapshot according to the device name in snapshot array. nb is
> >> >  * the number of snapshot array.
> >> >  * use the qmp transaction to ensure all snapshot of disk is coherence.
> >> >  */
> >> > int libxl_disk_snapshot_create(libxl_ctx *ctx, int domid,
> >> >                                libxl_snapshot *snapshot, int nb);
> >> > /* delete number of nb disk snapshot describe in snapshot array
> >> >  */
> >> > int libxl_disk_snapshot_delete(libxl_ctx *ctx, int domid,
> >> >                                libxl_snapshot *snapshot, int nb);
> >> > int libxl__disk_snapshot_delete(libxl_ctx *ctx, int domid,
> >> >                                 libxl_snapshot *snapshot);
> >> > /* apply the disk snapshot by qemu-img command
> >> >  */
> >> > int libxl_disk_snapshot_apply(libxl_ctx *ctx, int domid,
> >> >                               libxl_snapshot *snapshot, int nb);
> >> > /* list disk snapshot by qmp query-block command
> >> >  */
> >> > int libxl__disk_snapshot_list(libxl_ctx *ctx, int domid,
> >> >                               libxl_snapshot **snapshotp);
> >> >
> >> > /* vm snapshot api
> >> >  */
> >> > /* write vm snapshot information to file
> >> >  */
> >> > int libxl_vm_snapshot_add(libxl_ctx *ctx, uint32_t domid,
> >> >                           libxl_vm_snapshot *snapshot,
> >> >                           const libxl_asyncop_how *ao_how)
> >> >                           LIBXL_EXTERNAL_CALLERS_ONLY;
> >> > /* delete vm snapshot information file
> >> >  */
> >> > int libxl_vm_snapshot_remove(libxl_ctx *ctx, uint32_t domid,
> >> >                              libxl_vm_snapshot *snapshot,
> >> >                              const libxl_asyncop_how *ao_how)
> >> >                              LIBXL_EXTERNAL_CALLERS_ONLY;
> >> > /* list vm snapshot from file
> >> >  */
> >> > libxl_vm_snapshot *libxl_vm_snapshot_list(libxl_ctx *ctx, uint32_t domid,
> >> >                                           int *num)
> >> >                                           LIBXL_EXTERNAL_CALLERS_ONLY;
> >> > /* read vm snapshot information from dedicated file
> >> >  */
> >> > int libxl_vm_snapshot_getinfo(libxl_ctx *ctx, uint32_t domid,
> >> >                               libxl_vm_snapshot *snapshot);
> >> > /* delete vm save image */
> >> > int libxl_vm_snapshot_delete_save_image(libxl_ctx *ctx,
> >> >                                         libxl_vm_snapshot *snapshot);
> >> >
> >>
> >> IMO, these too should be combined into libxl_snapshot_*(), with disk vs
> >> domain snapshot controlled via a flags argument.
>
> Yes, I also think so.
> I was also thinking that one struct could be used to represent all
> snapshot(vm and disk) information.
except the id/name in vm, disk, there is no common information for vm and disk.
i am not sure how do you want to do, maybe you could give a example?

16:26 2014-05-08
software, filesystem, btrfs, compress
1, mount -o compress=lzo /dev/host/btrfs /mnt
2, 2014-05-08T16:07:40.823462+08:00 linux-3lk1 kernel: [ 9169.908179] btrfs: compression is not supported, load module with allow_unsupported=1
3, re insert btrfs module:
rmmod btrfs
modprobe btrfs allow_unsupported=1
4, linux-3lk1:~ # mount |grep mnt
/dev/mapper/host-btrfs on /mnt type btrfs (rw,relatime,compress=lzo,space_cache)

12:03 2014-05-09
1,
l3 workflow
l3support.suse.de

l3doc.suse.de
https://wiki.innerweb.novell.com/index.php/SUSE/Labs_Publications/Linux_Kernel_Maintainers

2, support the last service pack or buy dedicate ltss.

3, btrfs
1), extent based filesystem.
ext2/ext3: block based. use bitmap and point to block. 文件和inode数量有限制.
2), 64bit.
3), snapshot.
4), subvolume.
5), checksums on data and metadata(crc32c).
6), SSD awareness.
mount -t btrfs -o SSD /dev/sda5 /btrfsdisk
7), support software RAID.
mkfs.btrfs -d raid1 /dev/sda6 /dev/sda7
mount -t btrfs /dev/sda6 /btrfsdisk
8), from celen
btrfs balance

4, Hao Junwei
1), tourist
Albrecht Dürer
2, Prague
1), SLEPOS
https://wiki.innerweb.novell.com/index.php/SLEPOS

10:28 2014-05-12
GTD
0, 10:00-18:40

1, today
1), -10:42 misc stuff.
2), 2' 14:09-14:37 reply to David kiakie.
3), 11:30 13:52- write snapshot doc.
4), 11:30-12:45 lunch.
LWN done.
5), -13:52 misc.
install sles12 beta6 candidate.
6), 17:45-18:35 bios. see"18:00 2014-05-12"

2, todo
work report.

10:53 2014-05-12
(21:59 2014-05-15)
snapshot
cc
David kiarie <davidkiarie4@gmail.com>
jim, roger, chunyan.
ian c, ian j, Anthony.perard


[RFC v3] domain snapshot documents
Hi,

here is the third version about domain snapshot documents, the second version
and the first version is here[1][2].

there are lots of potential feature about snapshots. for now, i focus on
providing the api for libvirt libxl driver in order to support the same
functionality compare with libvirt qemu driver. and you may already notice
david is working on libvirt libxl driver. it is a GSoC project[3]. it is
important for him to know the api in order to start coding in libvirt side.
i plan to work on other "advanced feature" after my first stage patch ack.

there are two types of snapshots supported by libxl: disk snapshot and domain
snapshot and four types of operations: create, delete, list and revert.

Disk snapshot will only be crash-consistent if the domain is running. Disk
snapshots can also be internal (qcow2) or external (snapshot in one file, delta
in another).

Domain snapshots include disk snapshots and domain state, allowing to resume
the domain from the same state when the snapshot was created. This type of
snapshot is also referred to as a domain checkpoint or system checkpoint.

In libvirt, there is a something like resource manager for domain snapshot
managements. So, in libxl, all these information is transfered through
libxl_domain_snapshot struct. xl will manage the snapshot by itself.

Domain snapshot create means save domain state and do disk snapshots.
At the beginning of domain snapshot create, it will check whether it is
snapshotable. it is snapshotable if all the disk is qdisk backed.

Domain snapshot revert means rollback the current snapshot state. and
Because the limitation of the qemu qmp, the revert could only support domain
snapshot with internal disk snapshot. revert the domain snapshot with external
snapshot doest not support.

there are live flag in snasphot configuration file, it will be save domain
memory and do external disk snapshot. to make the thing simple, i do not want
to implement in my first verion of patch.

As Ian Campbell said, the support of non-qdisk snapshot is very useful.
unfortuntely, i have no idea what it need to do. the only non-qdisk i know is
blktap. and i do not know does how to do snapshot create, delete, list and
revert for blktap? does it support internal or external support?
i treat it as an "advanced" feature, i will not cover it in my first version of
patch.

the new struct, api and command is as follows:
1, new struct
1), libxl_snapshot
store a disk snapshot information, it is used by disk snapshot create and delete.
libxl_disk_snapshot = Struct("disk_snapshot",[
    ("device",        string),
    ("name",          string),
    ("file",          string),
    ("format",        string),
    ])

device: device name to snapshot. e.g. sda, hda...
name: snapshot name given by user. it will the be same name as domain snapshot
name.
the following paramenter is only useful for external snapshot.
file: external snapshot file.
format: the format of external snapshot file

2), libxl_domain_snapshot
store domain snapshot information which store in the path shown above. i add
some api for create, delete and list these information.
libxl_domain_snapshot = Struct("domain_snapshot",[
    ("name",          string),
    ("creation_time", uint64),
    ("save",          string),
    ("disks", Array(libxl_disk_snapshot, "num_disks")),
    ])

name: snapshot name given by user. if user do not provide the name, it will be
the epoch seconds.
creation_time: the epoch seconds.
save: the memory save file for this snapshot.
disks: store the disk snapshot information assoiate with this domain.

2, new functions
there is no common api like libxl_snapshot_xxx. the reason is that different
toolstack may need to different event handling machanism(synchronize or
asynchronize). and obviously, domain snapshot create need async handler. so i
decide to only provide the sub api for xl and other toolstack(e.g. libvirt).
it make eailer for toolstack to handle the event by themselves.

1), in libxl/libxl.h
the implementation will be located in libxl_snapshot.c
/* disk snapshot api
 * support create for external and internal disks, support delete for internal
 * snapshot of disks.
 */
/* create disk snapshot according to the device name in snapshot array. nb is
 * the number of snapshot array.
 * use the qmp transaction to ensure all snapshot of disk is coherence.
 */
int libxl_disk_snapshot_create(libxl_ctx *ctx, int domid,
                               libxl_disk_snapshot *snapshot, int nb,
                               const libxl_asyncop_how *ao_how);
/* delete number of nb disk snapshot describe in snapshot array
 */
int libxl_disk_snapshot_delete(libxl_ctx *ctx, int domid,
                               libxl_disk_snapshot *snapshot, int nb);

2), xl_cmdimpl.c
int libxl_snapshot_create(int domid, libxl_domain_snapshot *snapshot);
int libxl_snapshot_delete(int domid, libxl_domain_snapshot *snapshot);
int libxl_snapshot_get(int domid, libxl_domain_snapshot *snapshot, int nb);
int libxl_snapshot_revert(int domid, libxl_domain_snapshot *snapshot);

support create, delete, list and revert for domain snasphot.

libxl_snapshot_get will read the domain snapshot configuration file stored in
disk and list snapshot information in simple or long format.

3, snapshot information file
i will write manpage for this with patch.

i found the Wei v5 patch about xl json format.
http://lists.xen.org/archives/html/xen-devel/2014-05/msg01670.html
it seems that i could use these apis for parsing and generating the snapshot
information file.

the domain snapshot information will store in the follow path:
/var/lib/xen/snapshots/<domain_uuid>/snapshotdata-<snapshot_name>.xl

here is an example for snapshot information file:
description="a snapshot after installation"
name="1397207577"
creationtime="1397207577"
save="1397207577.save"
type="internal"/"external"
live="no"
disk_only="no"
disk=[ 'hda=disk_hda.qcow2,type=qcow2', 'hdc=disk_hdc.qcow2,type=qcow2']

the save and disk image file base on the path of "/var/lib/xen/snapshots/<domain_uuid>"

the user could give a snapshot name when vm snapshot created. if not, the epoch
seconds will set as name as the above examples.

3, new command
i will write manpage for this with patch.
1), snapshot-create
Usage: xl snapshot-create <ConfigFile> [options] [Domain]

Create domain snapshot with ConfigFile or options

Options:
-n                snapshot name
--live            do live snapshot
--disk-only       only disk snapshot, do not save memory.

2), snapshot-list
Usage: xl snapshot-list [options] [Domain]

List domain snapshot information about all/some snapshot in one domain.

Options:
-l, --long        Output all domain snapshot details
-n                snapshot name

3), snapshot-delete
Usage: xl snapshot-delete [options] [Domain]

Delete domain snapshot relative data, including domain state, disk snapshot
and domain snapshot information file.

Options:
-n                snapshot name

4), snapshot-revert
Usage: xl snapshot-revert [options] [Domain]

Rollback the domain to snapshot state.

Options:
-n                snapshot name

[1] http://lists.xen.org/archives/html/xen-devel/2014-04/msg00414.html
    http://lists.xen.org/archives/html/xen-devel/2014-04/msg00244.html
[2] http://lists.xen.org/archives/html/xen-devel/2014-04/msg02549.html
[3] http://en.opensuse.org/openSUSE:GSOC_ideas#Add_virtual_machine_snapshot_support_to_libvirt_Xen_driver

changes since v2:
1), reorgnized the whole docments.
2), do not export the dedicated the disk snapshot commands.
3), others changes according to Ian and Jim's comment.

14:09 2014-05-12
snapshot, gsoc, xen
Hi, david

> Am  running the the unstable version from xen.git.
sorry for confuse. but i want to know the exact version for your xen and
toolstack. you could get xen hypervior version from xl dmesg or sysfs, here is
my version(it is a stable version, not for testing).
get xen hypervisor version from xl:
# xl dmesg | head
 __  __            _  _    ____    _____    ___  _    _   ____  ____   _  _
 \ \/ /___ _ __   | || |  |___ \  |___ /   / _ \/ |  / | |___ \|___ \ | || |
  \  // _ \ '_ \  | || |_   __) |   |_ \  | | | | |__| |   __) | __) || || |_
  /  \  __/ | | | |__   _| / __/ _ ___) | | |_| | |__| |_ / __/ / __/ |__   _|
 /_/\_\___|_| |_|    |_|(_)_____(_)____/___\___/|_|  |_(_)_____|_____(_) |_|
                                      |_____|
(XEN) Xen version 4.2.3_01-1.22.4 (abuild@) (gcc (SUSE Linux) 4.7.2 20130108 [gcc-4_7-branch revision 195012]) Mon Dec  2 12:22:01 UTC 2013
(XEN) Latest ChangeSet: 26064
(XEN) Bootloader: GRUB2 2.00
(XEN) Command line:

get xen version from sysfs:
/sys/hypervisor/version # cat major
4
/sys/hypervisor/version # cat minor
2
/sys/hypervisor/version # cat extra
.3_01-1.22.4

>
> Am suspecting a supporting module is not loaded.Am using a SUSE kernel-xen
yes, i think so. meanwhile i guess the mismatch of xen hypervisor and toolstack
 could be the root cause which lead to xen module load fail.
usually, we need to update hypervisor and toolstack at the same time.
> uname -r
>
> 3.11.10-7-xen
>
> cat /boot/config-$(uname -r)* | grep XEN
>
> CONFIG_X86_64_XEN=y
...
> CONFIG_XEN_DEV_EVTCHN=m
...
> CONFIG_XEN_PRIVCMD=y
>
> I see that xen-evtchn is a module but I don't think it is getting loaded.
>
>
> I try to probe it
>
> #modprobe xen-evtchn
> #FATAL :module xen-evtch not found
FYI: you may be already know it. there is a script for managing xen start, stop:
"/etc/init.d/xencommons"

>
>
> Can I recompile this kernel?
of course u could. but i am not sure if u really needed.
in your kernel configuration, EVTCHN is compiled as module. it should be there.
in my system(opensuse 12.3 with kernel 3.7) the module is named evtchn:
> lsmod |grep evt
evtchn                 13162  1

and it located at
/lib/modules/3.7.10-1.16-xen/kernel/drivers/xen/evtchn.ko
are you look into the xen directories before asking?

here is my system version:
> uname -a
Linux laptop-work.site 3.7.10-1.16-xen #1 SMP Fri May 31 20:21:23 UTC 2013 (97c14ba) x86_64 x86_64 x86_64 GNU/Linux

>
> On Mon, May 12, 2014 at 5:45 AM, Bamvor Jian Zhang <bjzhang@suse.com> wrote:
> > Hi,
> >
> >  >>>David kiarie <davidkiarie4@gmail.com> wrote:
> >> Could you know why am running into this?
> > which version of xen hypervisor do you use?
> > different version between xen and toolstack may lead to this issue.
> >
> > BTW: you could cc me and Jim when you send mail to xen-users.
> >
regards

bamvor

18:00 2014-05-12
(13:21 2014-05-13)
virtualization, xen, hvmloader, bios
1, http://people.cs.nctu.edu.tw/~huangmc/works/web/Boot_x86/Boot_x86.html
電源正常啟動後，x86 CPU 會先執行 0xFFFF0，也就是 BIOS ROM 的進入點。由於 0xFFFF0 ~ 0xFFFFF 只有少的很可憐的 16 bytes，真正的 BIOS code 勢必要擺到其他位置，此時 0xFFFF0 的作用便是 jmp 到該位置執行 BIOS 程式。

2, xen/tools/firmware/hvmloader/hvmloader.c
hvmloader start at 0x100000.

    /* Enter real mode, reload all segment registers and IDT. */
    "    ljmp $"STR(SEL_CODE16)",$0x0\n"
    "trampoline_start: .code16       \n"
    "    mov  %eax,%cr0              \n"
    "    ljmp $0,$1f-trampoline_start\n"
    "1:  mov  %ax,%ds                \n"
    "    mov  %ax,%es                \n"
    "    mov  %ax,%fs                \n"
    "    mov  %ax,%gs                \n"
    "    mov  %ax,%ss                \n"
    "    lidt 1f-trampoline_start    \n"
    "    ljmp $0xf000,$0xfff0        \n"
    "1:  .word 0x3ff,0,0             \n"
    "trampoline_end:   .code32       \n"

3, So, how and when does hvmloader put bios to ffff0?
there is a tricky that bios is loaded at the end of 0x100000. and bios set a jump instruction at the fff0. the segment should be f000, so the jump instruction is located at ffff0.

1), seabios
(1), tools/firmware/seabios-dir/src/romlayout.S
entry_iret_official:
        iretw

        ORG 0xff54
        IRQ_ENTRY_ARG 05

        ORG 0xfff0 // Power-up Entry Point
        .global reset_vector
reset_vector:
        ljmpw $SEG_BIOS, $entry_post

        // 0xfff5 - BiosDate in misc.c

        // 0xfffe - BiosModelId in misc.c

        // 0xffff - BiosChecksum in misc.c

        .end


src/config.h:#define SEG_BIOS     0xf000
and the entry_post is defined in the romlayout.S too:
/****************************************************************
 * Fixed position entry points
 ****************************************************************/

        // Specify a location in the fixed part of bios area.
        .macro ORG addr
        .section .fixedaddr.\addr
        .endm

        ORG 0xe05b
entry_post:
        cmpl $0, %cs:HaveRunPost                // Check for resume/reboot
        jnz entry_resume
        ENTRY_INTO32 _cfunc32flat_handle_post   // Normal entry point

TODO: where does 0x3630 come from?

(2), check binary and confirm it:
> objdump -b binary -m i386 -D out/bios.bin  | tail
   1ffec:       fa                      cli
   1ffed:       fc                      cld
   1ffee:       66 c3                   retw 1fff0:       ea 5b e0 00 f0 30 36    ljmp   $0x3630,$0xf000e05b
   1fff7:       2f                      das
   1fff8:       32 33                   xor    (%ebx),%dh
   1fffa:       2f                      das
   1fffb:       39 39                   cmp    %edi,(%ecx)
   1fffd:       00 fc                   add    %bh,%ah
        ...

the instruction at 0xffff0 is 0x3630f000e05bea

notes: i could list the architecture with objdump -i. i could see only i386 is supported in binary.

2), rombios
tools/firmware/rombios/rombios.c

#ifdef HVMTEST
.org 0xffe0
  jmp 0xf000:post;
#endif

.org 0xfff0 ; Power-up Entry Point
#ifdef HVMTEST
  jmp 0xd000:0x0003;
#else
   jmp 0xf000:post
#endif

.org 0xfff5 ; ASCII Date ROM was built - 8 characters in MM/DD/YY
.ascii BIOS_BUILD_DATE

4, "tools/libxc/xc_hvm_build_x86.c"

15:34 2014-05-13
beijing, sync
1, Lin ma
1), oss test
(1), deploy on lvm.
(2), copy ssh public key from host to guest.
(3), how to get guest linux ip address.

07:58 2014-05-14
US, sync

14:14 2014-05-14
1,
qemu: hw/core/loader.c
    rom_add_option -> rom_add_file: genroms
seabios: src/optionroms.c
    run_file_roms("genroms/", 0, sources);

17:21 2014-05-14
GTD
0, -18:56

1, today
1), write snapshot doc.
-17:21 finish basic doc.

11:32 2014-05-15
1, today
1), snapshot doc: review and consider Ian C comments.

17:30 2014-05-15
1, reply to roger
Hi, roger
> Bamvor,
>
> Very honestly speaking, you know what (even I repeat it again), when I
> met Jason and Olaf we talked about you. I told them there is no issue on
> your functional/Technical Skills competency, and your knowledge sharing
> habit is really good things. I am confident you could generate very good
> business result. I do hope, and like to work with you to make it, together.
thanks. meanwhile if you could join the discussion about snapshot on the
technique side. i will appreciate your real help.
>
> For that reason, I have to point out WFH without reason and approval
> ahead of time is not a professional approach.
i sent the email before the core time. i could send it early next time. or you
could give me your definition of ahead of time. and it just a WFH not a FTO,
that means i will do the normal work. and for WFH, obviously there is something
personal issue. for me, our gas water heater broken and need replacement today.
and it will waste time if i came to company. i mean came to company, leave
company, replace heater, and came to company afternoon. obviously, the better
solution is WFH or it takes me 4 hours on my road.
> In addition to that, you
> already took Tuesday morning off. If it keeps happening, it does give me
> doubt on what I believe. That is really not what we want.
it is a 20-30min late compare with core time. what is the better solution for
this case?
>
> Let's have a 1on1 next Monday.
the reason is above. email discussion is enough for now.
>
> The other thing, I don't think I talked to you in 1on1 even though we
> talked in the team when you are there. There are some organizational
> policy in regarding to 8 hours working policy when Olaf was here in Feb.
> Company respects flexibility, but 11AM~4PM should be the core working
> hours we must be in office in respect the whole team. Other than that,
> it need be approved by the manager. The same WFH. We need balance
> between flexibility and discipline.
i am not sure what you really mean. if you mean i need send a email to you
alone and after you approve it and i need cc to others. it make sense to me.
>
> Anyway, don't be too much upset for my email. I am still confident like
> I said in the first paragraph. Life, the world, the company, and all of
> us is just about to CHANGE. You can have some proposal how we change
> etc. I will do the same like Planning and Driving for Results.
>
> ps. I do aware that you reply email on Saturday, and have dinner in
> office yesterday. Your current snapshot feature is a good task to make
> change, and to make it part of real success, and keep going! BTW,
> normally there will be pains or challenges before we success.
thanks to your observation. it is enough to fill the 30 min late for core time
on Tuesday i guess.
>
> have a good day,
> Roger
>
>
> On 05/15/2014 10:48 AM, Bamvor Jian Zhang wrote:
>
> --
> regards,
> Roger (office#:+86 10 65339283, cellphone# +86 13391978086)

2, reply to roger about documents review.
Hi, roger

here is the v3 documents for snapshot. it is important for me to confirm the
scoping and api from xen-devel. could you please review and reply in the
xen-devel?

http://lists.xen.org/archives/html/xen-devel/2014-05/msg01977.html

regards

bamvor

17:53 2014-05-15
Hi, David

 >>>David kiarie <davidkiarie4@gmail.com> wrote:
> On Wed, Apr 2, 2014 at 2:21 PM, Bamvor Jian Zhang <bjzhang@suse.com> wrote:
>
> > add internal disk snapshot support through qemu qmp, including create,
> > delete and list.
> >
> > Signed-off-by: Bamvor Jian Zhang <bjzhang@suse.com>
> > ---
> >  tools/libxl/libxl.c          |  29 ++++++++++
> >  tools/libxl/libxl.h          |   9 +++
> >  tools/libxl/libxl_internal.h |  15 +++++
> >  tools/libxl/libxl_qmp.c      | 132
> > +++++++++++++++++++++++++++++++++++++++++++
> >  tools/libxl/libxl_types.idl  |  12 ++++
> >  tools/libxl/xl.h             |   3 +
> >  tools/libxl/xl_cmdimpl.c     | 114 +++++++++++++++++++++++++++++++++++++
> >  tools/libxl/xl_cmdtable.c    |  18 ++++++
> >  8 files changed, 332 insertions(+)
> >
>
> What qemu are you using?
>
> IIUC the block-snapshot-<operation>-internal-sync commands have not being
> implemented here.Well they are not synchronized.I can only see
> block-snapshot-<operation>- ones; no sync
it is upstream qemu, it maybe takes a long time for discussion and reviewing
the snapshot feature and code. i guess xen will update the qemu before our
snasphot patch get ack.

regards

bamvor

19:40 2014-05-15
snapshot
1,
do i need update domain configuration after external snapshot?

21:11 2014-05-15
claudia
question about sick leave
1, how many days do i have the sick leave?
2, does sick leave acc?

