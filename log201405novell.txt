.LOG
11:01 2014-05-04
On 22.04.14 11:33, Chun Yan Liu wrote:
> Hello, Alex,
>
> This is Chunyan. Currently I'm working on xen HVM direct kernel boot and encounter some qemu questions. I found you have some patches regarding to that before, so just send mail for some knowledge.
>
> I know KVM supports direct kernel boot but not quite clear how it's implemented. In pc_memory_init(), I can see bochs_bios_init() and load_linux(). I think I can understand load_linux(). It reads kernel and initrd to specific addresses, and put linuxboot.bin to option rom which override int19 to intercept BIOS process. linuxboot.bin can then goto 'kernel' address to execute. But I don't quite understand bochs_bios_init(), seems it only registers some things to FW_CFG (?) So, when is BIOS loaded?

The BIOS gets loaded directly into the guest's address space on machine
init. -kernel and -initrd however only get loaded into blobs in fw_cfg
that the linuxboot.bin file pull from fw_cfg during runtime.

> For Xen, hvmloader provides BIOS, which is loaded to RAM in xen code, and will probe disk MBR for kernel and initrd. So in SLE11 SP2, to support xen hvm direct kernel boot, we make a different load_linux function, which read kernel and initrd and generate a boot sector in the head of first disk. But surely that's not acceptable for upstream. Now, I wonder if we can reuse qemu code. If BIOS can be loaded in qemu for xen, then maybe I can remove hvmloader from xen code (don't load it at all), and in qemu side, let it do pc_memory_init() also for xen case. Don't know if there is other blocks, but can try.

I don't see why this wouldn't work, yeah. I think it makes a lot of sense.

Alex

16:53 2014-05-04
http://wiki.qemu.org/QMP#By_hand

17:25 2014-05-04
company, reserve machine
orthos.arch.suse.de

17:53 2014-05-04
company, email, client
http://147.2.207.240/gw_mail.html
Server Name: imap.novell.com Port: 993
User Name: your novell ID
Connection security: STARTTLS
Outgoing Server Name: smtp.novell.com Port:25

13:54 2014-05-05
software skill, gdb, signal, ignore
http://www.cppblog.com/elva/archive/2011/11/01/159457.html
handle SIGUSR1 nostop noprint

16:30 2014-05-05
bnc: bugzilla@novell.com

16:34 2014-05-05
(10:17 2014-05-06)
1,
Hi, Ian

thanks your reply. sorry reply late, i just finish my vacation.
>
> On Fri, 2014-04-18 at 04:10 -0600, Bamvor Jian Zhang wrote:
> > Hi,
> >
> > here is the second version about vm snapshot documents, the first version is
> > here[1]
>
> Have you considered attending the hackathon and proposing this as a
> topic? See http://wiki.xen.org/wiki/Hackathon/May2014 for details, there
> are not a lot of places remaining...
>
> > thanks Ian Jackson suggestion, i write this doc about vm snapshot including
> > new
> > command, struct and api. currently, the disk snapshot(create, delete, list)
> > worked(patch is already sent[2]), and vm snapshot(create, delete, list) work.
> > but i have some questions about snapshot apply(see the last section).
> >
> > feel free to comment it. thanks.
> >
> > 1, new command
> > =head1 SNAPSHOT
> >
> > there are two types of snapshots supported by libxl: disk snapshot and vm
> > snapshot. The following subcommands management the snapshot of a domain,
> > including create, delete, list and apply.
> > a domain snapshot(or a vm snapshot) means save the domain config, memory and
> > disk snapshot. currently, only support qcow2 and internal disk snapshot.
> >
> > the vm snapshot information will store in the follow path:
> > /var/lib/xen/snapshots/<domain_uuid>/snapshotdata-<snapshot_name>.xl
> >
> > here is an example for snapshot information file:
> > snapshot_name="1397207577"
> > snapshot_creationtime="1397207577"
> > snapshot_save="/var/lib/xen/snapshots/5c84adcc-bd59-788a-96d2-195f9b599cfe/1397207577.save"
> >
> > the user could give a snapshot name when vm snapshot created. if not, the
> > epoch
> > seconds will set as name as the above examples.
>
> This all sounds like internal libxl implementation detail. What might be
> interesting to see would be the libxl library API for causing this stuff
> to happen.
???
>
> At an implementation level I think this could profitably use the
> autogenerated json generators and parsers which the libxl IDL can
> provide (Wei is working on the parsing side now).
currently, i use the xlu_cfg_get_xxx for parse the file. i write the file with
my own function.
>
> > =over 4
> >
> > =item B<vm-snapshot-create> [I<OPTIONS>] I<domain-id>
> >
> > create vm snapshot.
>
> Which aspects of the VM are snapshotted? Disk you mention. What about
> memory?
vm snapshot include disk and memory.
>
> > it will call the qmp transaction for creating the disk snapshot in order to
> > ensure the disk snapshot is coherence.
>
> I'm not sure if you intend this document to literally become the xl
> manpage at some point or if it is an outline with implementation notes
> sprinkled around.
>
> If this is the manpage then talking about qmp (an implementation detail)
> isn't appropriate here.
sorry for confuse. i want to discuss the implementation. i will write the
dedicated man page with my code.
>
> If this is a implementation note then it is worth considering non-qdisk
> backed snapshots and/or the mechanisms for configuring a domain such
> that it is "snapshotable" (which might for instance put constraints on
> which disk backend is used).
>
> > vm is paused during snapshot create, and is unpause after take snapshot
> > finished.
>
> Is there any possibility of a live snapshot? (Might make sense in the
> context of snapshotting memory at the same time)
>
> > =item B<vm-snapshot-apply> [I<OPTIONS>] I<domain-id>
> >
> > apply vm snapshot for the dedicated domain.
>
> What does "apply" mean here? DO you mean some sort of rollback for a
> running domain? Or just "start this snapshot as a fresh domain"?
>
> It's a bit unusual for an xl level API to provide this sort of
> management of a list of resources. Would it make more sense to structure
> this more like save/restore (where the user provides the filename)?
>
> That doesn't constrain other toolstack from providing higher level
> management functionality, providing you design the libxl layer
> correctly.
>
> > =item B<disk-snapshot-create> [I<OPTIONS>] I<domain-id>
> >
> > create disk snapshot.
>
> Hrm. so I got the impression that vm-snapshot was creating disk
> snapshots too. Maybe one is a subset of the other?
>
> Since this is described here in terms of xl and not libxl then how do
> you envisage this working in the absence of any kind of "storage
> manager" like functionality, e.g. describe where the snapshots disks
> should go etc.
>
> > 2, new struct and new api
>
> It would have been less confusing for me if you would have started at
> the low level (libxl) and then done the higher level stuff (xl) rather
> than vice versa.
>
> > 1), new struct
> > (1), libxl_snapshot struct store a disk snapshot information, which get from
> > qcow2 image through "query-block" qmp command.
>
> This all seems very specific to a particular backend. The API should be
> designed so that it can work with multiple backends, even if the
> implementation only supports qdisk at first.
>
> > libxl_snapshot = Struct("snapshot",[
> >     ("device",        string),
>
> What is this? Which device?
>
> >     ("name",          string),
> >     ("id",            string),
>
> ID of what?
>
> >     ("vm_state_size", uint64),
> >     ("date_sec",      uint64),
> >     ("date_nsec",     uint64),
> >     ("vm_clock_sec",  uint64),
> >     ("vm_clock_nsec", uint64),
>
> Sounds like we need a time and/or date meta types.
>
> > (2), libxl_vm_snapshot store vm snapshot information which store in the path
> > shown above. i add some api for create, delete and list these information.
> > at first, i want to add these information to xenstore, but it will lose when
> > xenstore reboot or dom0 reboot.
> >
> > libxl_vm_snapshot = Struct("vm_snapshot",[
> >     ("name",          string),
> >     ("creation_time", uint64),
> >     ("save",          string),
>
> What is "save"?
>
> >     ])
> >
> > 2), new api
> > (1), in libxl/libxl.h
> > /* disk snapshot api
> >  * support create, delete and list for internal snapshot of a single disk
> >  * only support internal snapshot rigt now.
> >  */
> > /* create disk snapshot according to the device name in snapshot array. nb is
> >  * the number of snapshot array.
>
> So the user must supply one of these per guest disk?
>
> Is libxl_snapshot really libxl_disk_snapshot?
>
> Should there be some mechanisms to convert from libxl_disk_snapshot to a
> libxl_device_disk so that it can then be attached to a domain? Either a
> utility function or there should be a field in the snapshot.
>
> >  * use the qmp transaction to ensure all snapshot of disk is coherence.
> >  */
> > int libxl_disk_snapshot_create(libxl_ctx *ctx, int domid,
> >                                libxl_snapshot *snapshot, int nb);
>
> None of these seem to have support for async. What if the disk backend
> might take a few seconds to take a snapshot.
>
> (as a thought experiment you might consider taking a snapshot with "dd")
>
> > /* delete number of nb disk snapshot describe in snapshot array
> >  */
> > int libxl_disk_snapshot_delete(libxl_ctx *ctx, int domid,
> >                                libxl_snapshot *snapshot, int nb);
> > int libxl__disk_snapshot_delete(libxl_ctx *ctx, int domid,
> >                                 libxl_snapshot *snapshot);
> > /* apply the disk snapshot by qemu-img command
>
> There's that "apply" again. What does it mean?
>
> >  */
> > int libxl_disk_snapshot_apply(libxl_ctx *ctx, int domid,
> >                               libxl_snapshot *snapshot, int nb);
> > /* list disk snapshot by qmp query-block command
> >  */
> > int libxl__disk_snapshot_list(libxl_ctx *ctx, int domid,
> >                               libxl_snapshot **snapshotp);
>
> I think the resource management aspect here needs careful thought. i.e.
> the management of the backing storage, should it be the responsibility
> of libxl or the toolstack (xl, libvirt) etc.
>
> I have a feeling that libxl should be providing the mechanisms but the
> actual management (i.e. lists of snapshots) will be the toolstack's
> doing -- i.e. doesn't libvirt already have a storage and snapshot
> manager?
>
> > (2), libxl/xl_cmdimpl.c
> > /* get disk device name (hda, hdc..) and image path through
> >  * libxl_device_disk_list and libxl_device_disk_getinfo
> >  */
> > static int get_disk(uint32_t domid, libxl_snapshot **snapshotp, char *name);
>
> What is this?
>
> > 3, question
> > 1), how to do disk snapshot apply?
> > when i apply the vm snapshot, i need to revert the disk snapshot. in qemu, it
> > is
> > done by bdrv_snapshot_goto api. this api is only called by loadvm hmp or
> > qemu-img commands.
> > there is no hmp api in libxl. so the only choice is using qemu-img command.
>
> "hmp"? Did you mean "qmp"? I don't think so because there is definitely
> a qmp api in libxl, but I can't htink of anything else.
>
> > i know usually i should call qmp for qemu operation. but there is not qmp at
> > the moment, could i call qemu-img in libxl for disk snapshot apply?
>
> In general I think it would be useful to go back and define what you
> actually mean by all these things at a higher semantic layer. i.e. what
> does it mean to "apply" a disk snapshot?
>
> Ian.

do you interesting about my current status, only support qemu backend disk: qcow2, no live snapshot.

2, live snapshot
1), check the status in libvirt
only external checkpoints supported by live snapshot.

09:41 2014-05-06
GTD
0, 9:30

1, today
1), reply Ian C. see"16:34 2014-05-05"
2), -14:45. personal.
3), go to Xiaozhuang Hospital.
4), 电热水器。

17:06 2014-05-06
software skill, glibc, "corrupted double-linked list"
1,
struct malloc_chunk {

  INTERNAL_SIZE_T      prev_size;  /* Size of previous chunk (if free).  */
  INTERNAL_SIZE_T      size;       /* Size in bytes, including overhead. */

  struct malloc_chunk* fd;         /* double links -- used only if free. */
  struct malloc_chunk* bk;

  /* Only used for large blocks: pointer to next larger size.  */
  struct malloc_chunk* fd_nextsize; /* double links -- used only if free. */
  struct malloc_chunk* bk_nextsize;
};

#define unlink(P, BK, FD) {                                            \
    FD = P->fd;                                   \
    BK = P->bk;                                   \
    if (__builtin_expect (FD->bk != P || BK->fd != P, 0))             \
      malloc_printerr (check_action, "corrupted double-linked list", P);      \
    else {                                    \
        FD->bk = BK;                                  \
        BK->fd = FD;                                  \
...

17:16 2014-05-06
1, Lin Ma
1), oss test
installation is almost ok.
2), virt-test.

