.LOG
12:31 2014-06-03
mailing list, virtualization
1, "Wei Liu"_email_"Discussion on libxl domain configuration synchronization"

Ian and Ian

There are many open questions regarding the design and implementation of this
new feature so I think it's worthwhile having a dedicated email thread to
discuss this before I write any new code.

The new mechanism will use the new JSON infrastructure so that application can
easily consume domain configuration.

### Exposed API

From API design point of view, what API should we expose to applications? We
seem to have agreed that a "retrieve" API is certainly necessary (let's name it
libxl_retrieve_domain_configuration for ease of discussion), while a "store" /
"save" is debatable.

If we set the rule that libxl application is not allowed to manipulate domain
configuration then "store" is not necessary.

### Implementation

Further more, how should we implement the new API?

My original thought is that the JSON version of domain configuration always
contains the up to date domain configuration, so that we can get hold of that
information even without the help of our library.

Ian J pointed out that this design is bogus. If I understand correctly,
he was referring to the fact that the most up to date information is
actually stored in xenstore. So his suggestion was that we make
libxl_retrieve_domain_configuration a) use JSON version as template, b)
pull in xenstore knob values to fill in the things that need to be
updated. I think this approach is appealing, if we agree on the design
"libxl-json" is completely private to libxl, that application should not
poke at it at all. I am now keen on taking this approach.

What knobs should libxl_retrieve_domain_configuration be looking at, domain
name, memory targets and devices etc. Memory targets are easy as they are only
some integer values.

As for devices, see next session.

### Device hotplug / unplug and half-baked state

When we build a domain, the devices presented in config file (later transform
into JSON) and the devices presented in xenstore (backend entries) are
consistent, otherwise we would have failed when building a domain.

Device hotplug and unplug are a bit trickier to handle.

Hotplug steps:
1. libxl__get_domain_configuration (this reads libxl-json file and parse
   it into libxl_domain_config struct).
2. add new device to JSON config
3. hotplug device
4. if hotplug success, libx__set_domain_configuration

If 3 fails, we may end up in a situation that some xenstore entries have been
written, but that device is not presented in JSON version of configuration. 4
can also fail when libxl tries to write libxl-json file. The steps and analyse
for device unplug are similar.

Later when we do libxl_retrieve_domain_configuration, the xenstore entries and
JSON config don't agree with each other. What should we do here? We cannot say
one view is more acurate then the other, given the fact that either 3 or 4 can
fail. Ian J suggested merging the two views, I agree with him.

### Summary

1. libxl_retrieve_domain_configuration is the only public API.
2. libxl_retrieve_domain_configuration should retrieve JSON version of
   domain configuration as template and update it according to xenstore
   entries.
3. libxl_retrieve_domain_configuration should be able to merge xenstore
   view and JSON config view of a domain.
4. JSON version gets saved when creating a domain.
5. JSON version gets updated when we do device hotplug / unplug. Other
   libxl functionalities will not touch it, unless they need to save
   some user provided values as template.

Thoughts?

Wei.

2, (09:45 2014-06-05)
"Ian Jaskson"_email_"Re: [Xen-devel] Discussion on libxl domain configuration synchronization"
Wei Liu writes ("Discussion on libxl domain configuration synchronization"):
> ### Exposed API
>
> From API design point of view, what API should we expose to
> applications? We seem to have agreed that a "retrieve" API is
> certainly necessary (let's name it
> libxl_retrieve_domain_configuration for ease of discussion), while a
> "store" / "save" is debatable.

Right.

> If we set the rule that libxl application is not allowed to manipulate domain
> configuration then "store" is not necessary.

Indeed.  If the libxl application wants to change the domain's setup
they must make individual calls to add and remove devices, increase
and decrease memory, etc.


> ### Summary
>
> 1. libxl_retrieve_domain_configuration is the only public API.
> 2. libxl_retrieve_domain_configuration should retrieve JSON version of
>    domain configuration as template and update it according to xenstore
>    entries.
> 3. libxl_retrieve_domain_configuration should be able to merge xenstore
>    view and JSON config view of a domain.
> 4. JSON version gets saved when creating a domain.
> 5. JSON version gets updated when we do device hotplug / unplug. Other
>    libxl functionalities will not touch it, unless they need to save
>    some user provided values as template.

Right.


> ### Implementation
>
> Further more, how should we implement the new API?
...
> Ian J pointed out that this design is bogus. If I understand
> correctly, he was referring to the fact that the most up to date
> information is actually stored in xenstore. So his suggestion was
> that we make libxl_retrieve_domain_configuration a) use JSON version
> as template, b) pull in xenstore knob values to fill in the things
> that need to be updated. I think this approach is appealing, if we
> agree on the design "libxl-json" is completely private to libxl,
> that application should not poke at it at all. I am now keen on
> taking this approach.

Exactly.  Yes, the JSON template would be completely private to libxl.

> What knobs should libxl_retrieve_domain_configuration be looking at,
> domain name, memory targets and devices etc. Memory targets are easy
> as they are only some integer values.

I think it should be sufficient to consider every function like
  libxl_do_something_to_existing_domain

Most (but not all) of these will do something which ought to be
reflected in the results from libxl_retrieve_domain_configuration.

There should be nothing else that modifies anything that shows up in
libxl_retrieve_domain_configuration.

> ### Device hotplug / unplug and half-baked state
...
> Device hotplug and unplug are a bit trickier to handle.
>
> Hotplug steps:
> 1. libxl__get_domain_configuration (this reads libxl-json file and parse
>    it into libxl_domain_config struct).
> 2. add new device to JSON config
> 3. hotplug device
> 4. if hotplug success, libx__set_domain_configuration
>
> If 3 fails, we may end up in a situation that some xenstore entries
> have been written, but that device is not presented in JSON version
> of configuration. 4 can also fail when libxl tries to write
> libxl-json file. The steps and analyse for device unplug are
> similar.


There are two ways to do this:


--------------------------------------------------

[ADD] JSON FIRST

This is what you have.

You tolerate as follows:

 * Devices present in xenstore may not be present in the JSON.

   Such devices cannot be reported properly by retrieve because
   in general it is not possible to synthesize a libxl device
   backend configuration out of what appears in xenstore.

   So such devices (which may or may not be working as far
   as the guest is concerned) must be reported with some kind
   of degraded information in retreive.

Conversely you add the new invariants:

 * Any device present in the JSON is also present in xenstore.

 * Any device which has any resources allocated to it in the host or
   guest has an entry in the JSON.

In retrieve, you can use the JSON as the primary reference.

I think the degraded reporting is undesirable.  Also there is a
locking problem - see below.

--------------------------------------------------

[ADD] XENSTORE FIRST

This is what I would suggest instead.

We establish the new invariant:

 * Any device which is present in xenstore has a corresponding entry
   in the JSON (where "corresponding" relates to the virtual device
   index - ie the guest-visible "name" - not the backend info).

I believe we already have this invariant:

 * Any device which has any resources allocated to it in the host or
   guest has an entry in the xenstore.

(There is a corrollary of these two which is that a device with
resources allocated must appear in the JSON.)

Conversely, we tolerate this:

 * Devices present in JSON may not be present in xenstore.
   Such a device is to be regarded as "not present" for all purposes.

To implement this, it is necessary to write out the new JSON before
updating xenstore.

In retrieve, you use xenstore as the primary reference.  The retrieve
function would look at xenstore and simply elide any entries which
were listed in the JSON but not in xenstore.

This also avoids having to take out the lock (there has to be a lock
on the stored JSON to prevent erronous concurrent updates) across the
actual device addition.

I think the retrieval merging algorithm works a bit like this:

  * Read JSON with list of stored device configs

  * Iterate over actual devices a la libxl_device_disk_list
    * For each device, look for corresponding configuration in
      JSON.
      * If no such configuration, print a warning and make one up
        along the lines of libxl_device_disk_getinfo
      * If there is such a configuration, use the JSON information
        except:
      * Check whether the backing device is empty, and if so
        delete the backing device from the JSON.
        (This is for cdroms; see below.)
    * Add resulting device config to new list of device configs

  * Replace stored list of device configs with new one

The effect is that the pre-libxl-defaulted configuration takes
precedence but that it is safe to write a JSON configuration
containing devices which are about to be added.

--------------------------------------------------


And now some loose ends:

For hot unplug, do the operations in the reverse order.  (The reverse
of whichever order we picked above, according to which design we have
chosen.)


CDROM insert/eject presents a problem.  I think it is not possible to
reliably convert the xenstore information back into the original
backend specification.  So we have a difficulty: if we switch straight
from cdrom-has-image-A to cdrom-has-image-B, we might end up with JSON
showing image X (= A or B) and xenstore implementing image Y (= B or
A).  Retrieve wouldn't know what to return.

I propose to solve this by adding a new invariant:

 * The actual disk backend device (in xenstore) for an "ejectable"
   device (currently, only cdroms) is always the same as the JSON;
 * EXCEPT that it is permissible for the JSON to specify a
   nonempty image while the xenstore information specifies "empty".

That allows us to always return correct data: if the xenstore backend
is empty, retrieve says empty.  Otherwise we use the JSON info.

To maintain this invariant we need to change libxl_cdrom_insert.  A
"swap", ie replacement of one image with another, has to be
implemented as first an eject and then an insert.  That way we never
have a situation where xenstore contains one set of non-empty
information and JSON another.


Thanks,
Ian.

3, (19:00 2014-06-05)
Hi, Wei and Ian

sorry if i am offtopic.
i am working on add snapshot support for libxl. currently, i write my code base
on Wei's lastest libxl json patch series. i could call
libxl_domain_snapshot_from_json and libxl_domain_snapshot_to_json during
retrieve and store domain snapshot configuration. it is very useful for me.
> "Ian Jaskson"_email_"Re: [Xen-devel] Discussion on libxl domain configuration synchronization"
> Wei Liu writes ("Discussion on libxl domain configuration synchronization"):
> > ### Exposed API
> >
> > From API design point of view, what API should we expose to
> > applications? We seem to have agreed that a "retrieve" API is
> > certainly necessary (let's name it
> > libxl_retrieve_domain_configuration for ease of discussion), while a
> > "store" / "save" is debatable.
>
> Right.
>
> > If we set the rule that libxl application is not allowed to manipulate domain
> > configuration then "store" is not necessary.
>
> Indeed.  If the libxl application wants to change the domain's setup
> they must make individual calls to add and remove devices, increase
> and decrease memory, etc.
doing the domain snasphot operation is something like "change the domain's setup",
it seems not a good idea to add snapshot infor into domain configuation.


14:35 2014-06-03
software skill, network, ssh, ssh-add; github, error
"Error: Agent admitted failure to sign"

     ssh-add — adds private key identities to the authentication agent

DESCRIPTION
     ssh-add adds private key identities to the authentication agent, ssh-agent(1).  When run without
     arguments, it adds the files ~/.ssh/id_rsa, ~/.ssh/id_dsa, ~/.ssh/id_ecdsa and ~/.ssh/identity.
     After loading a private key, ssh-add will try to load corresponding certificate information from
     the filename obtained by appending -cert.pub to the name of the private key file.  Alternative
     file names can be given on the command line.

reference: https://help.github.com/articles/error-agent-admitted-failure-to-sign

15:06 2014-06-03
software skill, network, ssh, commands
1, http://blog.urfix.com/25-ssh-commands-tricks/
Chinese Translation: http://blog.csdn.net/moubenmao_jun/article/details/10392061
2, ssh port
http://zhumeng8337797.blog.163.com/blog/static/100768914201172125444948/

15:38 2014-06-03
1, Chunyan
1), budget 4000$. 30$/day.

17:43 2014-06-03
xen aarch64
http://community.arm.com/groups/processors/blog/2014/03/28/virtualization-on-arm-with-xen

10:40 2014-06-04
GTD
0, 9:30-18:05

1, today
1), 10:40-11:40 compile and test snapshot.
2), 11:40-12:28 lunch
3), 12:28-13:08 rest
4), 40' nap
5), 40' personal stuff
6), 16:29-17:52 coding excise: assert

13:10 2014-06-04
#if __GNUC__ > 3 || (__GNUC__ == 3 && __GNUC_MINOR__ >= 1)
#define _hidden __attribute__((visibility("hidden")))
#define _protected __attribute__((visibility("protected")))
#else
#define _hidden
#define _protected
#endif

14:35 2014-06-04
suse, doc, core dump
1, application core dump
http://www.novell.com/support/kb/doc.php?id=3054866
2, kernel core dump
http://www.novell.com/support/kb/doc.php?id=3374462

14:48 2014-06-04
snapshot
1, libxl idl
libxl_domain_snapshot = Struct("domain_snapshot",[
    ("name",          string),
    ("creation_time", uint64),
    ("save",          string, {'const': True}),
    ("external",      libxl_defbool),
    ("disks", Array(libxl_disk_snapshot, "num_disks")),
    ])

15:26 2014-06-04
(13:49 2014-06-17)
software skill, man page
1, list man page search path
# echo $MANPATH
/usr/lib64/mpi/gcc/openmpi/share/man:/usr/local/man:/usr/local/share/man:/usr/share/man:/opt/novell/man:/opt/novell/man
2, list man page path
# man -w bash
/usr/share/man/man1/bash.1.gz

15:40 2014-06-04
(16:41 2014-06-05)
mailing list, forum, virtualization
dir.gmane.org is a very good archieve for mailing list and very good rss source. better than gossamer-threads.com
1, xen
http://dir.gmane.org/gmane.comp.emulators.xen.devel
http://www.gossamer-threads.com/lists/xen/devel/
2, qemu
http://www.mail-archive.com/qemu-devel@nongnu.org/
3, libvirt
http://www.mail-archive.com/libvir-list@redhat.com/
4, easy to search
markmail.org

17:07 2014-06-04
software skill, editor, vim, insert TAB
http://stackoverflow.com/questions/4781070/how-to-insert-tab-character-when-expandtab-option-is-on-in-vim

You can use <CTRL-V><Tab> in "insert mode". In insert mode <CTRL-V> inserts a literal copy of your next character.

EDIT: As noted by feedbackloop, on windows you may need to press <CTRL-Q> rather than <CTRL-V>.

17:19 2014-06-04
software skill, build service, linkdiff
  -l, --link  (osc linkdiff): compare against the base revision of
                              the link

17:42 2014-06-04
posic, linux, exit
1, exit status 134

http://stackoverflow.com/questions/2862731/when-assert-fails-what-is-the-program-exit-code

Many UNIX systems will return 128 plus the signal number (SIGABRT is signal number 6) so you may get 134. Whatever you get, it should be documented by the C implementation.

2, exit status
http://www.gnu.org/software/libc/manual/html_node/Exit-Status.html#Exit-Status

When a program exits, it can return to the parent process a small amount of information about the cause of termination, using the exit status. This is a value between 0 and 255 that the exiting process passes as an argument to exit.

Normally you should use the exit status to report very broad information about success or failure. You can't provide a lot of detail about the reasons for the failure, and most parent processes would not want much detail anyway.

There are conventions for what sorts of status values certain programs should return. The most common convention is simply 0 for success and 1 for failure. Programs that perform comparison use a different convention: they use status 1 to indicate a mismatch, and status 2 to indicate an inability to compare. Your program should follow an existing convention if an existing convention makes sense for it.

A general convention reserves status values 128 and up for special purposes. In particular, the value 128 is used to indicate failure to execute another program in a subprocess. This convention is not universally obeyed, but it is a good idea to follow it in your programs.

Warning: Don't try to use the number of errors as the exit status. This is actually not very useful; a parent process would generally not care how many errors occurred. Worse than that, it does not work, because the status value is truncated to eight bits. Thus, if the program tried to report 256 errors, the parent would receive a report of 0 errors—that is, success.

For the same reason, it does not work to use the value of errno as the exit status—these can exceed 255.

Portability note: Some non-POSIX systems use different conventions for exit status values. For greater portability, you can use the macros EXIT_SUCCESS and EXIT_FAILURE for the conventional status value for success and failure, respectively. They are declared in the file stdlib.h.

— Macro: int EXIT_SUCCESS

    This macro can be used with the exit function to indicate successful program completion.

    On POSIX systems, the value of this macro is 0. On other systems, the value might be some other (possibly non-constant) integer expression.

— Macro: int EXIT_FAILURE

    This macro can be used with the exit function to indicate unsuccessful program completion in a general sense.

    On POSIX systems, the value of this macro is 1. On other systems, the value might be some other (possibly non-constant) integer expression. Other nonzero status values also indicate failures. Certain programs use different nonzero status values to indicate particular kinds of "non-success". For example, diff uses status value 1 to mean that the files are different, and 2 or more to mean that there was difficulty in opening the files.

Don't confuse a program's exit status with a process' termination status. There are lots of ways a process can terminate besides having its program finish. In the event that the process termination is caused by program termination (i.e., exit), though, the program's exit status becomes part of the process' termination status.

3, TODO
read check-TEST
libvirt/tests/Makefile

check-TESTS: $(TESTS)
	@failed=0; all=0; xfail=0; xpass=0; skip=0; \
	srcdir=$(srcdir); export srcdir; \
	list=' $(TESTS) '; \
	$(am__tty_colors); \
	if test -n "$$list"; then \
	  for tst in $$list; do \
	    if test -f ./$$tst; then dir=./; \
	    elif test -f $$tst; then dir=; \
	    else dir="$(srcdir)/"; fi; \
	    if $(TESTS_ENVIRONMENT) $${dir}$$tst $(AM_TESTS_FD_REDIRECT); then \
	      all=`expr $$all + 1`; \
	      case " $(XFAIL_TESTS) " in \
	      *[\ \	]$$tst[\ \	]*) \
		xpass=`expr $$xpass + 1`; \
		failed=`expr $$failed + 1`; \
		col=$$red; res=XPASS; \
	      ;; \
	      *) \
		col=$$grn; res=PASS; \
	      ;; \
	      esac; \
	    elif test $$? -ne 77; then \
	      all=`expr $$all + 1`; \
	      case " $(XFAIL_TESTS) " in \
	      *[\ \	]$$tst[\ \	]*) \
		xfail=`expr $$xfail + 1`; \
		col=$$lgn; res=XFAIL; \
	      ;; \
	      *) \
		failed=`expr $$failed + 1`; \
		col=$$red; res=FAIL; \
	      ;; \
	      esac; \
	    else \
	      skip=`expr $$skip + 1`; \
	      col=$$blu; res=SKIP; \
	    fi; \
	    echo "$${col}$$res$${std}: $$tst"; \
	  done; \
	  if test "$$all" -eq 1; then \
	    tests="test"; \
	    All=""; \
	  else \
	    tests="tests"; \
	    All="All "; \
	  fi; \
	  if test "$$failed" -eq 0; then \
	    if test "$$xfail" -eq 0; then \
	      banner="$$All$$all $$tests passed"; \
	    else \
	      if test "$$xfail" -eq 1; then failures=failure; else failures=failures; fi; \
	      banner="$$All$$all $$tests behaved as expected ($$xfail expected $$failures)"; \
	    fi; \
	  else \
	    if test "$$xpass" -eq 0; then \
	      banner="$$failed of $$all $$tests failed"; \
	    else \
	      if test "$$xpass" -eq 1; then passes=pass; else passes=passes; fi; \
	      banner="$$failed of $$all $$tests did not behave as expected ($$xpass unexpected $$passes)"; \
	    fi; \
	  fi; \
	  dashes="$$banner"; \
	  skipped=""; \
	  if test "$$skip" -ne 0; then \
	    if test "$$skip" -eq 1; then \
	      skipped="($$skip test was not run)"; \
	    else \
	      skipped="($$skip tests were not run)"; \
	    fi; \
	    test `echo "$$skipped" | wc -c` -le `echo "$$banner" | wc -c` || \
	      dashes="$$skipped"; \
	  fi; \
	  report=""; \
	  if test "$$failed" -ne 0 && test -n "$(PACKAGE_BUGREPORT)"; then \
	    report="Please report to $(PACKAGE_BUGREPORT)"; \
	    test `echo "$$report" | wc -c` -le `echo "$$banner" | wc -c` || \
	      dashes="$$report"; \
	  fi; \
	  dashes=`echo "$$dashes" | sed s/./=/g`; \
	  if test "$$failed" -eq 0; then \
	    col="$$grn"; \
	  else \
	    col="$$red"; \
	  fi; \
	  echo "$${col}$$dashes$${std}"; \
	  echo "$${col}$$banner$${std}"; \
	  test -z "$$skipped" || echo "$${col}$$skipped$${std}"; \
	  test -z "$$report" || echo "$${col}$$report$${std}"; \
	  echo "$${col}$$dashes$${std}"; \
	  test "$$failed" -eq 0; \
	else :; fi

18:21 2014-6-4
mailing list, arm, linaro
kevin hilman linaro reply to will deacon
> I had a go with this, but I couldn't seem to trigger any context tracking
> without forcing CONFIG_CONTEXT_TRACKING_FORCE=y. Does that mean we're
> missing something else?

No, it just means that you never hit the conditions to trigger full
NOHZ.  Using _FORCE is a good way to do that since it forces the context
tracking paths whether or not it's actually needed by full NOHZ.

09:26 2014-06-05
Jason_email_"[devel] Welcome Juergen Gross"

Hi everyone,

I'm pleased to announce that Juergen Gross has joined SUSE this week as
a member of the Virtualization Team. Juergen will be working with the kernel and xen communities to add features to and increase the stability
of the PVOPS xen kernel.

I'd ask each of you to welcome Juergen to the company and to offer a
helping hand to him whenever possible. Thanks!

Jason


--
Jason Douglas
Sr. Engineering Manager
Virtualization Team
SUSE
jdouglas@suse.com
+1-801-861-1649

09:31 2014-06-05
virtualization, snapshot, GSoC
> Hi there,
>
> I am confused about something here.
> We have this structs.
>
> 1), libxl_snapshot
> store a disk snapshot information, it is used by disk snapshot create
> and delete.
> libxl_disk_snapshot = Struct("disk_snapshot",[
>     ("device",        string),
>     ("name",          string),
>     ("file",          string),
>     ("format",        string),
>     ])
>
> device: device name to snapshot. e.g. sda, hda...
> name: snapshot name given by user. it will the be same name as domain snapshot
> name.
>                                          <---HERE (1
> the following parameter is only useful for external snapshot.
> file: external snapshot file.
> format: the format of external snapshot file
>
> 2), libxl_domain_snapshot
> store domain snapshot information which store in the path shown above. i add
> some api for create, delete and list these information.
> libxl_domain_snapshot = Struct("domain_snapshot",[
>     ("name",          string),
>     ("creation_time", uint64),
>     ("save",          string),
>     ("disks", Array(libxl_disk_snapshot, "num_disks")),
>                        <---HERE (2
>     ])
> name: snapshot name given by user. if user do not provide the name, it will be
> the epoch seconds.
> creation_time: the epoch seconds.
> save: the memory save file for this snapshot.
> disks: store the disk snapshot information associate with this domain
>
> question
> --------------
> How should I deal with domains with multiple disks?Could I assume that
> there is only one disk?
no.
for domain snapshot with internal disk snapshot: savevm/delvm/loadvm will also
do disk snapshot.
for domain snapshot with external disk, the follow api will take disk snapshot
while libvirt will provide the disk list from snapshot.
reference libvirt qemu driver qemuDomainSnapshotCreateDiskActive for how it
works in libxl driver, and qemuMonitorJSONDiskSnapshot show add the disk into
qmp transaction.s

/* create disk snapshot with qmp transaction
 */
int libxl_disk_snapshot_create(libxl_ctx *ctx, int domid,
                               libxl_disk_snapshot *snapshot, int nb);

/* delete disk snapshot with qmp delete(TODO) one by one
 */
int libxl_disk_snapshot_delete(libxl_ctx *ctx, int domid,
                               libxl_disk_snapshot *snapshot, int nb);

> Just some clarification because in the libxl_domain_snapshot we have
> an array of libxl_disk_snapshot
> which somehow suggest you are taking care of domains with multiple disks.
yes, see my comments above.
>
> Regards,
> David.

09:32 2014-06-05
GTD
0, 9:10-19:20

1, today
1), 9:32-11:35 16:01-17:20 snapshot code review.
2), 11:35-12:30 lunch
3), 12:54-15:42 nap 20' read LWN weekly edition.
4), 15:42 16:01 virsh network command. see"16:01 2014-06-05"
5), 18:29-18:57 reply David kiarie email. see"09:31 2014-06-05"
6), 19:00-19:13 try to write email to Wei Liu and Ian Jackson. i am not sure whether i need it or not. see"12:31 2014-06-03"3.

2, next
1), study daemon
1), study about exit and wait.
2), study about socket.

09:32 2014-06-05
(18:21 2014-06-06)
snapshot
1, plan
1), test snapshot with script.
2), write code for external disk snapshot.
3), TODO
(1), only support snapshot all disks?
it will easy to implement but is different from libvirt.
(2), do i need "info snapshots"?
i would be useful for checking before snapshot operations.
(3), i do not know how to write libxlu_disk_l.l, temperary use the libxl_device_disks format.
(4), (21:50 2014-06-13)
readonly device do not need to snapshot.
"21:50 2014-06-13"end
(5), (21:17 2014-06-15)
sort before show snapshot list?
"21:17 2014-06-15"end
4), TODO current
(1), support domain state?
TODO: discuss with Jim.
    bool running;
    bool blocked;
    bool paused;
    bool shutdown;
    bool dying;
2, do
1), done: recheck create and delete function.
2), done: recheck list and revert function.

12:53 2014-06-05
software skill, vnc, openstack, noVNC
http://www.vpsee.com/2013/07/integrating-novnc-with-our-vm-control-panel/
使用 noVNC 开发 Web 虚拟机控制台

http://kanaka.github.io/noVNC/noVNC/vnc.html
http://kanaka.github.io/noVNC/

16:01 2014-06-05
software skill, network, virsh, net
# virsh net-dumpxml nat
<network>
  <name>nat</name>
  <uuid>cb7b045a-3cd2-459e-8625-a754b7a60e5a</uuid>
  <forward mode='nat'>
    <nat>
      <port start='1024' end='65535'/>
    </nat>
  </forward>
  <bridge name='virbr0' stp='on' delay='0'/>
  <mac address='52:54:00:5c:aa:2d'/>
  <domain name='nat'/>
  <ip address='192.168.100.1' netmask='255.255.255.0'>
    <dhcp>
      <range start='192.168.100.128' end='192.168.100.254'/>
    </dhcp>
  </ip>
</network>

# virsh net-info nat
Name:           nat
UUID:           cb7b045a-3cd2-459e-8625-a754b7a60e5a
Active:         yes
Persistent:     yes
Autostart:      yes
Bridge:         virbr0

09:46 2014-06-06
GTD
0, 9:30

1, today
1), 9:50-10:04 mailing list: arm. see"10:00 2014-06-06"
2), 10:04-11:34 14:32-15:50 17:39-18:50 snapshot. see"10:04 2014-06-06"
3), 11:34-12:14 lunch.
4), 13:52-14:32 nap.

2, next
1), try opensuse aarch64. http://en.opensuse.org/openSUSE:AArch64
1), study daemon
1), study about exit and wait.
2), study about socket.

10:00 2014-06-06
mailing list, virtualization, xen, arm, aarch64
1, http://lists.xen.org/archives/html/xen-devel/2014-06/msg00606.html
[Xen-devel] [PATCH] xen: arm: implement generic multiboot compatibility strings (Was: Re: [Linaro-uefi] The GRUB multiboot support patch for aarch64(V3.1))

This causes Xen to accept the more generic names originally proposed by
Andre in http://thread.gmane.org/gmane.linux.linaro.announce.boot/326 and
incorporated into the proposal in
http://wiki.xen.org/wiki/Xen_ARM_with_Virtualization_Extensions/Multiboot

2, details
http://thread.gmane.org/gmane.linux.linaro.announce.boot/326
From: Andre Przywara <andre.przywara@...>
Subject: [PROPOSAL] ARM/FDT: passing multiple binaries to a kernel
Newsgroups: gmane.linux.linaro.announce.boot, gmane.linux.drivers.devicetree, gmane.linux.ports.arm.kernel
Date: 2013-09-03 15:53:44 GMT (39 weeks, 2 days, 9 hours and 55 minutes ago)

Hi,

a normal Linux kernel currently supports reading the start and end
address of a single binary blob via the FDT's /chosen node.
This will be interpreted as the location of an initial RAM disk.

The Xen hypervisor itself is a kernel, but needs up to _two_ binaries
for proper operation: a Dom0 Linux kernel and it's associated initrd.
On x86 this is solved via the multiboot protocol used by the Grub
bootloader, which supports to pass an arbitrary number of binary modules
to any kernel.

Since in the ARM world we have the versatile device tree, we don't need
to implement the mulitboot protocol.

So I'd like to propose a new binding which denotes binary modules a
kernel can use at it's own discretion.
The need is triggered by the Xen hypervisor (which already uses a very
similar scheme), but the approach is deliberately chosen to be as
generic as possible to allow future uses (like passing firmware blobs
for devices or the like).
Credits for this go to Ian Campbell, who started something very similar
[1] for the Xen hypervisor. The intention of this proposal is to make
this generic and publicly documented.

Looking forward to any comments!

Thanks,
Andre.

[1]
http://xenbits.xen.org/gitweb/?p=xen.git;a=blob;f=docs/misc/arm/device-tree/booting.txt;h=94cd3f18a4e1317a35e1255bf5c6e1e091001d1a;hb=HEAD
----------------------------
* Multiple boot modules device tree bindings

Boot loaders wanting to pass multiple additional binaries to a kernel
shall add a node "module" for each binary blob under the /chosen node
with the following properties:

- compatible:
     compatible = "boot,module";
   A bootloader may add names to more specifically describe the module,
   e.g. Xen may use "xen,dom0-kernel" or "xen,dom0-ramdisk".
   If possible a kernel should be able to use modules even without a
   descriptive naming, by enumerating them in order and using hard-coded
   meanings for each module (e.g. first is kernel, second is initrd).

- reg: specifies the base physical address and size of a region in
   memory where the bootloader loaded the respective binary data to.

- bootargs:
   An optional property describing arguments to use for this module.
   Could be a command line or configuration data.

Example:
/chosen {
     #size-cells = <0x1>;
     #address-cells = <0x1>;
     module <at> 0 {
         compatible = "xen,linux-zimage", "xen,multiboot-module",
"boot,module";
         reg = <0x80000000 0x003dcff8>;
         bootargs = "console=hvc0 earlyprintk ro root=/dev/sda1 nosmp";
     };
     module <at> 1 {
         compatible = "xen,linux-initrd", "xen,multiboot-module",
"boot,module";
         reg = <0x08000000 0x00123456>;
     };
...

10:04 2014-06-06
(22:08 2014-06-09)
snapshot, Jim
1, Jim
1),
> Bamvor Jian Zhang wrote:
> > Hi,
> >
> > here is the third version about domain snapshot documents, the second version
> > and the first version is here[1][2].
> >
> > there are lots of potential feature about snapshots. for now, i focus on
> > providing the api for libvirt libxl driver in order to support the same
> > functionality compare with libvirt qemu driver. and you may already notice
> > david is working on libvirt libxl driver. it is a GSoC project[3]. it is
> > important for him to know the api in order to start coding in libvirt side.
> >
>
> Right.  We can't do much on the libvirt side without libxl APIs :-).
>
> > i plan to work on other "advanced feature" after my first stage patch ack.
> >
> > there are two types of snapshots supported by libxl: disk snapshot and domain
> > snapshot and four types of operations: create, delete, list and revert.
> >
>
> Should the operations also include 'info', to get name, creation time,
> disks, etc. on a particular snapshot?
list with --long options will do this: list details of one/all snapshot
information. reference our discuss here[1].
>
> > Disk snapshot will only be crash-consistent if the domain is running. Disk
> > snapshots can also be internal (qcow2) or external (snapshot in one file,
> > delta
> > in another).
> >
> > Domain snapshots include disk snapshots and domain state, allowing to resume
> > the domain from the same state when the snapshot was created. This type of
> > snapshot is also referred to as a domain checkpoint or system checkpoint.
> >
> > In libvirt, there is a something like resource manager for domain snapshot
> > managements. So, in libxl, all these information is transfered through
> > libxl_domain_snapshot struct. xl will manage the snapshot by itself.
> >
> > Domain snapshot create means save domain state and do disk snapshots.
> > At the beginning of domain snapshot create, it will check whether it is
> > snapshotable. it is snapshotable if all the disk is qdisk backed.
> >
> > Domain snapshot revert means rollback the current snapshot state. and
> > Because the limitation of the qemu qmp, the revert could only support domain
> > snapshot with internal disk snapshot. revert the domain snapshot with external
> > snapshot doest not support.
> >
> > there are live flag in snasphot configuration file, it will be save domain
> > memory and do external disk snapshot. to make the thing simple, i do not want
> > to implement in my first verion of patch.
> >
> > As Ian Campbell said, the support of non-qdisk snapshot is very useful.
> > unfortuntely, i have no idea what it need to do. the only non-qdisk i know is
> > blktap. and i do not know does how to do snapshot create, delete, list and
> > revert for blktap? does it support internal or external support?
> >
>
> Looking in tools/blktap2, it seems only external snapshots are supported
> via td-util or vhd-util.  I suppose lvm would also be considered an
> external disk snapshot.
thanks. do you mean call these utils in libxl__exec? it seems that only a few
binary called by this functions: bootloader, libxl-save-helper, qemu-dm,
qemu-system-i386.
the core function of vhd_util snapshot is vhd_snashot which defined in
(tools/blktap2/include/libvhd.h, /usr/lib64/libvhd.so). although it included in
xen-devel package in opensuse12.3. maybe it is a better choice?
>
> > i treat it as an "advanced" feature, i will not cover it in my first version
> > of
> > patch.
> >
> > the new struct, api and command is as follows:
> > 1, new struct
> > 1), libxl_snapshot
> > store a disk snapshot information, it is used by disk snapshot create and
> > delete.
> > libxl_disk_snapshot = Struct("disk_snapshot",[
> >     ("device",        string),
> >     ("name",          string),
> >     ("file",          string),
> >     ("format",        string),
> >
>
> Should format be an enum?
yeah, it should be. i will use as libxl_disk_format. i only support qcow2 as
external snapshot image.
>
> >     ])
> >
> > device: device name to snapshot. e.g. sda, hda...
> > name: snapshot name given by user. it will the be same name as domain snapshot
> > name.
> > the following paramenter is only useful for external snapshot.
> > file: external snapshot file.
> > format: the format of external snapshot file
> >
> > 2), libxl_domain_snapshot
> > store domain snapshot information which store in the path shown above. i add
> > some api for create, delete and list these information.
> > libxl_domain_snapshot = Struct("domain_snapshot",[
> >     ("name",          string),
> >     ("creation_time", uint64),
> >     ("save",          string),
> >     ("disks", Array(libxl_disk_snapshot, "num_disks")),
> >     ])
> >
>
> I think 'description' and 'state' would be useful fields, the latter
> being the state of the domain when the snapshot was created.
sorry missing this. when you mention state, do you mean the following
combination in libxl_dominfo?
    bool running;
    bool blocked;
    bool paused;
    bool shutdown;
    bool dying;
>
> > name: snapshot name given by user. if user do not provide the name, it will be
> > the epoch seconds.
> > creation_time: the epoch seconds.
> > save: the memory save file for this snapshot.
> > disks: store the disk snapshot information assoiate with this domain.
> >
> > 2, new functions
> > there is no common api like libxl_snapshot_xxx. the reason is that different
> > toolstack may need to different event handling machanism(synchronize or
> > asynchronize). and obviously, domain snapshot create need async handler. so i
> > decide to only provide the sub api for xl and other toolstack(e.g. libvirt).
> > it make eailer for toolstack to handle the event by themselves.
> >
>
> What is the domain_snapshot struct used for then?  Seems it is unneeded
> if not exposed in the API, something to be declared by the app.  I'd
> like to hear what the Xen tools devs think about this approach.  Ian J,
> Ian C, Anthony?
currently, libxl_domain_snapshot is only used by xl_cmdimpl.c.
domain load snapshot configuration may be useful when libvirt libxl driver
want to load libxl snapshot configuration.
there is a issue i want to discuss with you. do we need libvirt and libxl
use the same domain snapshot configuration? when domain snapshot is created
or reloading, use libxl-json format file instead of libvirt xml format. in this
way, it is easy to handle the snapshot if user migrate between libxl and libvirt
toolstack.
>
> > 1), in libxl/libxl.h
> > the implementation will be located in libxl_snapshot.c
> > /* disk snapshot api
> >  * support create for external and internal disks, support delete for internal
> >  * snapshot of disks.
> >  */
> > /* create disk snapshot according to the device name in snapshot array. nb is
> >  * the number of snapshot array.
> >  * use the qmp transaction to ensure all snapshot of disk is coherence.
> >  */
> > int libxl_disk_snapshot_create(libxl_ctx *ctx, int domid,
> >                                libxl_disk_snapshot *snapshot, int nb,
> >                                const libxl_asyncop_how *ao_how);
> > /* delete number of nb disk snapshot describe in snapshot array
> >  */
> > int libxl_disk_snapshot_delete(libxl_ctx *ctx, int domid,
> >                                libxl_disk_snapshot *snapshot, int nb);
> >
>
> As David mentioned in his reply, there should be a revert as well.  Is
> list (and info if added) handled by the app (xl, libvirt, etc)?
for revert, reference my previous reply.
for list, it will be handled by the app, just like libvirt qemu driver.
>
> > 2), xl_cmdimpl.c
> > int libxl_snapshot_create(int domid, libxl_domain_snapshot *snapshot);
> > int libxl_snapshot_delete(int domid, libxl_domain_snapshot *snapshot);
> > int libxl_snapshot_get(int domid, libxl_domain_snapshot *snapshot, int nb);
> > int libxl_snapshot_revert(int domid, libxl_domain_snapshot *snapshot);
> >
> > support create, delete, list and revert for domain snasphot.
> >
> > libxl_snapshot_get will read the domain snapshot configuration file stored in
> > disk and list snapshot information in simple or long format.
> >
> > 3, snapshot information file
> > i will write manpage for this with patch.
> >
> > i found the Wei v5 patch about xl json format.
> > http://lists.xen.org/archives/html/xen-devel/2014-05/msg01670.html
> > it seems that i could use these apis for parsing and generating the snapshot
> > information file.
> >
>
> Yeah, with all of this work, seems json would be a better way to go.
>
> Regards,
> Jim
>
> > the domain snapshot information will store in the follow path:
> > /var/lib/xen/snapshots/<domain_uuid>/snapshotdata-<snapshot_name>.xl
> >
> > here is an example for snapshot information file:
> > description="a snapshot after installation"
> > name="1397207577"
> > creationtime="1397207577"
> > save="1397207577.save"
> > type="internal"/"external"
> > live="no"
> > disk_only="no"
> > disk=[ 'hda=disk_hda.qcow2,type=qcow2', 'hdc=disk_hdc.qcow2,type=qcow2']
> >
> > the save and disk image file base on the path of
> > "/var/lib/xen/snapshots/<domain_uuid>"
> >
> > the user could give a snapshot name when vm snapshot created. if not, the
> > epoch
> > seconds will set as name as the above examples.
> >
> > 3, new command
> > i will write manpage for this with patch.
> > 1), snapshot-create
> > Usage: xl snapshot-create <ConfigFile> [options] [Domain]
> >
> > Create domain snapshot with ConfigFile or options
> >
> > Options:
> > -n                snapshot name
> > --live            do live snapshot
> > --disk-only       only disk snapshot, do not save memory.
> >
> > 2), snapshot-list
> > Usage: xl snapshot-list [options] [Domain]
> >
> > List domain snapshot information about all/some snapshot in one domain.
> >
> > Options:
> > -l, --long        Output all domain snapshot details
> > -n                snapshot name
> >
> > 3), snapshot-delete
> > Usage: xl snapshot-delete [options] [Domain]
> >
> > Delete domain snapshot relative data, including domain state, disk snapshot
> > and domain snapshot information file.
> >
> > Options:
> > -n                snapshot name
> >
> > 4), snapshot-revert
> > Usage: xl snapshot-revert [options] [Domain]
> >
> > Rollback the domain to snapshot state.
> >
> > Options:
> > -n                snapshot name
> >
> > [1] http://lists.xen.org/archives/html/xen-devel/2014-04/msg00414.html
> >     http://lists.xen.org/archives/html/xen-devel/2014-04/msg00244.html
> > [2] http://lists.xen.org/archives/html/xen-devel/2014-04/msg02549.html
> > [3]
> > http://en.opensuse.org/openSUSE:GSOC_ideas#Add_virtual_machine_snapshot_support_to_libvirt_Xen_driver
> >
> > changes since v2:
> > 1), reorgnized the whole docments.
> > 2), do not export the dedicated the disk snapshot commands.
> > 3), others changes according to Ian and Jim's comment.
> >
> > regards
> >
> > bamvor
> >
> >
[1] http://lists.xen.org/archives/html/xen-devel/2014-04/msg02904.html

2),
Hi, Jim
> Bamvor Jian Zhang wrote:
> > Hi, David
> >
> >> On Fri, May 16, 2014 at 6:00 PM, Bamvor Jian Zhang <bjzhang@xxxxxxxx> wrote:
> >>
> >>
> >>> Hi, david
> >>>
> >>>
> >>>> On Thu, May 15, 2014 at 4:58 PM, Bamvor Jian Zhang <bjzhang@xxxxxxxx>
> >>>>
> >>> wrote:
> >>>
> >>>>> Hi,
> >>>>>
> >>>>> here is the third version about domain snapshot documents, the second
> >>>>> version
> >>>>> and the first version is here[1][2].
> >>>>>
> >>> ...
> >>>
> >>>>> 2, new functions
> >>>>> there is no common api like libxl_snapshot_xxx. the reason is that
> >>>>> different
> >>>>> toolstack may need to different event handling machanism(synchronize or
> >>>>> asynchronize). and obviously, domain snapshot create need async
> >>>>>
> >>> handler.
> >>>
> >>>>> so i
> >>>>> decide to only provide the sub api for xl and other toolstack(e.g.
> >>>>> libvirt).
> >>>>> it make eailer for toolstack to handle the event by themselves.
> >>>>>
> >>>>> 1), in libxl/libxl.h
> >>>>> the implementation will be located in libxl_snapshot.c
> >>>>> /* disk snapshot api
> >>>>>  * support create for external and internal disks, support delete for
> >>>>> internal
> >>>>>  * snapshot of disks.
> >>>>>  */
> >>>>> /* create disk snapshot according to the device name in snapshot
> >>>>>
> >>> array. nb
> >>>
> >>>>> is
> >>>>>  * the number of snapshot array.
> >>>>>  * use the qmp transaction to ensure all snapshot of disk is coherence.
> >>>>>  */
> >>>>> int libxl_disk_snapshot_create(libxl_ctx *ctx, int domid,
> >>>>>                                libxl_disk_snapshot *snapshot, int nb,
> >>>>>                                const libxl_asyncop_how *ao_how);
> >>>>> /* delete number of nb disk snapshot describe in snapshot array
> >>>>>  */
> >>>>> int libxl_disk_snapshot_delete(libxl_ctx *ctx, int domid,
> >>>>>                                libxl_disk_snapshot *snapshot, int nb);
> >>>>>
> >> Are this the only two functions you are exposing?I mean the API?or am I
> >> getting something wrong?
> >>
> >>
> > there are functions libxl_qmp_loadvm, libxl_qmp_savevm which is called by
> > domain internal snapshot create/revert. currently, they are internal
> > functions.
> > (libxl_intenal.h). but i should expose them, e.g. look like:
> > int libxl_loadvm(libxl_ctx *ctx, int domid, libxl_domain_snapshot *snapshot,
> >                  const libxl_asyncop_how *ao_how);
> > int libxl_savevm(libxl_ctx *ctx, int domid, libxl_domain_snapshot *snapshot,
> >                  const libxl_asyncop_how *ao_how);
> >
>
> IMO, would be better to have libxl_disk_snapshot_create() and
> libxl_disk_snapshot_delete() exposed, and keep these as internal.
i am not sure if i could follow you. i guess you mean we should expose unique
api for dedicate operations. this is what i want.
but the issue is there is no single revert disk snapshot qmp or hmp in qemu.
so, in libvirt qemu driver, internal or external disk snapshot call
loadvm/savevm/delvm and transaction/blockdev-snapshot-sync/
blockdev-snapshot-internal-sync respectively.
IMO, it will be better if qemu support blockdev snapshot revert qmp which expose
bdrv_snapshot_goto to qemu user(e.g. libxl).
i do not know how qemu upstream think about it.  how about discuss it later
when basic snapshot function accept in libxl?

regards

bamvor

>
> Regards,
> Jim
>

13:27 2014-06-06
software skill, build sercie, obs, maintenance
http://doc.opensuse.org/products/draft/OBS/obs-reference-guide_draft/cha.obs.maintenance_setup.html
osc maintenancerequest(mr)

10:58 2014-06-09
software skill, yast, yast2, debug; strace, only trace specific system call
1, http://en.opensuse.org/openSUSE:YaST_debugging
Logs

The logs are in /var/log/YaST2. The primary log file is y2log which is rotated to y2log-[1-9]. If YaST is run as non-root, the logs are ~/.y2log, ~/.y2log-[1-9].

More Log Messages

Debugging log messages can be enabled in several ways:

Y2DEBUG=1
this is an environment variable. Use like Y2DEBUG=1 /sbin/yast2 lan. It also works on the kernel command line during the installation.
UI dialog
Press Shift-F7 in the Qt UI to switch the debugging messages at run time.
signal
Send the y2base process a USR1 signal to toggle debug logging. killall -USR1 y2base
There also are environment variables for controlling the level of logging libzypp (package management library used by YaST) events:

ZYPP_MEDIA_CURL_DEBUG=1
enables logging of detailed CURL information. Useful for debugging network-related problems with installation or package management.
strace

Strace is very useful for debugging YaST and other programs too. Here are the basics.

This is a frequently useful line: Y2DEBUG=1 strace -eopen -ostrace.log /usr/lib/YaST2/bin/y2base lan qt.

2, bamvor:
"-eopen" mean that only trace open system call.

11:00 2014-06-09
GTD
0, 9:50-18:05

1, today
1), 14:01-18:01 snapshot.
2), 11:40-12:30 lunch.

12:29 2014-06-09
真功夫
1, zhangjian: 28.5
2, 茄子：邹钰，wangliuhua。19.5
3，香干：lixia，chunyan. 25.2

13:01 2014-06-09
bug, virtualization, xen
https://bugzilla.novell.com/show_bug.cgi?id=880636

13:02 2014-06-09
fail between the follow line:
(XEN) Dom0 has maximum 2 VCPUs
(XEN) elf_load_binary: phdr 0 at 0xffffffff80002000 -> 0xffffffff8087d000

13:52 2014-06-09
default password: N0vell.

16:32 2014-06-09
virtualization, vnc, xl, virt-viewer, virt-manager
1, there is a difference between "xl vnc" and virt-viewer/virt-manager, the former one call vncviewer directly while the latter one use its own vnc client.
PS: i do not read the code, i just check that whether there is a vncviewer process exist.

10:06 2014-06-10
GTD
0, 9:50

1, today
1), 10:06-10:21 mailing list. see"10:21 2014-06-10"
2), 10:21-11:09 14:30-14:47 xen, memory, balloon(?), guest: windows, bnc#881609. see"10:26 2014-06-10"
3), 11:21-11:40 13:14-14:30 14:54-15:03 16:30-18:28 snapshot. check email. testing.
4), 11:40-12:43 lunch.
5), 15:03- sync.

10:15 2014-06-10
mailing list, virtualization, xen; bootloader, uefi
1, ijc at hellion
Jun 5, 2014, 11:01 AM
Re: [Linaro-uefi] [PATCH] xen: arm: implement generic multiboot compatibility strings (Was: Re: The GRUB multiboot support patch for aarch64(V3.1)) [In reply to]
On Thu, 2014-06-05 at 18:05 +0100, Leif Lindholm wrote:
> On Thu, Jun 05, 2014 at 05:55:31PM +0100, Ian Campbell wrote:
> > > > - if ( fdt_node_check_compatible(fdt, node, "xen,linux-zimage") == 0 )
> > > > + if ( fdt_node_check_compatible(fdt, node, "xen,linux-zimage") == 0 ||
> > > > + fdt_node_check_compatible(fdt, node, "multiboot,linux-zimage") == 0 )
> > >
> > > While we are modifying the protocol, "linux-zImage" is confusing in the
> > > name. Actually we can use it for an ELF, another OS... I don't think Xen
> > > will change his behavior depending of the DOM0 image.
> >
> > zImage defines the boot protocol to use. Since the protocol is defined
> > by Linux as zImage I think that is the appropriate name, if some other
> > OS wants to mimic Linux then fine. But if we end up supporting some OS
> > with its own boot protocol which doesn't match Linux's then that should
> > have a distinct name of its own.
>
> Actually, there is no zImage support in arm64 - only Image.

Right, that's a slight wrinkle in the naming. arm64's image is
essentially equivalent (from a calling into it PoV) to zImage, it
doesn't seem worth dupping the name here.

Ian.

2, fu.wei at linaro (RedHat)
Jun 6, 2014, 5:24 AM
Re: [Linaro-uefi] [PATCH] xen: arm: implement generic multiboot compatibility strings (Was: Re: The GRUB multiboot support patch for aarch64(V3.1)) [In reply to]
On 06/06/2014 02:31 AM, Ian Campbell wrote:
> On Thu, 2014-06-05 at 18:03 +0100, Julien Grall wrote:
>>>> While we are modifying the protocol, "linux-zImage" is confusing in the
>>>> name. Actually we can use it for an ELF, another OS... I don't think Xen
>>>> will change his behavior depending of the DOM0 image.
>
> Actually thinking about this some more I think you are right. Xen
> already probes the kernel it gets so we can safely implement this as
> multiboot,kernel, since we don't really need the more specific type. If
> in the future some non-probable kernel comes along which we want to
> support we still have the option of adding more specific compatibility
> strings.
>
> Fu Wei -- if this is OK with you I will modify the wiki page to
> s/multiboot,linux-zimage/multiboot,kernel/ and rev this patch to suit.

This is OK for me, And I think the "multiboot,kernel" is better and more generic. :-)

>
> Can we do something similar with linux-ramdisk? I'm not sure since we
> cannot easily probe the ramdisk contents. We could base the ramdisk
> behaviour on the probed behaviour of the kernel. Anyone got any
> thoughts?

My thought looks exactly the same as yours :
The cpio utility can detect the cpio file format. Maybe we can just probe the file, see if this is a cpio or cpio.gz.

>
> Ian.
>

--
Best regards,

Fu Wei
Enterprise Server Engineer From Red Hat
LEG Team
Linaro.org | Open source software for ARM SoCs
Ph: +86 186 2020 4684 (mobile)
IRC: fuwei
Skype: tekkamanninja
Room 1512, Regus One Corporate Avenue,Level 15,
One Corporate Avenue,222 Hubin Road,Huangpu District,
Shanghai,China 200021

10:26 2014-06-10
virtualization, xen, memory, balloon(?), guest: windows, bnc#881609
Private Lin Ma 2014-06-09 09:34:35 UTC

After turning balloon off, The "Complete memory dump" completes successfully
and no error output on qemu-dm log.
Hope this could be a piece of clue for fixing the problem.

Private Comment 4 Kirk Allan 2014-06-09 22:47:10 UTC

With the clue of turning off ballooning, I noticed that the
XENMEM_maximum_reservation and XENMEN_current_reservation yield different
values for the same memory configuration for vms running on a sles 11 host and
on a sles 12 host. The different values cause the balloon driver to balloon
memory.  Then when the crash dump happens, the dump tries to access the
ballooned pages which causes the vm to hang.

11:41 2014-06-11
GTD
1, today
1), 11:40-11:53 lunch.
2), 10' mailing list. see"12:00 2014-06-11".
3), kindle rss.
4), 14:14-14:39 nap.

2, next
1), TODO test snasphot create fail. where is the log?

12:00 2014-06-11
mailing list, virtualization, xen, qemu for dom0
"Ian Campbell"_reply_"Steven Haigh <netwiz@xxxxxxxxx>"_email_"Re: [Xen-devel] Starting QEMU as disk backend for dom0"_20140606
On Fri, 2014-06-06 at 15:55 +1000, Steven Haigh wrote:
> Hi guys,
>
> I'm just picking through the xencommons initscript provided with the Xen
> 4.4 source tarball...
>
> I notice this section starting at line 119:
>         echo Starting QEMU as disk backend for dom0
>         test -z "$QEMU_XEN" && QEMU_XEN="${LIBEXEC}/qemu-system-i386"
>         $QEMU_XEN -xen-domid 0 -xen-attach -name dom0 -nographic -M
> xenpv -daemonize \
>                 -monitor /dev/null -serial /dev/null -parallel /dev/null \
>                 -pidfile $QEMU_PIDFILE
>
> Can anyone tell me the purpose of this?

It is used if dom0 needs to be able to access a virtual disk in a
non-trivial form, i.e. qcow or vhd. In this case a vbd is attached to
dom0 which is backed by qdisk in this qemu process. For trivial disk
formats like raw files or block partitions dom0 can just access them
directly.

This comes into play when you are using pygrub, since dom0 needs to be
able to read guest disks. I'm not sure if their are other places. I
suppose you can also do "xl block-attach 0 <spec>" if you like e.g. in
order to mount a qcow image in dom0 for some reason.

Ian.

15:07 2014-06-11
software skill, debugger, gdb
1, condition breakpoints on return value
1), http://stackoverflow.com/questions/4498965/how-to-set-conditional-breakpoint-if-malloc-returns-null-via-gdb
2), http://stackoverflow.com/questions/1378594/is-it-possible-to-set-a-conditional-breakpoint-at-the-end-of-a-function-based-on
Agree with previous commenter that this is probably something you don't want to do, but for me, setting a conditional breakpoint at the last instruction on $eax (or $rax if you are on 64-bit x86) works just fine.

For the code

unsigned int foo(void) { return 1; }
unsigned int bar(void) { return 4; }
unsigned int myFunc(void) { return foo()+bar(); }

using gdb ..

(gdb) disass myFunc
Dump of assembler code for function myFunc:
0x080483d8 <myFunc+0>:  push   %ebp
0x080483d9 <myFunc+1>:  mov    %esp,%ebp
0x080483db <myFunc+3>:  push   %ebx
0x080483dc <myFunc+4>:  call   0x80483c4 <foo>
0x080483e1 <myFunc+9>:  mov    %eax,%ebx
0x080483e3 <myFunc+11>: call   0x80483ce <bar>
0x080483e8 <myFunc+16>: lea    (%ebx,%eax,1),%eax
0x080483eb <myFunc+19>: pop    %ebx
0x080483ec <myFunc+20>: pop    %ebp
0x080483ed <myFunc+21>: ret
End of assembler dump.
(gdb) b *0x080483ed if $eax==5
Breakpoint 1 at 0x80483ed
(gdb) run
Starting program: /tmp/x
Breakpoint 1, 0x080483ed in myFunc ()
(gdb)

2, using gdb variable for function calling counter comparasion.
http://blog.timac.org/?p=118

17:01 2014-06-11
qemu disk
  <devices>
    <emulator>/usr/lib/xen/bin/qemu-system-i386</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2'/>
      <source file='/home/bamvor/vm/xen/images/disk0.qcow2'/>
      <target dev='hda' bus='ide'/>
      <address type='drive' controller='0' bus='0' target='0' unit='0'/>
    </disk>
    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source file='/home/bamvor/vm/iso/win-7-fcs-64-auto.iso'/>
      <target dev='hdb' bus='ide'/>
      <readonly/>
      <address type='drive' controller='0' bus='0' target='0' unit='1'/>
    </disk>
    ...
  </devices>

17:48 2014-06-11
software skill, virtualization, xen, hypervisor, debug, log level
grub2
multiboot /boot/xen.gz loglvl=all guest_loglvl=all watchdog=true watchdog_timeout=10

10:55 2014-06-12
software skill, virtualization, vnc, remote
1, libvirt do not allow remote vnc connection by default. after update the following vnc_listen line you need to restart libvirtd and reboot virtual machine.
1), /etc/libvirt/qemu.conf
# VNC is configured to listen on 127.0.0.1 by default.
# To make it listen on all public interfaces, uncomment
# this next option.
#
# NB, strong recommendation to enable TLS + x509 certificate
# verification when allowing public access
#
vnc_listen = "0.0.0.0"

2), qemu command before and after reboot "-vnc 127.0.0.1:8" to "-vnc 0.0.0.0:8"
note that the following command line do not come from same vm(look at the -name parameter). it is just a example for comparasion vnc listen port.
before:
root     26449     1  1 May12 ?        09:16:09 /usr/bin/qemu-kvm -name cl1_n1_sles12 -S -machine pc-i440fx-1.4,accel=kvm,usb=off -m 768 -smp 4,sockets=4,cores=1,threads=1 -uuid b887a3be-7939-510f-d0bb-0213176bfad1 -no-user-config -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/cl1_n1_sles12.monitor,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown -device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 -drive file=/mnt/vm/cl1_n1_sles12/disk0.raw,if=none,id=drive-virtio-disk0,format=raw -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x4,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1 -netdev tap,fd=24,id=hostnet0 -device rtl8139,netdev=hostnet0,id=net0,mac=52:54:00:98:10:7f,bus=pci.0,addr=0x3 -chardev socket,id=charserial0,host=127.0.0.1,port=4555,server,nowait -device isa-serial,chardev=charserial0,id=serial0 -vnc 127.0.0.1:0 -vga cirrus -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5
after:
root     16220     1  8 10:55 ?        00:00:16 /usr/bin/qemu-kvm -name cldmzhang_ttestlifecycle -S -machine pc-i440fx-1.4,accel=kvm,usb=off -m 512 -smp 4,sockets=4,cores=1,threads=1 -uuid 6af9b2a7-db85-72b2-62a6-3f8c9c92f613 -no-user-config -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/cldmzhang_ttestlifecycle.monitor,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown -device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 -drive file=/var/lib/kvm/images/cldmzhang_ttestlifecycle/disk0.raw,if=none,id=drive-virtio-disk0,format=raw -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x4,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1 -netdev tap,fd=28,id=hostnet0,vhost=on,vhostfd=29 -device virtio-net-pci,netdev=hostnet0,id=net0,mac=52:54:00:6c:aa:2d,bus=pci.0,addr=0x3 -vnc 0.0.0.0:8 -vga cirrus -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5

11:13 2014-06-12
GTD
0, 9:45

1, today
1), 10' support Yu Zou for libvirt snapshot.
2), 12:53-13:12 taobao.
3), 13:12-13:26 news.
4), 13:26-14:00 send libvirt patch.
5), 15:41- snapshot.

10:02 2014-06-13
GTD
0, 9:50-17:10

1, today
1), 10:02-11:06 kindle rss.
2), 11:06-11:30 libvirt patch test.
3), 11:30-12:20 lunch.
4), 12:48-13:15 block device: virio-blk-data-plane
5), 13:20-14:02 talk with Jim: about domxml-to-native and snapshot work. Jim will send me a email about snapshot.
6), 14:02-14:40 nap.
7), 15:15-17:07 snapshot.

12:48 2014-06-13
virtualization, kvm, virtio, block device, virio-blk-data-plane
1, The Key Performance Challenges
1), Low throughput / high latencies (compared to bare metal)
Generally less than 30% of bare metal → configuration issue(s)
Between 30% and 60% of bare metal → performance tuning
With proper configuration + performance tuning → 90% or more
2), Low I/O rates (IOPS)
Some enterprise workloads require 100Ks I/Os Per Second (IOPS)
Most difficult challenge for I/O virtualization!
Current KVM tops out at ~147,000 IOPS
VMware claimed vSphere v5.1 could achieve 1.06 million IOPS for a single guest

2, approach
1), Vhost-blk
– Initially coded by Liu Yuan, new prototype by Asias He
– Submits guest I/Os directly to host via kernel threads (similar to vhost_net for network)
– Drawbacks:
• Involves the kernel (ring 0 privilege, etc.)
• Cannot take advantage of QEMU features (e.g. image formats, etc.)
• Cannot support live migration
2), “Data-Plane” QEMU
– Coded by Stefan Hajnoczi (~1500 LOC)
– Submits guest I/Os directly to host in user space (one user-space thread per virtual
        block device)
– Will become default mode of operations – eventually
– No kernel change is required

3, how to use data plane
Libvirt Domain XML
<domain type='kvm' xmlns:qemu='http://libvirt.org/schemas/domain/qemu/1.0'>
...
<disk type='file' device='disk'>
<driver name='qemu' type='raw' cache='none' io='native'/>
<source file='path/to/disk.img'/>
<target dev='vda' bus='virtio'/>
<address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
</disk>
...
<qemu:commandline>
<qemu:arg value='-set'/>
<qemu:arg value='device.virtio-disk0.scsi=off'/>
</qemu:commandline>
<!-- config-wce=off is not needed in RHEL 6.4 -->
<qemu:commandline>
<qemu:arg value='-set'/>
<qemu:arg value='device.virtio-disk0.config-wce=off'/>
</qemu:commandline>
<qemu:commandline>
<qemu:arg value='-set'/>
<qemu:arg value='device.virtio-disk0.x-data-plane=on'/>
</qemu:commandline>
<domain>
QEMU command-line
qemu -drive if=none,id=drive0,cache=none,aio=native,format=raw,file=path/to/disk.img \
-device virtio-blk,drive=drive0,scsi=off,config-wce=off,x-data-plane=on

4, Current limitations
– Only raw image format is supported
• Other image formats depend on QEMU block layer
–
 Live migration is not supported
–
 Hot unplug and block jobs are not supported
–
 I/O throttling limits are ignored
–
 Only Linux hosts are supported (due to Linux AIO usage)

5, On-going work (for upcoming QEMU releases)
– Patches have recently been submitted upstream to convert virtio-net to use
“data-plane”
– Reduce the scope of big QEMU lock → moving to RCU (Read Copy Update)

15:56 2014-06-13
software skill, shell, cat
> cat > test.sh << EOF
adf
d
d
EOF

notes: EOF could be any string.

19:27 2014-06-13
snapshot
1, there is a timeout in libxl qmp?
# xl hmp bjz_04_sles11_sp2 info version "return: 2.0.0"
domid<2> hmp<info>, hmp_arg<version>, expect<return: 2.0.0>
run info version hmp command through qmp interface
hmp_callback enter
return: 2.0.0

# xl hmp bjz_04_sles11_sp2 savevm 1926 1
domid<2> hmp<savevm>, hmp_arg<1926>, expect<1>
run savevm 1926 hmp command through qmp interface
libxl: error: libxl_qmp.c:450:qmp_next: Socket read error: Connection reset by peer

2, it is bus error
(gdb) where
#0  buffer_find_nonzero_offset (buf=buf@entry=0x7fdaa5f9c000, len=len@entry=4096) at util/cutils.c:200
#1  0x00007fdb07c3057d in is_zero_range (size=4096, p=
    0x7fdaa5f9c000 <Address 0x7fdaa5f9c000 out of bounds>) at /usr/src/debug/qemu-2.0.0/arch_init.c:157
#2  ram_save_block (f=f@entry=0x7fdb0c14c550, last_stage=last_stage@entry=false)
    at /usr/src/debug/qemu-2.0.0/arch_init.c:590
#3  0x00007fdb07c30a95 in ram_save_iterate (f=0x7fdb0c14c550, opaque=<optimized out>)
    at /usr/src/debug/qemu-2.0.0/arch_init.c:815
#4  0x00007fdb07ca78a6 in qemu_savevm_state_iterate (f=f@entry=0x7fdb0c14c550)
    at /usr/src/debug/qemu-2.0.0/savevm.c:540
#5  0x00007fdb07ca82f4 in qemu_savevm_state (f=0x7fdb0c14c550)
    at /usr/src/debug/qemu-2.0.0/savevm.c:663
#6  do_savevm (mon=0x7fffa7aba500, qdict=<optimized out>) at /usr/src/debug/qemu-2.0.0/savevm.c:975
#7  0x00007fdb07ca4f89 in handle_user_command (mon=mon@entry=0x7fffa7aba500, cmdline=cmdline@entry=
    0x7fdb09ee0490 "savevm 2012") at /usr/src/debug/qemu-2.0.0/monitor.c:4167
#8  0x00007fdb07ca514c in qmp_human_monitor_command (command_line=0x7fdb09ee0490 "savevm 2012",
    has_cpu_index=false, cpu_index=0, errp=errp@entry=0x7fffa7aba5d8)
    at /usr/src/debug/qemu-2.0.0/monitor.c:725
#9  0x00007fdb07be7289 in qmp_marshal_input_human_monitor_command (mon=<optimized out>,
    qdict=<optimized out>, ret=0x7fffa7aba660) at qmp-marshal.c:1935
#10 0x00007fdb07c9f978 in qmp_call_cmd (params=0x7fdb0c14a510, mon=0x7fdb09b994a0, cmd=<optimized out>)
    at /usr/src/debug/qemu-2.0.0/monitor.c:4760
#11 handle_qmp_command (parser=<optimized out>, tokens=<optimized out>)
    at /usr/src/debug/qemu-2.0.0/monitor.c:4826
#12 0x00007fdb07d2a99a in json_message_process_token (lexer=0x7fdb09b98fe0, token=0x7fdb09e24fe0, type=
    JSON_OPERATOR, x=1, y=11) at qobject/json-streamer.c:87

3, maybe qemu savevm does not work for xen?
try to comment the memory side
./configure --enable-xen --target-list=i386-softmmu --enable-debug --enable-trace-backend=stderr --prefix=/usr --localstatedir=/var

4, write to Jim
Hi, Jim

here is some update about libxl snapshot. the main issue is
a, revert of domain snapshot is destroy and restore domain. it sucks for me. is it ok for community?
b, how do i revert domain snapshot.
c, ao(async)

#######################################
for b:
my current snapshot implementation will call savevm/loadvm/delvm for domain internal snapshot. but now i realize that i could not rely on these hmp commands. let me take savevm as example:
1), savevm will save qemu memory, but it fail in my test. i guess the reasone is that qemu_savevm_state in savevm could not live with qemu xen memory.
of course, i could bypass the qemu_savevm_state. but then there is no significant differece between savevm hmp and transaction qmp. the latter one is only deal with disk snapshot.
2), savevm will do the disk snapshot for all disks from qemu point of view, e.g. the vm include two disks(hda, hdc), qemu think that there are four disks.
(qemu) info block 1
ide0-hd0: /var/lib/xen/images/bjz_04_sles11_sp2/disk0.qcow2 (qcow2)
    Backing file:     /var/lib/xen/images/bjz_04_sles11_sp2/disk0.raw (chain depth: 1)

ide1-hd0: /var/lib/xen/images/opensuse12_3_01/disk0.qcow2 (qcow2)

hda: /var/lib/xen/images/bjz_04_sles11_sp2/disk0.qcow2 (qcow2)
    Backing file:     /var/lib/xen/images/bjz_04_sles11_sp2/disk0.raw (chain depth: 1)

hdc: /var/lib/xen/images/opensuse12_3_01/disk0.qcow2 (qcow2)

so, the disk snapshot part is also fail in my test. i guess the reason is that qemu fail on snapshot on the same disk with the same name.

so, now, the question is how should i revert snapshot if i do not call xxxvm hmp?
as we discussed, i need to call bdrv_snapshot_goto in qemu for disk snapshot revert. i could call qemu-img directly in libxl or create a new qmp in qemu.

if i use "qemu-img" command. whether it is a good idea if i call qemu-img like libxl call libxl-save-helper(libxl__exec way).

#######################################
for c
if we want async for disk snasphot, my idea is create a dedicated thread/process for transaction qmp.
or run qemu-img through libxl__exec.

regards

bamvor

21:34 2014-06-15
snapshot
1, create, list, delete basic test pass.
2, TODO
call qemu-img through libxl__exec

9:24 2014-6-16
mailing list, virtualization, xen, irq
Stefano Stabellini | 13 Jun 13:58 2014
Re: [PATCH 0/2] xen/arm: make accesses to desc->status flags atomic On Fri, 13 Jun 2014, Jan Beulich wrote:
> >>> Stefano Stabellini <stefano.stabellini <at> eu.citrix.com> 06/13/14 1:26 PM >>>
> >The IRQ line status flags are stored as bits in the status field of
> >struct irq_desc. Accesses to the status flags could be atomic but at the
> >moment they are not.
> >
> >Make them atomic on ARM. This allows us to avoid taking the desc
> >lock when clearing the IRQ_INPROGRESS flag for irqs routed to the guest.
>
> So patch 2 doesn't drop any acquire/release pair of a lock - was it a bug then
> that the lock wasn't taken before?

Yes: after clearing an LR register (after the guest EOIs an irq), we
clear IRQ_INPROGRESS without taking the irq_desc lock.

In practice is not much of an issue because:

- at that point irqs are disabled so the same irq can only be received
  simultaneously if it is routed to another pcpu;

- when we receive an irq to be routed to a guest the only thing we do
  with IRQ_INPROGRESS is:

  desc->status |= IRQ_INPROGRESS;

11:32 2014-06-16
GTD
0, 10:00

1, today
1), 11:34-11:46 snapshot. see"11:34 2014-06-16"

11:34 2014-06-16
snapshot
1, add qemu-img command.

21:58 2014-06-16
1, libxl_exec Bad address
it is because there is no NULL at the end of char **args.
2, revert basic test ok.

10:25 2014-06-17
GTD
0, 10:10-17:40

1, today
1), 10:35-11:36 snapshot, reply to Jim. see"10:35 2014-06-17"
2), 11:36-12:59 lunch.
3), 12:59-13:34 personal/chat.
4), 14:05-14:24 guest vnc display. see"14:10 2014-06-17"
5), 15:00-16:40 Jason introducion virtualization.

10:35 2014-06-17
snapshot, reply to Jim
Hi, Jim
> Bamvor Jian Zhang wrote:
> > Hi, Jim
> >
> > here is some update about libxl snapshot. the main issue is
> > a, revert of domain snapshot is destroy and restore domain. it sucks for me. is it ok for community?
> >
>
> The old xend snapshot patch required the domain to be in halted state
> before applying (or reverting to) a snapshot.  But that patch was never
> upstreamed so I'm not sure is such an approach is acceptable to the
> community.
>
> > b, how do i revert domain snapshot.
> > c, ao(async)
> >
> > #######################################
> > for b:
> > my current snapshot implementation will call savevm/loadvm/delvm for domain internal snapshot. but now i realize that i could not rely on these hmp commands. let me take savevm as example:
> > 1), savevm will save qemu memory, but it fail in my test. i guess the reasone is that qemu_savevm_state in savevm could not live with qemu xen memory.
> > of course, i could bypass the qemu_savevm_state. but then there is no significant differece between savevm hmp and transaction qmp. the latter one is only deal with disk snapshot.
> >
>
> The xend snapshot patch used xc_save to save memory state of the domain,
> added a 'snapshot-name' setting to the domain config stored stored as
> part of the saved memory image, and then signaled qemu (via xenstore) to
> snapshot the disks .  When applying or reverting to a snapshot, xend
> would call xc_restore to start the domain from the saved memory image.
> The 'snapshot-name' setting would be written to xenstore, allowing qemu
> to know what snapshot to use in the qcow2 file.
>
> You are right that savevm/loadvm will not work for the memory state
> since that is handled by Xen, not qemu.
>
> > 2), savevm will do the disk snapshot for all disks from qemu point of view, e.g. the vm include two disks(hda, hdc), qemu think that there are four disks.
> > (qemu) info block 1
> > ide0-hd0: /var/lib/xen/images/bjz_04_sles11_sp2/disk0.qcow2 (qcow2)
> >     Backing file:     /var/lib/xen/images/bjz_04_sles11_sp2/disk0.raw (chain depth: 1)
> >
> > ide1-hd0: /var/lib/xen/images/opensuse12_3_01/disk0.qcow2 (qcow2)
> >
> > hda: /var/lib/xen/images/bjz_04_sles11_sp2/disk0.qcow2 (qcow2)
> >     Backing file:     /var/lib/xen/images/bjz_04_sles11_sp2/disk0.raw (chain depth: 1)
> >
> > hdc: /var/lib/xen/images/opensuse12_3_01/disk0.qcow2 (qcow2)
> >
> > so, the disk snapshot part is also fail in my test. i guess the reason is that qemu fail on snapshot on the same disk with the same name.
> >
> > so, now, the question is how should i revert snapshot if i do not call xxxvm hmp?
> >
>
> I  think you will need to do something similar to the old xend snapshot
> patch.  Use Xen's xc_save and xc_restore tools to handle the memory, and
> qemu's block drivers to handle the disk snapshot.
yes, this is what i want. considering the libxl-save-helper for ao stuff. the
implementation may  be
snapshot create: libxl__xc_domain_save + "qmp transaction".
snapshot revert: libxl__xc_domain_restore/(destroy+create_domain)  + "qemu-img snapshot apply".
currently, for revert, i will destroy the domain and then restore the domain
state through create_domain. after think about your suggestion, i guess that
libxl__xc_domain_restore should work if no device change. if there is some
device changes between current state and previous snapshot state, destroy and
restore domain will work but too heavy. a lightweight method would be better.
>
> > as we discussed, i need to call bdrv_snapshot_goto in qemu for disk snapshot revert. i could call qemu-img directly in libxl or create a new qmp in qemu.
> >
> > if i use "qemu-img" command. whether it is a good idea if i call qemu-img like libxl call libxl-save-helper(libxl__exec way).
> >
> > #######################################
> > for c
> > if we want async for disk snasphot, my idea is create a dedicated thread/process for transaction qmp.
> > or run qemu-img through libxl__exec.
> >
>
> Sounds reasonable to me, but ultimately needs agreement with the Xen
> community.
yesterday, i wrote a patch to call qemu-img in revert and it works.
>
> That brings me to the topic of discussing this work on last week's Xen
> community call.  I asked Ian Campbell if the community could help move
> this work along.  He said yes, but confessed that he (and Ian Jackson
> IIRC) is a bit confused about the current proposal.  I think it would be
> best to submit another iteration of your proposal, focusing on
> documentation and the libxl interface for snapshots.
>
> I think you did a good job of describing the general concepts of
> snapshots (disk snapshots, vm snapshots, system checkpoints, etc.) in
> your V2
>
> http://lists.xen.org/archives/html/xen-devel/2014-04/msg02549.html
>
> In your next iteration, I'd include all the generic info about snapshots
> in a cover letter, along with some user stories that describe how
> snapshots work.  After the cover letter, provide a patch to the libxl
> interface that clearly describe the semantics of each new structure and
> API, and a patch to xl.pod describing the syntax and semantics of the xl
> snapshot commands.  For the libxl APIs, provide detailed information
> about the API.  Put yourself in a developer's seat that is trying to use
> these APIs in her libxl App.  Is there enough information on the API to
> consume it in an application?  As a reference, look at the API
> documentation for virDomainSnapshotCreateXML
>
> http://libvirt.org/html/libvirt-libvirt.html#virDomainSnapshotCreateXML
>
> I think the community will help answer the types of questions you asked
> in this mail once they understand proposed semantics.  I don't think we
> should get bogged down in implementation detail until there is some
> agreement on the interface.
i will write the V4 snapshot. as you said, the series would be:
0000: cover letter: some user stories that describe how snapshots work.
0001: libxl API: it will include two parts: domain snapshot configuration file
      operation(load, store, delete, it will base on Wei Liu's libxl-json api)
      and disk snapshot operation(create, delete, revert, including
      implementation details, why choose qmp or qemu-img command).
0002: patch for docs/man/xl.pod.1: describe the xl snapshot command, including
      create, list, delete, revert.
0003: xl snapshot command details. Ian Jackson/Campbell is confused about
      discuss implementation details in xl.pod.1 because the latter one is
      user manual in my V2 doc. here i would like to mention which api will be
      called in xl snapshot command. and discuss the revert one we discuss
      above: libxl__xc_domain_restore/(destroy+create_domain)  +
      "qemu-img snapshot apply".
0004: patch for xl.snapshot.pod.5: xl snapshot configuration file syntax.

regards

bamvor
>
> Regards,
> Jim

14:10 2014-06-17
virtualization, virt-manager, virt-viewer, vncviewer, -shared vs -noshared
1, if the user use virt-manager or virt-viewer connect to guest. then vncviewer could not be opened and exit without error:
> vncviewer 147.2.207.233:5900
Connected to RFB server, using protocol version 3.8
No authentication needed
Authentication successful
vncviewer: VNC server closed connection

if start vncviewer with "-noshared", it will be opened by vncviewer.
> vncviewer -noshared 147.2.207.233:5900

and even if vncviewer start with "-noshared", virt-viewer/virt-manager could connect to guest either. the former one maybe close the vncviewer connection before connect to guest.

14:29 2014-06-17
suse, colleague, Luis R. Rodriguez, xen systemd support
From: Luis R. Rodriguez <mcgrof <at> do-not-panic.com>
Subject: [PATCH v6 00/13] xen: add systemd support
Newsgroups: gmane.comp.emulators.xen.devel
Date: 2014-06-13 01:18:38 GMT (4 days, 5 hours and 11 minutes ago)

From: "Luis R. Rodriguez" <mcgrof <at> suse.com>
http://article.gmane.org/gmane.comp.emulators.xen.devel/203208/match=systemd

14:30 2014-06-17
Jason, question
1, what about hyper-V?
2, "Luis R. Rodriguez" <mcgrof <at> suse.com> send a systemd for xen.
how do we work with suse lab?

15:11 2014-06-17
1, Jason 2005 join virtualization. manager in 2009?
2, drop 32bit support for xen on sle11sp3.
3, KVM at SUSE
1), SUSE Studio uses KVM exclusively.
4, limited support for SPICE
http://www.spice-space.org/
5, lxc
http://docs.oracle.com/cd/E37670_01/E37355/html/ol_app_containers.html
Running Application Containers
You can use the lxc-execute command to create a temporary application container in which you can run a command that is effectively isolated from the rest of the system. For example, the following command creates an application container named guest that runs sleep for 100 seconds.
6, about "best effort"
it depend on whether suse could coorperation with guest provider.
7, virt-v2v
support it in virt-manager?
migration from xen guest on host A to kvm guest on host B.
8, SPECvirt.

16:32 2014-06-17
virtualization, docker, doc; lxc overview
systemctl start docker
1, docker hub
https://registry.hub.docker.com/
2, doc:
1), docker入门教程
http://segmentfault.com/a/1190000000366923
2), docker and opensuse
http://flavio.castelli.name/2013/04/12/docker-and-opensuse/
3, basic command
1), run a new command in specific container
(1), add "-t -i" for open a tty console
# docker run -t -i  ubuntu:14.04 /bin/bash
root@8806e715ff6d:/# cat /etc/issue
Ubuntu 14.04 LTS \n \l

root@8806e715ff6d:/# exit
exit
# docker ps -a
CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS                       PORTS                   NAMES
8806e715ff6d        ubuntu:14.04        /bin/bash              25 seconds ago      Exited (130) 2 seconds ago                           clever_pasteur
679b0b53b85c        wordpress:3         /usr/src/wordpress/d   2 hours ago         Up 2 hours                   0.0.0.0:49153->80/tcp   bamvor_wordpress
dd1f25fbf5b4        mysql:5             /entrypoint.sh mysql   3 hours ago         Up 3 hours                   3306/tcp                bamvor_wordpress/mysql,wordpress_mysql

# docker run --help 2>&1 |grep "\-[tip]"
  -i, --interactive=false    Keep stdin open even if not attached
  -p, --publish=[]           Publish a container's port to the host
  -P, --publish-all=false    Publish all exposed ports to the host interfaces
  -t, --tty=false            Allocate a pseudo-tty

2), limit the memory in container
https://www.digitalocean.com/community/tutorials/docker-explained-how-to-create-docker-containers-running-memcached
To run a container with memory limited to 256 MBs:

# Example: sudo docker run -name [name] -m [Memory (int)][memory unit (b, k, m or g)] -d (to run not to attach) -p (to set access and expose ports) [image ID]
sudo docker run -name memcached_ins -m 256m -d -p 45001:11211 memcached_img

To confirm the memory limit, you can inspect the container:

# Example: docker inspect [container ID] | grep Memory
sudo docker inspect memcached_ins | grep Memory

4, about lxc
https://linuxcontainers.org/
Current LXC uses the following kernel features to contain processes:

    Kernel namespaces (ipc, uts, mount, pid, network and user)
    Apparmor and SELinux profiles
    Seccomp policies
    Chroots (using pivot_root)
    Kernel capabilities
    Control groups (cgroups)

As such, LXC is often considered as something in the middle between a chroot on steroids and a full fledged virtual machine. The goal of LXC is to create an environment as close as possible as a standard Linux installation but without the need for a separate kernel.

5, docker doc
https://www.digitalocean.com/community/tutorials/docker-explained-how-to-create-docker-containers-running-memcached
including how to build docker image from dockerfile.

further reading
https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-getting-started
https://www.digitalocean.com/community/tutorials/docker-explained-using-dockerfiles-to-automate-building-of-images

17:22 2014-06-17
sles12
http://docserv.suse.de/documents/SLES12/

10:22 2014-06-18
virtualization, lxc, docker, set up a wordpress server
1, reference"16:32 2014-06-17" for docker basic usage.
2, docker hub
https://registry.hub.docker.com/_/mysql/
https://registry.hub.docker.com/_/wordpress/
3, pull
# docker pull mysql
# docker pull wordpress
# docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
wordpress           3                   c3fe87f1b6dd        13 days ago         374.2 MB
wordpress           3.9                 c3fe87f1b6dd        13 days ago         374.2 MB
wordpress           3.9.1               c3fe87f1b6dd        13 days ago         374.2 MB
wordpress           latest              c3fe87f1b6dd        13 days ago         374.2 MB
ubuntu              precise             ae8682f4ff20        13 days ago         209.9 MB
mysql               5                   01e690393148        13 days ago         3.621 GB
mysql               5.6                 01e690393148        13 days ago         3.621 GB
mysql               5.6.17              01e690393148        13 days ago         3.621 GB
mysql               latest              01e690393148        13 days ago         3.621 GB
base                latest              b750fe79269d        15 months ago       175.3 MB
base                ubuntu-12.10        b750fe79269d        15 months ago       175.3 MB
base                ubuntu-quantal      b750fe79269d        15 months ago       175.3 MB
base                ubuntu-quantl       b750fe79269d        15 months ago       175.3 MB

4, start mysql and wordpress
# docker run --name wordpress_mysql -e MYSQL_ROOT_PASSWORD=suse -d mysql
# docker run -p 80 --name bamvor_wordpress --link wordpress_mysql:mysql -d wordpress
679b0b53b85ccbbe9890f6a29cf179e6e83af9761167bdba2b8367652521e00b

notes that when you need public the port to outside word(i mean your host and network). you need add "-p portnum" for publishing speicific port. e.g. port 80 in wordpress is mapped to 49153 in my host.
# docker ps
CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS                   NAMES
679b0b53b85c        wordpress:3         /usr/src/wordpress/d   3 seconds ago       Up 1 seconds        0.0.0.0:49153->80/tcp   bamvor_wordpress
dd1f25fbf5b4        mysql:5             /entrypoint.sh mysql   26 minutes ago      Up 26 minutes       3306/tcp                bamvor_wordpress/mysql,wordpress_mysql

now:
"127.0.0.1:49153" will display "Apache2 Debian Default Page"

http://127.0.0.1:49153/wp-admin/
will rediect to the following page for install:
http://127.0.0.1:49153/wp-admin/install.php

5, the ip configuration right now
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: em1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast master br0 state UP group default qlen 1000
    link/ether d4:be:d9:a3:31:ed brd ff:ff:ff:ff:ff:ff
    inet6 fe80::d6be:d9ff:fea3:31ed/64 scope link
       valid_lft forever preferred_lft forever
3: br0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default
    link/ether d4:be:d9:a3:31:ed brd ff:ff:ff:ff:ff:ff
    inet 147.2.207.193/24 brd 147.2.207.255 scope global br0
       valid_lft forever preferred_lft forever
    inet6 fe80::d6be:d9ff:fea3:31ed/64 scope link
       valid_lft forever preferred_lft forever
4: virbr0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default
    link/ether 52:54:00:5c:aa:2d brd ff:ff:ff:ff:ff:ff
    inet 192.168.100.1/24 brd 192.168.100.255 scope global virbr0
       valid_lft forever preferred_lft forever
5: virbr0-nic: <BROADCAST,MULTICAST> mtu 1500 qdisc pfifo_fast master virbr0 state DOWN group default qlen 500
    link/ether 52:54:00:5c:aa:2d brd ff:ff:ff:ff:ff:ff
7: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default
    link/ether 56:84:7a:fe:97:99 brd ff:ff:ff:ff:ff:ff
    inet 172.17.42.1/16 scope global docker0
       valid_lft forever preferred_lft forever
    inet6 fe80::5484:7aff:fefe:9799/64 scope link
       valid_lft forever preferred_lft forever
21: veth9c32: <BROADCAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast master docker0 state UP group default qlen 1000
    link/ether 96:77:5d:2b:8f:9b brd ff:ff:ff:ff:ff:ff
    inet6 fe80::9477:5dff:fe2b:8f9b/64 scope link
       valid_lft forever preferred_lft forever
37: veth1197: <BROADCAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast master docker0 state UP group default qlen 1000
    link/ether 86:19:45:46:b4:ba brd ff:ff:ff:ff:ff:ff
    inet6 fe80::8419:45ff:fe46:b4ba/64 scope link
       valid_lft forever preferred_lft forever

10:52 2014-06-18
GTD
0, 9:10-18:05

1, today
PLAN: send snapshot doc out.
1), 20' chat with Yu Zou.
2), 10:00-10:40 try docker.
3), 10:52-11:37 snapshot. see"10:55 2014-06-18"
50' discuss with chunyan
4), 11:37-12:20 lunch.
5), 30' 20' docker. see"16:32 2014-06-17" and "10:22 2014-06-18".
6), 14:12-17:00 Jason provo and some discussion. see"14:13 2014-06-18"

10:55 2014-06-18
snapshot
1, Jim email
Bamvor Jian Zhang wrote:
> Hi, Jim
>
>> Bamvor Jian Zhang wrote:
>>
>>> Hi, Jim
>>>
>>> here is some update about libxl snapshot. the main issue is
>>> a, revert of domain snapshot is destroy and restore domain. it sucks for me. is it ok for community?
>>>
>>>
>> The old xend snapshot patch required the domain to be in halted state
>> before applying (or reverting to) a snapshot.  But that patch was never
>> upstreamed so I'm not sure is such an approach is acceptable to the
>> community.
>>
>>
>>> b, how do i revert domain snapshot.
>>> c, ao(async)
>>>
>>> #######################################
>>> for b:
>>> my current snapshot implementation will call savevm/loadvm/delvm for domain internal snapshot. but now i realize that i could not rely on these hmp commands. let me take savevm as example:
>>> 1), savevm will save qemu memory, but it fail in my test. i guess the reasone is that qemu_savevm_state in savevm could not live with qemu xen memory.
>>> of course, i could bypass the qemu_savevm_state. but then there is no significant differece between savevm hmp and transaction qmp. the latter one is only deal with disk snapshot.
>>>
>>>
>> The xend snapshot patch used xc_save to save memory state of the domain,
>> added a 'snapshot-name' setting to the domain config stored stored as
>> part of the saved memory image, and then signaled qemu (via xenstore) to
>> snapshot the disks .  When applying or reverting to a snapshot, xend
>> would call xc_restore to start the domain from the saved memory image.
>> The 'snapshot-name' setting would be written to xenstore, allowing qemu
>> to know what snapshot to use in the qcow2 file.
>>
>> You are right that savevm/loadvm will not work for the memory state
>> since that is handled by Xen, not qemu.
>>
>>
>>> 2), savevm will do the disk snapshot for all disks from qemu point of view, e.g. the vm include two disks(hda, hdc), qemu think that there are four disks.
>>> (qemu) info block 1
>>> ide0-hd0: /var/lib/xen/images/bjz_04_sles11_sp2/disk0.qcow2 (qcow2)
>>>     Backing file:     /var/lib/xen/images/bjz_04_sles11_sp2/disk0.raw (chain depth: 1)
>>>
>>> ide1-hd0: /var/lib/xen/images/opensuse12_3_01/disk0.qcow2 (qcow2)
>>>
>>> hda: /var/lib/xen/images/bjz_04_sles11_sp2/disk0.qcow2 (qcow2)
>>>     Backing file:     /var/lib/xen/images/bjz_04_sles11_sp2/disk0.raw (chain depth: 1)
>>>
>>> hdc: /var/lib/xen/images/opensuse12_3_01/disk0.qcow2 (qcow2)
>>>
>>> so, the disk snapshot part is also fail in my test. i guess the reason is that qemu fail on snapshot on the same disk with the same name.
>>>
>>> so, now, the question is how should i revert snapshot if i do not call xxxvm hmp?
>>>
>>>
>> I  think you will need to do something similar to the old xend snapshot
>> patch.  Use Xen's xc_save and xc_restore tools to handle the memory, and
>> qemu's block drivers to handle the disk snapshot.
>>
> yes, this is what i want. considering the libxl-save-helper for ao stuff. the
> implementation may  be
> snapshot create: libxl__xc_domain_save + "qmp transaction".
> snapshot revert: libxl__xc_domain_restore/(destroy+create_domain)  + "qemu-img snapshot apply".
> currently, for revert, i will destroy the domain and then restore the domain
> state through create_domain. after think about your suggestion, i guess that
> libxl__xc_domain_restore should work if no device change. if there is some
> device changes between current state and previous snapshot state, destroy and
> restore domain will work but too heavy. a lightweight method would be better.
>

I don't think this would be an issue If snapshot-create includes
stashing the domain config, right?  The stashed config could then be
used when reverting to the snapshot.

>>> as we discussed, i need to call bdrv_snapshot_goto in qemu for disk snapshot revert. i could call qemu-img directly in libxl or create a new qmp in qemu.
>>>
>>> if i use "qemu-img" command. whether it is a good idea if i call qemu-img like libxl call libxl-save-helper(libxl__exec way).
>>>
>>> #######################################
>>> for c
>>> if we want async for disk snasphot, my idea is create a dedicated thread/process for transaction qmp.
>>> or run qemu-img through libxl__exec.
>>>
>>>
>> Sounds reasonable to me, but ultimately needs agreement with the Xen
>> community.
>>
> yesterday, i wrote a patch to call qemu-img in revert and it works.
>
>> That brings me to the topic of discussing this work on last week's Xen
>> community call.  I asked Ian Campbell if the community could help move
>> this work along.  He said yes, but confessed that he (and Ian Jackson
>> IIRC) is a bit confused about the current proposal.  I think it would be
>> best to submit another iteration of your proposal, focusing on
>> documentation and the libxl interface for snapshots.
>>
>> I think you did a good job of describing the general concepts of
>> snapshots (disk snapshots, vm snapshots, system checkpoints, etc.) in
>> your V2
>>
>> http://lists.xen.org/archives/html/xen-devel/2014-04/msg02549.html
>>
>> In your next iteration, I'd include all the generic info about snapshots
>> in a cover letter, along with some user stories that describe how
>> snapshots work.  After the cover letter, provide a patch to the libxl
>> interface that clearly describe the semantics of each new structure and
>> API, and a patch to xl.pod describing the syntax and semantics of the xl
>> snapshot commands.  For the libxl APIs, provide detailed information
>> about the API.  Put yourself in a developer's seat that is trying to use
>> these APIs in her libxl App.  Is there enough information on the API to
>> consume it in an application?  As a reference, look at the API
>> documentation for virDomainSnapshotCreateXML
>>
>> http://libvirt.org/html/libvirt-libvirt.html#virDomainSnapshotCreateXML
>>
>> I think the community will help answer the types of questions you asked
>> in this mail once they understand proposed semantics.  I don't think we
>> should get bogged down in implementation detail until there is some
>> agreement on the interface.
>>
> i will write the V4 snapshot. as you said, the series would be:
> 0000: cover letter: some user stories that describe how snapshots work.
> 0001: libxl API: it will include two parts: domain snapshot configuration file
>       operation(load, store, delete, it will base on Wei Liu's libxl-json api)
>       and disk snapshot operation(create, delete, revert, including
>       implementation details, why choose qmp or qemu-img command).
> 0002: patch for docs/man/xl.pod.1: describe the xl snapshot command, including
>       create, list, delete, revert.
> 0003: xl snapshot command details. Ian Jackson/Campbell is confused about
>       discuss implementation details in xl.pod.1 because the latter one is
>       user manual in my V2 doc. here i would like to mention which api will be
>       called in xl snapshot command. and discuss the revert one we discuss
>       above: libxl__xc_domain_restore/(destroy+create_domain)  +
>       "qemu-img snapshot apply".
> 0004: patch for xl.snapshot.pod.5: xl snapshot configuration file syntax.
>

This sounds good to me.

Thanks!
Jim

2, write doc base on previous discussion

11:16 2014-06-18
maybe we are not talking about the same thing.
there are several qemu function will touch the domain snapshot configuration
directory and its file: qemuDomainSnapshotWriteMetadata,
qemuDomainSnapshotLoad(called by qemuStateInitialize), qemuDomainSnapshotDiscard,
qemuDomainRemoveInactive.

and these functions will call the libvirt snapshot parser.

my question is what will you do when libxl driver init or reloading? do you read
the libxl snapshot configuration files or only read the libvirt xml format?
if you will consider the libxl snapshot configuration file, will you update it
after libvirt update the domain snapshot configuration files?

it would be simple, when user want to migrate between difference toolstack
(libxl, libvirt).

and it will not affect the user experience, because we do not need to change

    .domainSnapshotCreateXML = qemuDomainSnapshotCreateXML, /* 0.8.0 */
    .domainSnapshotGetXMLDesc = qemuDomainSnapshotGetXMLDesc, /* 0.8.0 */

14:13 2014-06-18
1, Jason
1, Jeffrey(kirk's son)
1), lab hardware maintenance.
2), duplicate issue.
2, kirk
2003
vmdp.
3, Charles
4, Juergen
in nug
5, Mike
more than 20 year in novell/suse.
join several month. before join virtualization, NTS, doing great job in NTS.
v2v
6, bruce
adopt a Shanghai girl when her 2 or 3.
from 1989.
7, Jim
40 year old. from 1999.

2, other
1), Bob Flynn
2), Sccot Reeves. was(?) Desktop manager.
3), Carlg Gardner: set up the autobuild in provo.
4), John Jolly. IBM mainframe(?)
5), David Mair: L3 maintenance.
6), Mike miller
equal to ralf.
marketing
7), maure baker
8), darren davis
ISV: certified the vendor's application on suse enterprise linux.
9), alan clark(Chairman of openstack)
report to mike miller.

3, other in virtualization team
1), Jan B from 1992.
2), Cedric Bosdonnat
France Lyon
3), Andreas Faerber
4), olaf hering.
very good experience in powerPC.

4, hamsta.virt.lab.novell.com
xenXXX mean 151.155.144.XXX
root: novell or susetesting

add search domain: virt.lab.novell.com
hamsta user name and password is both email id.

5, orthos.arch.suse.de
bjzhang, bjzhang-pass

16:01 2014-06-18
virtualization

17:38 2014-06-18
software skill, Linux, X11, copy and paste
1, xclip
# echo 111 | xclip -i --sel clip
# xclip -o --sel clip
111

2, Gnome-terminal
Ctrl+C, Ctrl+V.

10:19 2014-06-19
GTD
0, 10:00

1, today
1), 10:25-11:38 15:30-16:01 snapshot doc
2), 11:38-12:42 lunch and chating.
3), 12:42-14:15 buy books.
4), 30' buy Moleskine notebook for evernote.
5), 16:00-16:30 nap.

14:18 2014-06-19
development, gmail
https://developers.google.com/apps-script/
https://developers.google.com/apps-script/reference/gmail/
use case:
http://www.harryonline.net/evernote/send-google-mail-to-evernote/226

16:30 2014-06-19
Retrospective for SLE12 project
1, 152 for virtualization team.
4 for Beijing team.
4 for chunyan.
1 for Bamvor.

2,
jason: product manager
a: project manager.

10:29 2014-06-22
kernel training, Li Yong
1, BIOS加载引导扇区，引导山区加载NTLOADER, LILO等。
2, GRUB支持三种
1), multiboot spec
2), a.out
3), Linux boot protocol
mlxos使用multiboot spec.
3, "ld --verbose" will print the default link script.
GNU ld (GNU Binutils; openSUSE 12.3) 2.23.1
  Supported emulations:
   elf_x86_64
   elf32_x86_64
   elf_i386
   ...
   aarch64linux
   ...
   i386pe
using internal linker script:
==================================================
/* Script for -z combreloc: combine and sort reloc sections */
OUTPUT_FORMAT("elf64-x86-64", "elf64-x86-64",
	      "elf64-x86-64")
OUTPUT_ARCH(i386:x86-64)
ENTRY(_start)
SEARCH_DIR("/usr/x86_64-suse-linux/lib64"); SEARCH_DIR("/usr/local/lib64"); SEARCH_DIR("/lib64"); SEARCH_DIR("/usr/lib64"); SEARCH_DIR("/usr/x86_64-suse-linux/lib"); SEARCH_DIR("/usr/lib64"); SEARCH_DIR("/usr/local/lib"); SEARCH_DIR("/lib"); SEARCH_DIR("/usr/lib");
SECTIONS
{
  /* Read-only sections, merged into text segment: */
  PROVIDE (__executable_start = SEGMENT_START("text-segment", 0x400000)); . = SEGMENT_START("text-segment", 0x400000) + SIZEOF_HEADERS;
  .interp         : { *(.interp) }
  .note.gnu.build-id : { *(.note.gnu.build-id) }
  .hash           : { *(.hash) }
  .gnu.hash       : { *(.gnu.hash) }
  .dynsym         : { *(.dynsym) }
  .dynstr         : { *(.dynstr) }
  .gnu.version    : { *(.gnu.version) }
  .gnu.version_d  : { *(.gnu.version_d) }
  .gnu.version_r  : { *(.gnu.version_r) }
  .rela.dyn       :
    {
        ...
    }
  ...
  .init           :
  {
    KEEP (*(SORT_NONE(.init)))
  }
  .plt            : { *(.plt) *(.iplt) }
  .text           :
  {
    *(.text.unlikely .text.*_unlikely)
    *(.text.exit .text.exit.*)
    *(.text.startup .text.startup.*)
    *(.text.hot .text.hot.*)
    *(.text .stub .text.* .gnu.linkonce.t.*)
    /* .gnu.warning sections are handled specially by elf32.em.  */
    *(.gnu.warning)
  }
  ...
  /* Stabs debugging sections.  */
  ...
}

4, 线性地址和物理地址
虚拟地址经过段+段偏移量转成的地址是线性地址。

5, 代码段，数据段和BSS.
x86里面代码段和数据段访问方式不同？
页表放在代码段里面。

6, 链接地址和加载地址.
书：链接器和加载器。
应用程序不能指定加载地址（因为加载地址是物理地址，应用程序不允许设置）。
idt: interrupt description table. 需要页对齐。

AT: 指定加载地址(物理地址).

7, 内核链接格式
x86的一些约定：0-640k可用，1M+xxx 可用（用于加载内核）。
.text: {
} = 0x9090
用0x9090填充text里面剩余空间。9090是nop指令，避免text区域内代码跑飞后执行乱七八糟的东西。

当Grub加载内核mlxos.bin时,使用的是Grub自己的页表,线性地址同物理地址一致。因此这里的入口地址要和内核文件放置在内存中的物理地址一致。

8, Segment VS. Section
section: 链接
segment: 执行？

用户态应用程序加载与内核加载的不同
  代码段和数据段具有不同的读写属性,一般代码只读,数据段可读写
  即便代码和数据的section链接时被重叠放置在同一个页中,也会分别属于两个不同的segment
  bamvor: 如果代码和数据在硬盘上是一个页，复制到内存中还是会复制成两个页。

9,
Bit 31 (PG) must be cleared. Bit 0 (PE) must be set. Other bits are all undefined.
CR0: 没有打开分页模式的段模式。

A20: 第21根地址线，也就是大于1M时会用到。
A20 gate: enable表示可以访问大于1M的地址。对于我们来说，大于1M正是放的我们的内核。

esp: stack.

参数调用从右往左压栈。
EAX: 保存返回值。
x86不像arm，返回地址是直接/自动（？）放到栈里面.

10, 内核汇编代码
        call start_kernel
boot_die:
        hlt
        jmp boot_die

如果没有任何线程能被调度，会运行到这里，hlt是关cpu。

内核里面有32M固化的页表。

12:31 2014-06-22
GTD
0,

1, today
1), 10:00-12:30 内核加载。
2), 13:45-15:54 内核保护模式.

13:52 2014-06-22
talk with Dongmao
lvm snapshot与lvm thin provision。
lvm页表都在lvm meta data中（相当于在硬盘的superblock）。第一次使用后会缓存在内核中。
https://access.redhat.com/site/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Logical_Volume_Manager_Administration/thinprovisioned_volumes.html

14:00 2014-06-22
kernel, 保护模式
1, RPL, DPL, CS和SS段寄存器中CPL都是2个bit: ring0-3.
0: kernel.
3: userspace.

高优先级的代码不能调用低优先级代码。低优先级代码需要用特定接口访问高优先级代码（例如系统调用）。
bamvor: 前者在arm里面是怎么实现的？

R/W, User/supervisor: 都在PDE(page directory entry)设置，也就是4Mbytes都是一样的。

CR0:
[31]PG: paging页寻址。
[0]PE: protection enable: 保护模式。

2, 代码
91行的ljmp后，到93行就是kernel段寄存器了（之前是grub的段寄存器）。

stosl
http://blog.163.com/njut_wangjian/blog/static/165796425201248102957493
STOSL指令相当于将EAX中的值保存到ES:EDI指向的地址中，若设置了EFLAGS中的方向位置位(即在STOSL指令前使用STD指令)则EDI自减4，否则(使用CLD指令)EDI自增4。STOSD和STOSL类似，存储双字串数据。前者是GNU文档使用的，后者是Intel文档使用的。

swaper_pg_dir: 内核页目录基地址。

3, 中断
ralph brown interrupt list
http://www.ctyme.com/rbrown.htm

TODO: 将IDT中所有的中断处理例程地址都设置为ignore_int的地址(代码实例4)

传统Linux只能支持用户空间调度，因为所有进程共用一个内核栈，rebort love把每个进程改为有用户空间栈和内核空间栈。这样内核态也可以抢占。

sleep(0)会做调度么？2.6.32之后不行，因为有了高精度定时器后，sleep(0)不会引起调度。

线程模型： 1:1, N:1, N:M.

16:27 2014-06-22
系统调用
1,
应用程序通常会调用系统库封装的系统调用而非直接通过INT指令调用内核服务
• open(2), read(2), write(2)
有个别系统库的功能是不涉及优先级切换的
• vSysCall, vDSO
• gettimeofday(2),gettimeofday(3)

2, sendrec
SYSVEC:为33号中断 (lib/libc/asm-i386/bits/_ipc.S)
MLXOS使用了x86平台上第一个可自定义的中断号(0x21)给系统调用,与Linux(0x80)不同

