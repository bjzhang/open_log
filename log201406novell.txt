.LOG
12:31 2014-06-03
mailing list, virtualization
1, "Wei Liu"_email_"Discussion on libxl domain configuration synchronization"

Ian and Ian

There are many open questions regarding the design and implementation of this
new feature so I think it's worthwhile having a dedicated email thread to
discuss this before I write any new code.

The new mechanism will use the new JSON infrastructure so that application can
easily consume domain configuration.

### Exposed API

From API design point of view, what API should we expose to applications? We
seem to have agreed that a "retrieve" API is certainly necessary (let's name it
libxl_retrieve_domain_configuration for ease of discussion), while a "store" /
"save" is debatable.

If we set the rule that libxl application is not allowed to manipulate domain
configuration then "store" is not necessary.

### Implementation

Further more, how should we implement the new API?

My original thought is that the JSON version of domain configuration always
contains the up to date domain configuration, so that we can get hold of that
information even without the help of our library.

Ian J pointed out that this design is bogus. If I understand correctly,
he was referring to the fact that the most up to date information is
actually stored in xenstore. So his suggestion was that we make
libxl_retrieve_domain_configuration a) use JSON version as template, b)
pull in xenstore knob values to fill in the things that need to be
updated. I think this approach is appealing, if we agree on the design
"libxl-json" is completely private to libxl, that application should not
poke at it at all. I am now keen on taking this approach.

What knobs should libxl_retrieve_domain_configuration be looking at, domain
name, memory targets and devices etc. Memory targets are easy as they are only
some integer values.

As for devices, see next session.

### Device hotplug / unplug and half-baked state

When we build a domain, the devices presented in config file (later transform
into JSON) and the devices presented in xenstore (backend entries) are
consistent, otherwise we would have failed when building a domain.

Device hotplug and unplug are a bit trickier to handle.

Hotplug steps:
1. libxl__get_domain_configuration (this reads libxl-json file and parse
   it into libxl_domain_config struct).
2. add new device to JSON config
3. hotplug device
4. if hotplug success, libx__set_domain_configuration

If 3 fails, we may end up in a situation that some xenstore entries have been
written, but that device is not presented in JSON version of configuration. 4
can also fail when libxl tries to write libxl-json file. The steps and analyse
for device unplug are similar.

Later when we do libxl_retrieve_domain_configuration, the xenstore entries and
JSON config don't agree with each other. What should we do here? We cannot say
one view is more acurate then the other, given the fact that either 3 or 4 can
fail. Ian J suggested merging the two views, I agree with him.

### Summary

1. libxl_retrieve_domain_configuration is the only public API.
2. libxl_retrieve_domain_configuration should retrieve JSON version of
   domain configuration as template and update it according to xenstore
   entries.
3. libxl_retrieve_domain_configuration should be able to merge xenstore
   view and JSON config view of a domain.
4. JSON version gets saved when creating a domain.
5. JSON version gets updated when we do device hotplug / unplug. Other
   libxl functionalities will not touch it, unless they need to save
   some user provided values as template.

Thoughts?

Wei.

2, (09:45 2014-06-05)
"Ian Jaskson"_email_"Re: [Xen-devel] Discussion on libxl domain configuration synchronization"
Wei Liu writes ("Discussion on libxl domain configuration synchronization"):
> ### Exposed API
> 
> From API design point of view, what API should we expose to
> applications? We seem to have agreed that a "retrieve" API is
> certainly necessary (let's name it
> libxl_retrieve_domain_configuration for ease of discussion), while a
> "store" / "save" is debatable.

Right.

> If we set the rule that libxl application is not allowed to manipulate domain
> configuration then "store" is not necessary.

Indeed.  If the libxl application wants to change the domain's setup
they must make individual calls to add and remove devices, increase
and decrease memory, etc.


> ### Summary
> 
> 1. libxl_retrieve_domain_configuration is the only public API.
> 2. libxl_retrieve_domain_configuration should retrieve JSON version of
>    domain configuration as template and update it according to xenstore
>    entries.
> 3. libxl_retrieve_domain_configuration should be able to merge xenstore
>    view and JSON config view of a domain.
> 4. JSON version gets saved when creating a domain.
> 5. JSON version gets updated when we do device hotplug / unplug. Other
>    libxl functionalities will not touch it, unless they need to save
>    some user provided values as template.

Right.


> ### Implementation
> 
> Further more, how should we implement the new API?
...
> Ian J pointed out that this design is bogus. If I understand
> correctly, he was referring to the fact that the most up to date
> information is actually stored in xenstore. So his suggestion was
> that we make libxl_retrieve_domain_configuration a) use JSON version
> as template, b) pull in xenstore knob values to fill in the things
> that need to be updated. I think this approach is appealing, if we
> agree on the design "libxl-json" is completely private to libxl,
> that application should not poke at it at all. I am now keen on
> taking this approach.

Exactly.  Yes, the JSON template would be completely private to libxl.

> What knobs should libxl_retrieve_domain_configuration be looking at,
> domain name, memory targets and devices etc. Memory targets are easy
> as they are only some integer values.

I think it should be sufficient to consider every function like
  libxl_do_something_to_existing_domain

Most (but not all) of these will do something which ought to be
reflected in the results from libxl_retrieve_domain_configuration.

There should be nothing else that modifies anything that shows up in
libxl_retrieve_domain_configuration.

> ### Device hotplug / unplug and half-baked state
...
> Device hotplug and unplug are a bit trickier to handle.
> 
> Hotplug steps:
> 1. libxl__get_domain_configuration (this reads libxl-json file and parse
>    it into libxl_domain_config struct).
> 2. add new device to JSON config
> 3. hotplug device
> 4. if hotplug success, libx__set_domain_configuration
> 
> If 3 fails, we may end up in a situation that some xenstore entries
> have been written, but that device is not presented in JSON version
> of configuration. 4 can also fail when libxl tries to write
> libxl-json file. The steps and analyse for device unplug are
> similar.


There are two ways to do this:


--------------------------------------------------

[ADD] JSON FIRST

This is what you have.

You tolerate as follows:

 * Devices present in xenstore may not be present in the JSON.

   Such devices cannot be reported properly by retrieve because
   in general it is not possible to synthesize a libxl device
   backend configuration out of what appears in xenstore.

   So such devices (which may or may not be working as far
   as the guest is concerned) must be reported with some kind
   of degraded information in retreive.

Conversely you add the new invariants:

 * Any device present in the JSON is also present in xenstore.

 * Any device which has any resources allocated to it in the host or
   guest has an entry in the JSON.

In retrieve, you can use the JSON as the primary reference.

I think the degraded reporting is undesirable.  Also there is a
locking problem - see below.

--------------------------------------------------

[ADD] XENSTORE FIRST

This is what I would suggest instead.

We establish the new invariant:

 * Any device which is present in xenstore has a corresponding entry
   in the JSON (where "corresponding" relates to the virtual device
   index - ie the guest-visible "name" - not the backend info).

I believe we already have this invariant:

 * Any device which has any resources allocated to it in the host or
   guest has an entry in the xenstore.

(There is a corrollary of these two which is that a device with
resources allocated must appear in the JSON.)

Conversely, we tolerate this:

 * Devices present in JSON may not be present in xenstore.
   Such a device is to be regarded as "not present" for all purposes.

To implement this, it is necessary to write out the new JSON before
updating xenstore.

In retrieve, you use xenstore as the primary reference.  The retrieve
function would look at xenstore and simply elide any entries which
were listed in the JSON but not in xenstore.

This also avoids having to take out the lock (there has to be a lock
on the stored JSON to prevent erronous concurrent updates) across the
actual device addition.

I think the retrieval merging algorithm works a bit like this:

  * Read JSON with list of stored device configs

  * Iterate over actual devices a la libxl_device_disk_list
    * For each device, look for corresponding configuration in
      JSON.
      * If no such configuration, print a warning and make one up
        along the lines of libxl_device_disk_getinfo
      * If there is such a configuration, use the JSON information
        except:
      * Check whether the backing device is empty, and if so
        delete the backing device from the JSON.
        (This is for cdroms; see below.)
    * Add resulting device config to new list of device configs

  * Replace stored list of device configs with new one

The effect is that the pre-libxl-defaulted configuration takes
precedence but that it is safe to write a JSON configuration
containing devices which are about to be added.

--------------------------------------------------


And now some loose ends:

For hot unplug, do the operations in the reverse order.  (The reverse
of whichever order we picked above, according to which design we have
chosen.)


CDROM insert/eject presents a problem.  I think it is not possible to
reliably convert the xenstore information back into the original
backend specification.  So we have a difficulty: if we switch straight
from cdrom-has-image-A to cdrom-has-image-B, we might end up with JSON
showing image X (= A or B) and xenstore implementing image Y (= B or
A).  Retrieve wouldn't know what to return.

I propose to solve this by adding a new invariant:

 * The actual disk backend device (in xenstore) for an "ejectable"
   device (currently, only cdroms) is always the same as the JSON;
 * EXCEPT that it is permissible for the JSON to specify a
   nonempty image while the xenstore information specifies "empty".

That allows us to always return correct data: if the xenstore backend
is empty, retrieve says empty.  Otherwise we use the JSON info.

To maintain this invariant we need to change libxl_cdrom_insert.  A
"swap", ie replacement of one image with another, has to be
implemented as first an eject and then an insert.  That way we never
have a situation where xenstore contains one set of non-empty
information and JSON another.


Thanks,
Ian.

3, (19:00 2014-06-05)
Hi, Wei and Ian

sorry if i am offtopic.
i am working on add snapshot support for libxl. currently, i write my code base
on Wei's lastest libxl json patch series. i could call
libxl_domain_snapshot_from_json and libxl_domain_snapshot_to_json during
retrieve and store domain snapshot configuration. it is very useful for me.
> "Ian Jaskson"_email_"Re: [Xen-devel] Discussion on libxl domain configuration synchronization"
> Wei Liu writes ("Discussion on libxl domain configuration synchronization"):
> > ### Exposed API
> >
> > From API design point of view, what API should we expose to
> > applications? We seem to have agreed that a "retrieve" API is
> > certainly necessary (let's name it
> > libxl_retrieve_domain_configuration for ease of discussion), while a
> > "store" / "save" is debatable.
>
> Right.
>
> > If we set the rule that libxl application is not allowed to manipulate domain
> > configuration then "store" is not necessary.
>
> Indeed.  If the libxl application wants to change the domain's setup
> they must make individual calls to add and remove devices, increase
> and decrease memory, etc.
doing the domain snasphot operation is something like "change the domain's setup",
it seems not a good idea to add snapshot infor into domain configuation.


14:35 2014-06-03
software skill, network, ssh, ssh-add; github, error
"Error: Agent admitted failure to sign"

     ssh-add — adds private key identities to the authentication agent

DESCRIPTION
     ssh-add adds private key identities to the authentication agent, ssh-agent(1).  When run without
     arguments, it adds the files ~/.ssh/id_rsa, ~/.ssh/id_dsa, ~/.ssh/id_ecdsa and ~/.ssh/identity.
     After loading a private key, ssh-add will try to load corresponding certificate information from
     the filename obtained by appending -cert.pub to the name of the private key file.  Alternative
     file names can be given on the command line.

reference: https://help.github.com/articles/error-agent-admitted-failure-to-sign

15:06 2014-06-03
software skill, network, ssh, commands
1, http://blog.urfix.com/25-ssh-commands-tricks/
Chinese Translation: http://blog.csdn.net/moubenmao_jun/article/details/10392061
2, ssh port
http://zhumeng8337797.blog.163.com/blog/static/100768914201172125444948/

15:38 2014-06-03
1, Chunyan
1), budget 4000$. 30$/day.

17:43 2014-06-03
xen aarch64
http://community.arm.com/groups/processors/blog/2014/03/28/virtualization-on-arm-with-xen

10:40 2014-06-04
GTD
0, 9:30-18:05

1, today
1), 10:40-11:40 compile and test snapshot.
2), 11:40-12:28 lunch
3), 12:28-13:08 rest
4), 40' nap
5), 40' personal stuff
6), 16:29-17:52 coding excise: assert

13:10 2014-06-04
#if __GNUC__ > 3 || (__GNUC__ == 3 && __GNUC_MINOR__ >= 1)
#define _hidden __attribute__((visibility("hidden")))
#define _protected __attribute__((visibility("protected")))
#else
#define _hidden
#define _protected
#endif

14:35 2014-06-04
suse, doc, core dump
1, application core dump
http://www.novell.com/support/kb/doc.php?id=3054866
2, kernel core dump
http://www.novell.com/support/kb/doc.php?id=3374462

14:48 2014-06-04
snapshot
1, libxl idl
libxl_domain_snapshot = Struct("domain_snapshot",[
    ("name",          string),
    ("creation_time", uint64),
    ("save",          string, {'const': True}),
    ("external",      libxl_defbool),
    ("disks", Array(libxl_disk_snapshot, "num_disks")),
    ])

15:26 2014-06-04
software skill, man page
1, list man page path
# echo $MANPATH
/usr/lib64/mpi/gcc/openmpi/share/man:/usr/local/man:/usr/local/share/man:/usr/share/man:/opt/novell/man:/opt/novell/man

15:40 2014-06-04
(16:41 2014-06-05)
mailing list, forum, virtualization
1, xen
http://www.gossamer-threads.com/lists/xen/devel/
2, qemu
http://www.mail-archive.com/qemu-devel@nongnu.org/
3, libvirt
http://www.mail-archive.com/libvir-list@redhat.com/
4, easy to search
markmail.org

17:07 2014-06-04
software skill, editor, vim, insert TAB
http://stackoverflow.com/questions/4781070/how-to-insert-tab-character-when-expandtab-option-is-on-in-vim

You can use <CTRL-V><Tab> in "insert mode". In insert mode <CTRL-V> inserts a literal copy of your next character.

EDIT: As noted by feedbackloop, on windows you may need to press <CTRL-Q> rather than <CTRL-V>.

17:19 2014-06-04
software skill, build service, linkdiff
  -l, --link  (osc linkdiff): compare against the base revision of
                              the link

17:42 2014-06-04
posic, linux, exit
1, exit status 134

http://stackoverflow.com/questions/2862731/when-assert-fails-what-is-the-program-exit-code

Many UNIX systems will return 128 plus the signal number (SIGABRT is signal number 6) so you may get 134. Whatever you get, it should be documented by the C implementation.

2, exit status
http://www.gnu.org/software/libc/manual/html_node/Exit-Status.html#Exit-Status

When a program exits, it can return to the parent process a small amount of information about the cause of termination, using the exit status. This is a value between 0 and 255 that the exiting process passes as an argument to exit.

Normally you should use the exit status to report very broad information about success or failure. You can't provide a lot of detail about the reasons for the failure, and most parent processes would not want much detail anyway.

There are conventions for what sorts of status values certain programs should return. The most common convention is simply 0 for success and 1 for failure. Programs that perform comparison use a different convention: they use status 1 to indicate a mismatch, and status 2 to indicate an inability to compare. Your program should follow an existing convention if an existing convention makes sense for it.

A general convention reserves status values 128 and up for special purposes. In particular, the value 128 is used to indicate failure to execute another program in a subprocess. This convention is not universally obeyed, but it is a good idea to follow it in your programs.

Warning: Don't try to use the number of errors as the exit status. This is actually not very useful; a parent process would generally not care how many errors occurred. Worse than that, it does not work, because the status value is truncated to eight bits. Thus, if the program tried to report 256 errors, the parent would receive a report of 0 errors—that is, success.

For the same reason, it does not work to use the value of errno as the exit status—these can exceed 255.

Portability note: Some non-POSIX systems use different conventions for exit status values. For greater portability, you can use the macros EXIT_SUCCESS and EXIT_FAILURE for the conventional status value for success and failure, respectively. They are declared in the file stdlib.h.

— Macro: int EXIT_SUCCESS

    This macro can be used with the exit function to indicate successful program completion.

    On POSIX systems, the value of this macro is 0. On other systems, the value might be some other (possibly non-constant) integer expression.

— Macro: int EXIT_FAILURE

    This macro can be used with the exit function to indicate unsuccessful program completion in a general sense.

    On POSIX systems, the value of this macro is 1. On other systems, the value might be some other (possibly non-constant) integer expression. Other nonzero status values also indicate failures. Certain programs use different nonzero status values to indicate particular kinds of "non-success". For example, diff uses status value 1 to mean that the files are different, and 2 or more to mean that there was difficulty in opening the files.

Don't confuse a program's exit status with a process' termination status. There are lots of ways a process can terminate besides having its program finish. In the event that the process termination is caused by program termination (i.e., exit), though, the program's exit status becomes part of the process' termination status.

3, TODO
read check-TEST
libvirt/tests/Makefile

check-TESTS: $(TESTS)
	@failed=0; all=0; xfail=0; xpass=0; skip=0; \
	srcdir=$(srcdir); export srcdir; \
	list=' $(TESTS) '; \
	$(am__tty_colors); \
	if test -n "$$list"; then \
	  for tst in $$list; do \
	    if test -f ./$$tst; then dir=./; \
	    elif test -f $$tst; then dir=; \
	    else dir="$(srcdir)/"; fi; \
	    if $(TESTS_ENVIRONMENT) $${dir}$$tst $(AM_TESTS_FD_REDIRECT); then \
	      all=`expr $$all + 1`; \
	      case " $(XFAIL_TESTS) " in \
	      *[\ \	]$$tst[\ \	]*) \
		xpass=`expr $$xpass + 1`; \
		failed=`expr $$failed + 1`; \
		col=$$red; res=XPASS; \
	      ;; \
	      *) \
		col=$$grn; res=PASS; \
	      ;; \
	      esac; \
	    elif test $$? -ne 77; then \
	      all=`expr $$all + 1`; \
	      case " $(XFAIL_TESTS) " in \
	      *[\ \	]$$tst[\ \	]*) \
		xfail=`expr $$xfail + 1`; \
		col=$$lgn; res=XFAIL; \
	      ;; \
	      *) \
		failed=`expr $$failed + 1`; \
		col=$$red; res=FAIL; \
	      ;; \
	      esac; \
	    else \
	      skip=`expr $$skip + 1`; \
	      col=$$blu; res=SKIP; \
	    fi; \
	    echo "$${col}$$res$${std}: $$tst"; \
	  done; \
	  if test "$$all" -eq 1; then \
	    tests="test"; \
	    All=""; \
	  else \
	    tests="tests"; \
	    All="All "; \
	  fi; \
	  if test "$$failed" -eq 0; then \
	    if test "$$xfail" -eq 0; then \
	      banner="$$All$$all $$tests passed"; \
	    else \
	      if test "$$xfail" -eq 1; then failures=failure; else failures=failures; fi; \
	      banner="$$All$$all $$tests behaved as expected ($$xfail expected $$failures)"; \
	    fi; \
	  else \
	    if test "$$xpass" -eq 0; then \
	      banner="$$failed of $$all $$tests failed"; \
	    else \
	      if test "$$xpass" -eq 1; then passes=pass; else passes=passes; fi; \
	      banner="$$failed of $$all $$tests did not behave as expected ($$xpass unexpected $$passes)"; \
	    fi; \
	  fi; \
	  dashes="$$banner"; \
	  skipped=""; \
	  if test "$$skip" -ne 0; then \
	    if test "$$skip" -eq 1; then \
	      skipped="($$skip test was not run)"; \
	    else \
	      skipped="($$skip tests were not run)"; \
	    fi; \
	    test `echo "$$skipped" | wc -c` -le `echo "$$banner" | wc -c` || \
	      dashes="$$skipped"; \
	  fi; \
	  report=""; \
	  if test "$$failed" -ne 0 && test -n "$(PACKAGE_BUGREPORT)"; then \
	    report="Please report to $(PACKAGE_BUGREPORT)"; \
	    test `echo "$$report" | wc -c` -le `echo "$$banner" | wc -c` || \
	      dashes="$$report"; \
	  fi; \
	  dashes=`echo "$$dashes" | sed s/./=/g`; \
	  if test "$$failed" -eq 0; then \
	    col="$$grn"; \
	  else \
	    col="$$red"; \
	  fi; \
	  echo "$${col}$$dashes$${std}"; \
	  echo "$${col}$$banner$${std}"; \
	  test -z "$$skipped" || echo "$${col}$$skipped$${std}"; \
	  test -z "$$report" || echo "$${col}$$report$${std}"; \
	  echo "$${col}$$dashes$${std}"; \
	  test "$$failed" -eq 0; \
	else :; fi

18:21 2014-6-4
mailing list, arm, linaro
kevin hilman linaro reply to will deacon
> I had a go with this, but I couldn't seem to trigger any context tracking
> without forcing CONFIG_CONTEXT_TRACKING_FORCE=y. Does that mean we're
> missing something else?

No, it just means that you never hit the conditions to trigger full
NOHZ.  Using _FORCE is a good way to do that since it forces the context
tracking paths whether or not it's actually needed by full NOHZ.

09:26 2014-06-05
Jason_email_"[devel] Welcome Juergen Gross"

Hi everyone,

I'm pleased to announce that Juergen Gross has joined SUSE this week as
a member of the Virtualization Team. Juergen will be working with the kernel and xen communities to add features to and increase the stability
of the PVOPS xen kernel.

I'd ask each of you to welcome Juergen to the company and to offer a
helping hand to him whenever possible. Thanks!

Jason


--
Jason Douglas
Sr. Engineering Manager
Virtualization Team
SUSE
jdouglas@suse.com
+1-801-861-1649

09:31 2014-06-05
virtualization, snapshot, GSoC
> Hi there,
>
> I am confused about something here.
> We have this structs.
>
> 1), libxl_snapshot
> store a disk snapshot information, it is used by disk snapshot create
> and delete.
> libxl_disk_snapshot = Struct("disk_snapshot",[
>     ("device",        string),
>     ("name",          string),
>     ("file",          string),
>     ("format",        string),
>     ])
>
> device: device name to snapshot. e.g. sda, hda...
> name: snapshot name given by user. it will the be same name as domain snapshot
> name.
>                                          <---HERE (1
> the following parameter is only useful for external snapshot.
> file: external snapshot file.
> format: the format of external snapshot file
>
> 2), libxl_domain_snapshot
> store domain snapshot information which store in the path shown above. i add
> some api for create, delete and list these information.
> libxl_domain_snapshot = Struct("domain_snapshot",[
>     ("name",          string),
>     ("creation_time", uint64),
>     ("save",          string),
>     ("disks", Array(libxl_disk_snapshot, "num_disks")),
>                        <---HERE (2
>     ])
> name: snapshot name given by user. if user do not provide the name, it will be
> the epoch seconds.
> creation_time: the epoch seconds.
> save: the memory save file for this snapshot.
> disks: store the disk snapshot information associate with this domain
>
> question
> --------------
> How should I deal with domains with multiple disks?Could I assume that
> there is only one disk?
no.
for domain snapshot with internal disk snapshot: savevm/delvm/loadvm will also
do disk snapshot.
for domain snapshot with external disk, the follow api will take disk snapshot
while libvirt will provide the disk list from snapshot.
reference libvirt qemu driver qemuDomainSnapshotCreateDiskActive for how it
works in libxl driver, and qemuMonitorJSONDiskSnapshot show add the disk into
qmp transaction.s

/* create disk snapshot with qmp transaction
 */
int libxl_disk_snapshot_create(libxl_ctx *ctx, int domid,
                               libxl_disk_snapshot *snapshot, int nb);

/* delete disk snapshot with qmp delete(TODO) one by one
 */
int libxl_disk_snapshot_delete(libxl_ctx *ctx, int domid,
                               libxl_disk_snapshot *snapshot, int nb);

> Just some clarification because in the libxl_domain_snapshot we have
> an array of libxl_disk_snapshot
> which somehow suggest you are taking care of domains with multiple disks.
yes, see my comments above.
>
> Regards,
> David.

09:32 2014-06-05
GTD
0, 9:10-19:20

1, today
1), 9:32-11:35 16:01-17:20 snapshot code review.
2), 11:35-12:30 lunch
3), 12:54-15:42 nap 20' read LWN weekly edition.
4), 15:42 16:01 virsh network command. see"16:01 2014-06-05"
5), 18:29-18:57 reply David kiarie email. see"09:31 2014-06-05"
6), 19:00-19:13 try to write email to Wei Liu and Ian Jackson. i am not sure whether i need it or not. see"12:31 2014-06-03"3.

2, next
1), study daemon
1), study about exit and wait.
2), study about socket.

09:32 2014-06-05
(18:21 2014-06-06)
snapshot
1, plan
1), test snapshot with script.
2), write code for external disk snapshot.
3), TODO
(1), only support snapshot all disks?
it will easy to implement but is different from libvirt.
(2), do i need "info snapshots"?
i would be useful for checking before snapshot operations.
(3), i do not know how to write libxlu_disk_l.l, temperary use the libxl_device_disks format.
4), TODO current
(1), support domain state?
TODO: discuss with Jim.
    bool running;
    bool blocked;
    bool paused;
    bool shutdown;
    bool dying;
2, do
1), done: recheck create and delete function.
2), done: recheck list and revert function.

12:53 2014-06-05
software skill, vnc, openstack, noVNC
http://www.vpsee.com/2013/07/integrating-novnc-with-our-vm-control-panel/
使用 noVNC 开发 Web 虚拟机控制台

http://kanaka.github.io/noVNC/noVNC/vnc.html
http://kanaka.github.io/noVNC/

16:01 2014-06-05
software skill, network, virsh, net
# virsh net-dumpxml nat
<network>
  <name>nat</name>
  <uuid>cb7b045a-3cd2-459e-8625-a754b7a60e5a</uuid>
  <forward mode='nat'>
    <nat>
      <port start='1024' end='65535'/>
    </nat>
  </forward>
  <bridge name='virbr0' stp='on' delay='0'/>
  <mac address='52:54:00:5c:aa:2d'/>
  <domain name='nat'/>
  <ip address='192.168.100.1' netmask='255.255.255.0'>
    <dhcp>
      <range start='192.168.100.128' end='192.168.100.254'/>
    </dhcp>
  </ip>
</network>

# virsh net-info nat
Name:           nat
UUID:           cb7b045a-3cd2-459e-8625-a754b7a60e5a
Active:         yes
Persistent:     yes
Autostart:      yes
Bridge:         virbr0

09:46 2014-06-06
GTD
0, 9:30

1, today
1), 9:50-10:04 mailing list: arm. see"10:00 2014-06-06"
2), 10:04-11:34 14:32-15:50 17:39-18:50 snapshot. see"10:04 2014-06-06"
3), 11:34-12:14 lunch.
4), 13:52-14:32 nap.

2, next
1), try opensuse aarch64. http://en.opensuse.org/openSUSE:AArch64
1), study daemon
1), study about exit and wait.
2), study about socket.

10:00 2014-06-06
mailing list, virtualization, xen, arm, aarch64
1, http://lists.xen.org/archives/html/xen-devel/2014-06/msg00606.html
[Xen-devel] [PATCH] xen: arm: implement generic multiboot compatibility strings (Was: Re: [Linaro-uefi] The GRUB multiboot support patch for aarch64(V3.1))

This causes Xen to accept the more generic names originally proposed by
Andre in http://thread.gmane.org/gmane.linux.linaro.announce.boot/326 and
incorporated into the proposal in
http://wiki.xen.org/wiki/Xen_ARM_with_Virtualization_Extensions/Multiboot

2, details
http://thread.gmane.org/gmane.linux.linaro.announce.boot/326
From: Andre Przywara <andre.przywara@...>
Subject: [PROPOSAL] ARM/FDT: passing multiple binaries to a kernel
Newsgroups: gmane.linux.linaro.announce.boot, gmane.linux.drivers.devicetree, gmane.linux.ports.arm.kernel
Date: 2013-09-03 15:53:44 GMT (39 weeks, 2 days, 9 hours and 55 minutes ago)

Hi,

a normal Linux kernel currently supports reading the start and end
address of a single binary blob via the FDT's /chosen node.
This will be interpreted as the location of an initial RAM disk.

The Xen hypervisor itself is a kernel, but needs up to _two_ binaries
for proper operation: a Dom0 Linux kernel and it's associated initrd.
On x86 this is solved via the multiboot protocol used by the Grub
bootloader, which supports to pass an arbitrary number of binary modules
to any kernel.

Since in the ARM world we have the versatile device tree, we don't need
to implement the mulitboot protocol.

So I'd like to propose a new binding which denotes binary modules a
kernel can use at it's own discretion.
The need is triggered by the Xen hypervisor (which already uses a very
similar scheme), but the approach is deliberately chosen to be as
generic as possible to allow future uses (like passing firmware blobs
for devices or the like).
Credits for this go to Ian Campbell, who started something very similar
[1] for the Xen hypervisor. The intention of this proposal is to make
this generic and publicly documented.

Looking forward to any comments!

Thanks,
Andre.

[1]
http://xenbits.xen.org/gitweb/?p=xen.git;a=blob;f=docs/misc/arm/device-tree/booting.txt;h=94cd3f18a4e1317a35e1255bf5c6e1e091001d1a;hb=HEAD
----------------------------
* Multiple boot modules device tree bindings

Boot loaders wanting to pass multiple additional binaries to a kernel
shall add a node "module" for each binary blob under the /chosen node
with the following properties:

- compatible:
     compatible = "boot,module";
   A bootloader may add names to more specifically describe the module,
   e.g. Xen may use "xen,dom0-kernel" or "xen,dom0-ramdisk".
   If possible a kernel should be able to use modules even without a
   descriptive naming, by enumerating them in order and using hard-coded
   meanings for each module (e.g. first is kernel, second is initrd).

- reg: specifies the base physical address and size of a region in
   memory where the bootloader loaded the respective binary data to.

- bootargs:
   An optional property describing arguments to use for this module.
   Could be a command line or configuration data.

Example:
/chosen {
     #size-cells = <0x1>;
     #address-cells = <0x1>;
     module <at> 0 {
         compatible = "xen,linux-zimage", "xen,multiboot-module",
"boot,module";
         reg = <0x80000000 0x003dcff8>;
         bootargs = "console=hvc0 earlyprintk ro root=/dev/sda1 nosmp";
     };
     module <at> 1 {
         compatible = "xen,linux-initrd", "xen,multiboot-module",
"boot,module";
         reg = <0x08000000 0x00123456>;
     };
...

10:04 2014-06-06
(22:08 2014-06-09)
snapshot, Jim
1, Jim
1),
> Bamvor Jian Zhang wrote:
> > Hi,
> >
> > here is the third version about domain snapshot documents, the second version
> > and the first version is here[1][2].
> >
> > there are lots of potential feature about snapshots. for now, i focus on
> > providing the api for libvirt libxl driver in order to support the same
> > functionality compare with libvirt qemu driver. and you may already notice
> > david is working on libvirt libxl driver. it is a GSoC project[3]. it is
> > important for him to know the api in order to start coding in libvirt side.
> >
>
> Right.  We can't do much on the libvirt side without libxl APIs :-).
>
> > i plan to work on other "advanced feature" after my first stage patch ack.
> >
> > there are two types of snapshots supported by libxl: disk snapshot and domain
> > snapshot and four types of operations: create, delete, list and revert.
> >
>
> Should the operations also include 'info', to get name, creation time,
> disks, etc. on a particular snapshot?
list with --long options will do this: list details of one/all snapshot
information. reference our discuss here[1].
>
> > Disk snapshot will only be crash-consistent if the domain is running. Disk
> > snapshots can also be internal (qcow2) or external (snapshot in one file,
> > delta
> > in another).
> >
> > Domain snapshots include disk snapshots and domain state, allowing to resume
> > the domain from the same state when the snapshot was created. This type of
> > snapshot is also referred to as a domain checkpoint or system checkpoint.
> >
> > In libvirt, there is a something like resource manager for domain snapshot
> > managements. So, in libxl, all these information is transfered through
> > libxl_domain_snapshot struct. xl will manage the snapshot by itself.
> >
> > Domain snapshot create means save domain state and do disk snapshots.
> > At the beginning of domain snapshot create, it will check whether it is
> > snapshotable. it is snapshotable if all the disk is qdisk backed.
> >
> > Domain snapshot revert means rollback the current snapshot state. and
> > Because the limitation of the qemu qmp, the revert could only support domain
> > snapshot with internal disk snapshot. revert the domain snapshot with external
> > snapshot doest not support.
> >
> > there are live flag in snasphot configuration file, it will be save domain
> > memory and do external disk snapshot. to make the thing simple, i do not want
> > to implement in my first verion of patch.
> >
> > As Ian Campbell said, the support of non-qdisk snapshot is very useful.
> > unfortuntely, i have no idea what it need to do. the only non-qdisk i know is
> > blktap. and i do not know does how to do snapshot create, delete, list and
> > revert for blktap? does it support internal or external support?
> >
>
> Looking in tools/blktap2, it seems only external snapshots are supported
> via td-util or vhd-util.  I suppose lvm would also be considered an
> external disk snapshot.
thanks. do you mean call these utils in libxl__exec? it seems that only a few
binary called by this functions: bootloader, libxl-save-helper, qemu-dm,
qemu-system-i386.
the core function of vhd_util snapshot is vhd_snashot which defined in
(tools/blktap2/include/libvhd.h, /usr/lib64/libvhd.so). although it included in
xen-devel package in opensuse12.3. maybe it is a better choice?
>
> > i treat it as an "advanced" feature, i will not cover it in my first version
> > of
> > patch.
> >
> > the new struct, api and command is as follows:
> > 1, new struct
> > 1), libxl_snapshot
> > store a disk snapshot information, it is used by disk snapshot create and
> > delete.
> > libxl_disk_snapshot = Struct("disk_snapshot",[
> >     ("device",        string),
> >     ("name",          string),
> >     ("file",          string),
> >     ("format",        string),
> >
>
> Should format be an enum?
yeah, it should be. i will use as libxl_disk_format. i only support qcow2 as
external snapshot image.
>
> >     ])
> >
> > device: device name to snapshot. e.g. sda, hda...
> > name: snapshot name given by user. it will the be same name as domain snapshot
> > name.
> > the following paramenter is only useful for external snapshot.
> > file: external snapshot file.
> > format: the format of external snapshot file
> >
> > 2), libxl_domain_snapshot
> > store domain snapshot information which store in the path shown above. i add
> > some api for create, delete and list these information.
> > libxl_domain_snapshot = Struct("domain_snapshot",[
> >     ("name",          string),
> >     ("creation_time", uint64),
> >     ("save",          string),
> >     ("disks", Array(libxl_disk_snapshot, "num_disks")),
> >     ])
> >
>
> I think 'description' and 'state' would be useful fields, the latter
> being the state of the domain when the snapshot was created.
sorry missing this. when you mention state, do you mean the following
combination in libxl_dominfo?
    bool running;
    bool blocked;
    bool paused;
    bool shutdown;
    bool dying;
>
> > name: snapshot name given by user. if user do not provide the name, it will be
> > the epoch seconds.
> > creation_time: the epoch seconds.
> > save: the memory save file for this snapshot.
> > disks: store the disk snapshot information assoiate with this domain.
> >
> > 2, new functions
> > there is no common api like libxl_snapshot_xxx. the reason is that different
> > toolstack may need to different event handling machanism(synchronize or
> > asynchronize). and obviously, domain snapshot create need async handler. so i
> > decide to only provide the sub api for xl and other toolstack(e.g. libvirt).
> > it make eailer for toolstack to handle the event by themselves.
> >
>
> What is the domain_snapshot struct used for then?  Seems it is unneeded
> if not exposed in the API, something to be declared by the app.  I'd
> like to hear what the Xen tools devs think about this approach.  Ian J,
> Ian C, Anthony?
currently, libxl_domain_snapshot is only used by xl_cmdimpl.c.
domain load snapshot configuration may be useful when libvirt libxl driver
want to load libxl snapshot configuration.
there is a issue i want to discuss with you. do we need libvirt and libxl
use the same domain snapshot configuration? when domain snapshot is created
or reloading, use libxl-json format file instead of libvirt xml format. in this
way, it is easy to handle the snapshot if user migrate between libxl and libvirt
toolstack.
>
> > 1), in libxl/libxl.h
> > the implementation will be located in libxl_snapshot.c
> > /* disk snapshot api
> >  * support create for external and internal disks, support delete for internal
> >  * snapshot of disks.
> >  */
> > /* create disk snapshot according to the device name in snapshot array. nb is
> >  * the number of snapshot array.
> >  * use the qmp transaction to ensure all snapshot of disk is coherence.
> >  */
> > int libxl_disk_snapshot_create(libxl_ctx *ctx, int domid,
> >                                libxl_disk_snapshot *snapshot, int nb,
> >                                const libxl_asyncop_how *ao_how);
> > /* delete number of nb disk snapshot describe in snapshot array
> >  */
> > int libxl_disk_snapshot_delete(libxl_ctx *ctx, int domid,
> >                                libxl_disk_snapshot *snapshot, int nb);
> >
>
> As David mentioned in his reply, there should be a revert as well.  Is
> list (and info if added) handled by the app (xl, libvirt, etc)?
for revert, reference my previous reply.
for list, it will be handled by the app, just like libvirt qemu driver.
>
> > 2), xl_cmdimpl.c
> > int libxl_snapshot_create(int domid, libxl_domain_snapshot *snapshot);
> > int libxl_snapshot_delete(int domid, libxl_domain_snapshot *snapshot);
> > int libxl_snapshot_get(int domid, libxl_domain_snapshot *snapshot, int nb);
> > int libxl_snapshot_revert(int domid, libxl_domain_snapshot *snapshot);
> >
> > support create, delete, list and revert for domain snasphot.
> >
> > libxl_snapshot_get will read the domain snapshot configuration file stored in
> > disk and list snapshot information in simple or long format.
> >
> > 3, snapshot information file
> > i will write manpage for this with patch.
> >
> > i found the Wei v5 patch about xl json format.
> > http://lists.xen.org/archives/html/xen-devel/2014-05/msg01670.html
> > it seems that i could use these apis for parsing and generating the snapshot
> > information file.
> >
>
> Yeah, with all of this work, seems json would be a better way to go.
>
> Regards,
> Jim
>
> > the domain snapshot information will store in the follow path:
> > /var/lib/xen/snapshots/<domain_uuid>/snapshotdata-<snapshot_name>.xl
> >
> > here is an example for snapshot information file:
> > description="a snapshot after installation"
> > name="1397207577"
> > creationtime="1397207577"
> > save="1397207577.save"
> > type="internal"/"external"
> > live="no"
> > disk_only="no"
> > disk=[ 'hda=disk_hda.qcow2,type=qcow2', 'hdc=disk_hdc.qcow2,type=qcow2']
> >
> > the save and disk image file base on the path of
> > "/var/lib/xen/snapshots/<domain_uuid>"
> >
> > the user could give a snapshot name when vm snapshot created. if not, the
> > epoch
> > seconds will set as name as the above examples.
> >
> > 3, new command
> > i will write manpage for this with patch.
> > 1), snapshot-create
> > Usage: xl snapshot-create <ConfigFile> [options] [Domain]
> >
> > Create domain snapshot with ConfigFile or options
> >
> > Options:
> > -n                snapshot name
> > --live            do live snapshot
> > --disk-only       only disk snapshot, do not save memory.
> >
> > 2), snapshot-list
> > Usage: xl snapshot-list [options] [Domain]
> >
> > List domain snapshot information about all/some snapshot in one domain.
> >
> > Options:
> > -l, --long        Output all domain snapshot details
> > -n                snapshot name
> >
> > 3), snapshot-delete
> > Usage: xl snapshot-delete [options] [Domain]
> >
> > Delete domain snapshot relative data, including domain state, disk snapshot
> > and domain snapshot information file.
> >
> > Options:
> > -n                snapshot name
> >
> > 4), snapshot-revert
> > Usage: xl snapshot-revert [options] [Domain]
> >
> > Rollback the domain to snapshot state.
> >
> > Options:
> > -n                snapshot name
> >
> > [1] http://lists.xen.org/archives/html/xen-devel/2014-04/msg00414.html
> >     http://lists.xen.org/archives/html/xen-devel/2014-04/msg00244.html
> > [2] http://lists.xen.org/archives/html/xen-devel/2014-04/msg02549.html
> > [3]
> > http://en.opensuse.org/openSUSE:GSOC_ideas#Add_virtual_machine_snapshot_support_to_libvirt_Xen_driver
> >
> > changes since v2:
> > 1), reorgnized the whole docments.
> > 2), do not export the dedicated the disk snapshot commands.
> > 3), others changes according to Ian and Jim's comment.
> >
> > regards
> >
> > bamvor
> >
> >
[1] http://lists.xen.org/archives/html/xen-devel/2014-04/msg02904.html

2),
Hi, Jim
> Bamvor Jian Zhang wrote:
> > Hi, David
> >
> >> On Fri, May 16, 2014 at 6:00 PM, Bamvor Jian Zhang <bjzhang@xxxxxxxx> wrote:
> >>
> >>
> >>> Hi, david
> >>>
> >>>
> >>>> On Thu, May 15, 2014 at 4:58 PM, Bamvor Jian Zhang <bjzhang@xxxxxxxx>
> >>>>
> >>> wrote:
> >>>
> >>>>> Hi,
> >>>>>
> >>>>> here is the third version about domain snapshot documents, the second
> >>>>> version
> >>>>> and the first version is here[1][2].
> >>>>>
> >>> ...
> >>>
> >>>>> 2, new functions
> >>>>> there is no common api like libxl_snapshot_xxx. the reason is that
> >>>>> different
> >>>>> toolstack may need to different event handling machanism(synchronize or
> >>>>> asynchronize). and obviously, domain snapshot create need async
> >>>>>
> >>> handler.
> >>>
> >>>>> so i
> >>>>> decide to only provide the sub api for xl and other toolstack(e.g.
> >>>>> libvirt).
> >>>>> it make eailer for toolstack to handle the event by themselves.
> >>>>>
> >>>>> 1), in libxl/libxl.h
> >>>>> the implementation will be located in libxl_snapshot.c
> >>>>> /* disk snapshot api
> >>>>>  * support create for external and internal disks, support delete for
> >>>>> internal
> >>>>>  * snapshot of disks.
> >>>>>  */
> >>>>> /* create disk snapshot according to the device name in snapshot
> >>>>>
> >>> array. nb
> >>>
> >>>>> is
> >>>>>  * the number of snapshot array.
> >>>>>  * use the qmp transaction to ensure all snapshot of disk is coherence.
> >>>>>  */
> >>>>> int libxl_disk_snapshot_create(libxl_ctx *ctx, int domid,
> >>>>>                                libxl_disk_snapshot *snapshot, int nb,
> >>>>>                                const libxl_asyncop_how *ao_how);
> >>>>> /* delete number of nb disk snapshot describe in snapshot array
> >>>>>  */
> >>>>> int libxl_disk_snapshot_delete(libxl_ctx *ctx, int domid,
> >>>>>                                libxl_disk_snapshot *snapshot, int nb);
> >>>>>
> >> Are this the only two functions you are exposing?I mean the API?or am I
> >> getting something wrong?
> >>
> >>
> > there are functions libxl_qmp_loadvm, libxl_qmp_savevm which is called by
> > domain internal snapshot create/revert. currently, they are internal
> > functions.
> > (libxl_intenal.h). but i should expose them, e.g. look like:
> > int libxl_loadvm(libxl_ctx *ctx, int domid, libxl_domain_snapshot *snapshot,
> >                  const libxl_asyncop_how *ao_how);
> > int libxl_savevm(libxl_ctx *ctx, int domid, libxl_domain_snapshot *snapshot,
> >                  const libxl_asyncop_how *ao_how);
> >
>
> IMO, would be better to have libxl_disk_snapshot_create() and
> libxl_disk_snapshot_delete() exposed, and keep these as internal.
i am not sure if i could follow you. i guess you mean we should expose unique
api for dedicate operations. this is what i want.
but the issue is there is no single revert disk snapshot qmp or hmp in qemu.
so, in libvirt qemu driver, internal or external disk snapshot call
loadvm/savevm/delvm and transaction/blockdev-snapshot-sync/
blockdev-snapshot-internal-sync respectively.
IMO, it will be better if qemu support blockdev snapshot revert qmp which expose
bdrv_snapshot_goto to qemu user(e.g. libxl).
i do not know how qemu upstream think about it.  how about discuss it later
when basic snapshot function accept in libxl?

regards

bamvor

>
> Regards,
> Jim
>

13:27 2014-06-06
software skill, build sercie, obs, maintenance
http://doc.opensuse.org/products/draft/OBS/obs-reference-guide_draft/cha.obs.maintenance_setup.html
osc maintenancerequest(mr)

10:58 2014-06-09
software skill, yast, yast2, debug; strace, only trace specific system call
1, http://en.opensuse.org/openSUSE:YaST_debugging
Logs

The logs are in /var/log/YaST2. The primary log file is y2log which is rotated to y2log-[1-9]. If YaST is run as non-root, the logs are ~/.y2log, ~/.y2log-[1-9].

More Log Messages

Debugging log messages can be enabled in several ways:

Y2DEBUG=1
this is an environment variable. Use like Y2DEBUG=1 /sbin/yast2 lan. It also works on the kernel command line during the installation.
UI dialog
Press Shift-F7 in the Qt UI to switch the debugging messages at run time.
signal
Send the y2base process a USR1 signal to toggle debug logging. killall -USR1 y2base
There also are environment variables for controlling the level of logging libzypp (package management library used by YaST) events:

ZYPP_MEDIA_CURL_DEBUG=1
enables logging of detailed CURL information. Useful for debugging network-related problems with installation or package management.
strace

Strace is very useful for debugging YaST and other programs too. Here are the basics.

This is a frequently useful line: Y2DEBUG=1 strace -eopen -ostrace.log /usr/lib/YaST2/bin/y2base lan qt.

2, bamvor:
"-eopen" mean that only trace open system call.

11:00 2014-06-09
GTD
0, 9:50-18:05

1, today
1), 14:01-18:01 snapshot.
2), 11:40-12:30 lunch.

12:29 2014-06-09
真功夫
1, zhangjian: 28.5
2, 茄子：邹钰，wangliuhua。19.5
3，香干：lixia，chunyan. 25.2

13:01 2014-06-09
bug, virtualization, xen
https://bugzilla.novell.com/show_bug.cgi?id=880636

13:02 2014-06-09
fail between the follow line:
(XEN) Dom0 has maximum 2 VCPUs
(XEN) elf_load_binary: phdr 0 at 0xffffffff80002000 -> 0xffffffff8087d000

13:52 2014-06-09
default password: N0vell.

16:32 2014-06-09
virtualization, vnc, xl, virt-viewer, virt-manager
1, there is a difference between "xl vnc" and virt-viewer/virt-manager, the former one call vncviewer directly while the latter one use its own vnc client.
PS: i do not read the code, i just check that whether there is a vncviewer process exist.

10:06 2014-06-10
GTD
0, 9:50

1, today
1), 10:06-10:21 mailing list. see"10:21 2014-06-10"
2), 10:21-11:09 14:30-14:47 xen, memory, balloon(?), guest: windows, bnc#881609. see"10:26 2014-06-10"
3), 11:21-11:40 13:14-14:30 14:54-15:03 16:30-18:28 snapshot. check email. testing.
4), 11:40-12:43 lunch.
5), 15:03- sync.

10:15 2014-06-10
mailing list, virtualization, xen; bootloader, uefi
1, ijc at hellion
Jun 5, 2014, 11:01 AM
Re: [Linaro-uefi] [PATCH] xen: arm: implement generic multiboot compatibility strings (Was: Re: The GRUB multiboot support patch for aarch64(V3.1)) [In reply to]
On Thu, 2014-06-05 at 18:05 +0100, Leif Lindholm wrote:
> On Thu, Jun 05, 2014 at 05:55:31PM +0100, Ian Campbell wrote:
> > > > - if ( fdt_node_check_compatible(fdt, node, "xen,linux-zimage") == 0 )
> > > > + if ( fdt_node_check_compatible(fdt, node, "xen,linux-zimage") == 0 ||
> > > > + fdt_node_check_compatible(fdt, node, "multiboot,linux-zimage") == 0 )
> > >
> > > While we are modifying the protocol, "linux-zImage" is confusing in the
> > > name. Actually we can use it for an ELF, another OS... I don't think Xen
> > > will change his behavior depending of the DOM0 image.
> >
> > zImage defines the boot protocol to use. Since the protocol is defined
> > by Linux as zImage I think that is the appropriate name, if some other
> > OS wants to mimic Linux then fine. But if we end up supporting some OS
> > with its own boot protocol which doesn't match Linux's then that should
> > have a distinct name of its own.
>
> Actually, there is no zImage support in arm64 - only Image.

Right, that's a slight wrinkle in the naming. arm64's image is
essentially equivalent (from a calling into it PoV) to zImage, it
doesn't seem worth dupping the name here.

Ian.

2, fu.wei at linaro (RedHat)
Jun 6, 2014, 5:24 AM
Re: [Linaro-uefi] [PATCH] xen: arm: implement generic multiboot compatibility strings (Was: Re: The GRUB multiboot support patch for aarch64(V3.1)) [In reply to]
On 06/06/2014 02:31 AM, Ian Campbell wrote:
> On Thu, 2014-06-05 at 18:03 +0100, Julien Grall wrote:
>>>> While we are modifying the protocol, "linux-zImage" is confusing in the
>>>> name. Actually we can use it for an ELF, another OS... I don't think Xen
>>>> will change his behavior depending of the DOM0 image.
>
> Actually thinking about this some more I think you are right. Xen
> already probes the kernel it gets so we can safely implement this as
> multiboot,kernel, since we don't really need the more specific type. If
> in the future some non-probable kernel comes along which we want to
> support we still have the option of adding more specific compatibility
> strings.
>
> Fu Wei -- if this is OK with you I will modify the wiki page to
> s/multiboot,linux-zimage/multiboot,kernel/ and rev this patch to suit.

This is OK for me, And I think the "multiboot,kernel" is better and more generic. :-)

>
> Can we do something similar with linux-ramdisk? I'm not sure since we
> cannot easily probe the ramdisk contents. We could base the ramdisk
> behaviour on the probed behaviour of the kernel. Anyone got any
> thoughts?

My thought looks exactly the same as yours :
The cpio utility can detect the cpio file format. Maybe we can just probe the file, see if this is a cpio or cpio.gz.

>
> Ian.
>

--
Best regards,

Fu Wei
Enterprise Server Engineer From Red Hat
LEG Team
Linaro.org | Open source software for ARM SoCs
Ph: +86 186 2020 4684 (mobile)
IRC: fuwei
Skype: tekkamanninja
Room 1512, Regus One Corporate Avenue,Level 15,
One Corporate Avenue,222 Hubin Road,Huangpu District,
Shanghai,China 200021

10:26 2014-06-10
virtualization, xen, memory, balloon(?), guest: windows, bnc#881609
Private Lin Ma 2014-06-09 09:34:35 UTC

After turning balloon off, The "Complete memory dump" completes successfully
and no error output on qemu-dm log.
Hope this could be a piece of clue for fixing the problem.

Private Comment 4 Kirk Allan 2014-06-09 22:47:10 UTC

With the clue of turning off ballooning, I noticed that the
XENMEM_maximum_reservation and XENMEN_current_reservation yield different
values for the same memory configuration for vms running on a sles 11 host and
on a sles 12 host. The different values cause the balloon driver to balloon
memory.  Then when the crash dump happens, the dump tries to access the
ballooned pages which causes the vm to hang.

11:41 2014-06-11
GTD
1, today
1), 11:40-11:53 lunch.
2), 10' mailing list. see"12:00 2014-06-11".
3), kindle rss.
4), 14:14-14:39 nap.

2, next
1), TODO test snasphot create fail. where is the log?

12:00 2014-06-11
mailing list, virtualization, xen, qemu for dom0
"Ian Campbell"_reply_"Steven Haigh <netwiz@xxxxxxxxx>"_email_"Re: [Xen-devel] Starting QEMU as disk backend for dom0"_20140606
On Fri, 2014-06-06 at 15:55 +1000, Steven Haigh wrote:
> Hi guys,
>
> I'm just picking through the xencommons initscript provided with the Xen
> 4.4 source tarball...
>
> I notice this section starting at line 119:
>         echo Starting QEMU as disk backend for dom0
>         test -z "$QEMU_XEN" && QEMU_XEN="${LIBEXEC}/qemu-system-i386"
>         $QEMU_XEN -xen-domid 0 -xen-attach -name dom0 -nographic -M
> xenpv -daemonize \
>                 -monitor /dev/null -serial /dev/null -parallel /dev/null \
>                 -pidfile $QEMU_PIDFILE
>
> Can anyone tell me the purpose of this?

It is used if dom0 needs to be able to access a virtual disk in a
non-trivial form, i.e. qcow or vhd. In this case a vbd is attached to
dom0 which is backed by qdisk in this qemu process. For trivial disk
formats like raw files or block partitions dom0 can just access them
directly.

This comes into play when you are using pygrub, since dom0 needs to be
able to read guest disks. I'm not sure if their are other places. I
suppose you can also do "xl block-attach 0 <spec>" if you like e.g. in
order to mount a qcow image in dom0 for some reason.

Ian.

15:07 2014-06-11
software skill, debugger, gdb
1, condition breakpoints on return value
1), http://stackoverflow.com/questions/4498965/how-to-set-conditional-breakpoint-if-malloc-returns-null-via-gdb
2), http://stackoverflow.com/questions/1378594/is-it-possible-to-set-a-conditional-breakpoint-at-the-end-of-a-function-based-on
Agree with previous commenter that this is probably something you don't want to do, but for me, setting a conditional breakpoint at the last instruction on $eax (or $rax if you are on 64-bit x86) works just fine.

For the code

unsigned int foo(void) { return 1; }
unsigned int bar(void) { return 4; }
unsigned int myFunc(void) { return foo()+bar(); }

using gdb ..

(gdb) disass myFunc
Dump of assembler code for function myFunc:
0x080483d8 <myFunc+0>:  push   %ebp
0x080483d9 <myFunc+1>:  mov    %esp,%ebp
0x080483db <myFunc+3>:  push   %ebx
0x080483dc <myFunc+4>:  call   0x80483c4 <foo>
0x080483e1 <myFunc+9>:  mov    %eax,%ebx
0x080483e3 <myFunc+11>: call   0x80483ce <bar>
0x080483e8 <myFunc+16>: lea    (%ebx,%eax,1),%eax
0x080483eb <myFunc+19>: pop    %ebx
0x080483ec <myFunc+20>: pop    %ebp
0x080483ed <myFunc+21>: ret
End of assembler dump.
(gdb) b *0x080483ed if $eax==5
Breakpoint 1 at 0x80483ed
(gdb) run
Starting program: /tmp/x
Breakpoint 1, 0x080483ed in myFunc ()
(gdb)

2, using gdb variable for function calling counter comparasion.
http://blog.timac.org/?p=118

17:01 2014-06-11
qemu disk
  <devices>
    <emulator>/usr/lib/xen/bin/qemu-system-i386</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2'/>
      <source file='/home/bamvor/vm/xen/images/disk0.qcow2'/>
      <target dev='hda' bus='ide'/>
      <address type='drive' controller='0' bus='0' target='0' unit='0'/>
    </disk>
    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source file='/home/bamvor/vm/iso/win-7-fcs-64-auto.iso'/>
      <target dev='hdb' bus='ide'/>
      <readonly/>
      <address type='drive' controller='0' bus='0' target='0' unit='1'/>
    </disk>
    ...
  </devices>

17:48 2014-06-11
software skill, virtualization, xen, hypervisor, debug, log level
grub2
multiboot /boot/xen.gz loglvl=all guest_loglvl=all watchdog=true watchdog_timeout=10

